{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49eccdf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef get_agreements(tA, tB, tC):\\n    # tA, tB, and tC are binary outputs from classifiers  A, B, and C respectively\\n    tA_pos = np.where(tA == 1)[0]\\n    tA_neg = np.where(tA == 0)[0]\\n    tB_pos = np.where(tB == 1)[0]\\n    tB_neg = np.where(tB == 0)[0]\\n    tC_pos = np.where(tC == 1)[0]\\n    tC_neg = np.where(tC == 0)[0]\\n    n_tA_pos, n_tA_neg = len(tA_pos), len(tA_neg)\\n    n_tB_pos, n_tB_neg = len(tB_pos), len(tB_neg)\\n\\n    n_pos_pos_pos = len(set(tA_pos).intersection(tB_pos).intersection(tC_pos))\\n    n_pos_pos_neg = len(set(tA_pos).intersection(tB_pos).intersection(tC_neg))\\n\\n    n_pos_neg_pos = len(set(tA_pos).intersection(tB_neg).intersection(tC_pos))\\n    n_pos_neg_neg = len(set(tA_pos).intersection(tB_neg).intersection(tC_neg))\\n\\n    n_neg_pos_pos = len(set(tA_neg).intersection(tB_pos).intersection(tC_pos))\\n    n_neg_pos_neg = len(set(tA_neg).intersection(tB_pos).intersection(tC_neg))\\n\\n    n_neg_neg_pos = len(set(tA_neg).intersection(tB_neg).intersection(tC_pos))\\n    n_neg_neg_neg = len(set(tA_neg).intersection(tB_neg).intersection(tC_neg))\\n    \\n    return n_pos_pos_pos, n_pos_pos_neg, n_pos_neg_pos, n_pos_neg_neg, n_neg_pos_pos, n_neg_pos_neg, n_neg_neg_pos, n_neg_neg_neg\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import pdb\n",
    "\n",
    "import torch\n",
    "\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "\n",
    "# Alpha and beta control class-specific error rates; \n",
    "# alpha is the  false negative rate\n",
    "# beta is the false positive rate\n",
    "# I create classifiers with certain FNRs / FPRs but simulating different alphas & betas on the train set\n",
    "\n",
    "def apply_alpha_beta(y, alpha_opt, beta_opt):\n",
    "    t = y.copy()\n",
    "    for label in [0, 1]:\n",
    "        n_examples = (t == label).sum()\n",
    "        swap_rate = alpha_opt if label == 0 else beta_opt\n",
    "        swap_idxs = np.random.choice(np.where(t == label)[0], size=int(swap_rate * n_examples), replace=False)\n",
    "        t[swap_idxs] = 1 - label\n",
    "    return t\n",
    "\n",
    "def inv_sigm(y):\n",
    "    return np.log(y / (1 - y))\n",
    "\n",
    "def gibbs_classification(model, x):\n",
    "    p_y = model.predict_proba(x)[:,1]\n",
    "    samples = np.random.random(p_y.shape)\n",
    "    return (samples  < p_y).astype(int)\n",
    "\n",
    "def get_agreements(*tests):\n",
    "    # tests are binary outputs from classifiers\n",
    "    # This function calculates the number of agreements between the classifiers\n",
    "    # for all possible combinations of positive and negative results\n",
    "    \n",
    "    # Initialize a dictionary to store the results\n",
    "    agreements = {}\n",
    "    \n",
    "    # Get the indices of positive and negative results for each test\n",
    "    pos_indices = [np.where(test == 1)[0] for test in tests]\n",
    "    neg_indices = [np.where(test == 0)[0] for test in tests]\n",
    "    \n",
    "    # Calculate the number of agreements for all combinations\n",
    "    for i in range(2 ** len(tests)):\n",
    "        # Convert the combination to binary and pad with zeros\n",
    "        combination = bin(i)[2:].zfill(len(tests))\n",
    "        \n",
    "        # Get the indices for this combination\n",
    "        indices = [pos_indices[j] if combination[j] == '1' else neg_indices[j] for j in range(len(tests))]\n",
    "        \n",
    "        # Calculate the number of agreements\n",
    "        agreements[combination] = len(set(indices[0]).intersection(*indices[1:]))\n",
    "    \n",
    "    return agreements\n",
    "\n",
    "'''\n",
    "def get_agreements(tA, tB, tC):\n",
    "    # tA, tB, and tC are binary outputs from classifiers  A, B, and C respectively\n",
    "    tA_pos = np.where(tA == 1)[0]\n",
    "    tA_neg = np.where(tA == 0)[0]\n",
    "    tB_pos = np.where(tB == 1)[0]\n",
    "    tB_neg = np.where(tB == 0)[0]\n",
    "    tC_pos = np.where(tC == 1)[0]\n",
    "    tC_neg = np.where(tC == 0)[0]\n",
    "    n_tA_pos, n_tA_neg = len(tA_pos), len(tA_neg)\n",
    "    n_tB_pos, n_tB_neg = len(tB_pos), len(tB_neg)\n",
    "\n",
    "    n_pos_pos_pos = len(set(tA_pos).intersection(tB_pos).intersection(tC_pos))\n",
    "    n_pos_pos_neg = len(set(tA_pos).intersection(tB_pos).intersection(tC_neg))\n",
    "\n",
    "    n_pos_neg_pos = len(set(tA_pos).intersection(tB_neg).intersection(tC_pos))\n",
    "    n_pos_neg_neg = len(set(tA_pos).intersection(tB_neg).intersection(tC_neg))\n",
    "\n",
    "    n_neg_pos_pos = len(set(tA_neg).intersection(tB_pos).intersection(tC_pos))\n",
    "    n_neg_pos_neg = len(set(tA_neg).intersection(tB_pos).intersection(tC_neg))\n",
    "\n",
    "    n_neg_neg_pos = len(set(tA_neg).intersection(tB_neg).intersection(tC_pos))\n",
    "    n_neg_neg_neg = len(set(tA_neg).intersection(tB_neg).intersection(tC_neg))\n",
    "    \n",
    "    return n_pos_pos_pos, n_pos_pos_neg, n_pos_neg_pos, n_pos_neg_neg, n_neg_pos_pos, n_neg_pos_neg, n_neg_neg_pos, n_neg_neg_neg\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4ca046",
   "metadata": {},
   "source": [
    "## Testing Hui & Walter Method on completely synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa06cbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_alpha1, true_beta1 = .3, .1\n",
    "true_alpha2, true_beta2 = .2, .1\n",
    "true_alpha3, true_beta3 = .2, .5\n",
    "true_theta1, true_theta2 = .3, .6\n",
    "\n",
    "n_examples = 10000\n",
    "y1 = np.concatenate([np.ones((int(true_theta1*n_examples))), np.zeros((int((1-true_theta1)*n_examples)))])\n",
    "y2 = np.concatenate([np.ones((int(true_theta2*n_examples))), np.zeros((int((1-true_theta2)*n_examples)))])\n",
    "\n",
    "tA_1 = apply_alpha_beta(y1, true_alpha1, true_beta1)\n",
    "tA_2 = apply_alpha_beta(y2, true_alpha1, true_beta1)\n",
    "\n",
    "tB_1 = apply_alpha_beta(y1, true_alpha2, true_beta2)\n",
    "tB_2 = apply_alpha_beta(y2, true_alpha2, true_beta2)\n",
    "\n",
    "tC_1 = apply_alpha_beta(y1, true_alpha3, true_beta3)\n",
    "tC_2 = apply_alpha_beta(y2, true_alpha3, true_beta3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17fcceb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2795 1795 1165 4245\n",
      "5062 1418 1058 2462\n",
      "Test A alpha:  0.10000000149011612 \tbeta:  0.8999999761581421\n",
      "Test B alpha:  0.10000000149011612 \tbeta:  0.8999999761581421\n",
      "Theta1:  0.8999999761581421 Theta2:  0.30000001192092896\n",
      "\n",
      "Test A alpha:  0.16481351852416992 \tbeta:  0.17984192073345184\n",
      "Test B alpha:  0.10040285438299179 \tbeta:  0.2142598032951355\n",
      "Theta1:  0.44045692682266235 Theta2:  0.7248611450195312\n",
      "\n",
      "Test A alpha:  0.23203293979167938 \tbeta:  0.13754768669605255\n",
      "Test B alpha:  0.1380850225687027 \tbeta:  0.14417274296283722\n",
      "Theta1:  0.35659152269363403 Theta2:  0.657833993434906\n",
      "\n",
      "Test A alpha:  0.25460129976272583 \tbeta:  0.11267326027154922\n",
      "Test B alpha:  0.16368131339550018 \tbeta:  0.11577228456735611\n",
      "Theta1:  0.32136300206184387 Theta2:  0.6220173239707947\n",
      "\n",
      "Test A alpha:  0.2620953321456909 \tbeta:  0.10411601513624191\n",
      "Test B alpha:  0.17151999473571777 \tbeta:  0.10535009205341339\n",
      "Theta1:  0.3102375268936157 Theta2:  0.6090832948684692\n",
      "\n",
      "Test A alpha:  0.26395490765571594 \tbeta:  0.10227874666452408\n",
      "Test B alpha:  0.17316076159477234 \tbeta:  0.10268832743167877\n",
      "Theta1:  0.30771055817604065 Theta2:  0.6059979796409607\n",
      "\n",
      "Test A alpha:  0.2641430199146271 \tbeta:  0.10211345553398132\n",
      "Test B alpha:  0.17330780625343323 \tbeta:  0.10241726040840149\n",
      "Theta1:  0.3074687719345093 Theta2:  0.6056982278823853\n",
      "\n",
      "Test A alpha:  0.2641460597515106 \tbeta:  0.1021110862493515\n",
      "Test B alpha:  0.17330996692180634 \tbeta:  0.1024128794670105\n",
      "Theta1:  0.307465136051178 Theta2:  0.6056935787200928\n",
      "\n",
      "Test A alpha:  0.2641465365886688 \tbeta:  0.10211064666509628\n",
      "Test B alpha:  0.17331036925315857 \tbeta:  0.10241223871707916\n",
      "Theta1:  0.30746448040008545 Theta2:  0.6056928038597107\n",
      "\n",
      "Test A alpha:  0.26414674520492554 \tbeta:  0.1021103709936142\n",
      "Test B alpha:  0.17331060767173767 \tbeta:  0.10241188853979111\n",
      "Theta1:  0.3074640929698944 Theta2:  0.605692446231842\n",
      "\n",
      "Job duration:  11.51572322845459\n"
     ]
    }
   ],
   "source": [
    "# Two tests, two populations\n",
    "n_epochs = 10000\n",
    "log_interval = 2000\n",
    "\n",
    "start = time.time()\n",
    "alpha1_v = Variable(torch.Tensor([inv_sigm(.1)]), requires_grad=True)\n",
    "alpha2_v = Variable(torch.Tensor([inv_sigm(.1)]), requires_grad=True)\n",
    "beta1_v = Variable(torch.Tensor([inv_sigm(.9)]), requires_grad=True)\n",
    "beta2_v = Variable(torch.Tensor([inv_sigm(.9)]), requires_grad=True)\n",
    "theta1_v = Variable(torch.Tensor([inv_sigm(.9)]), requires_grad=True)\n",
    "theta2_v = Variable(torch.Tensor([inv_sigm(.3)]), requires_grad=True)\n",
    "\n",
    "tA_1_pos = np.where(tA_1 == 1)[0]\n",
    "tA_1_neg = np.where(tA_1 == 0)[0]\n",
    "tB_1_pos = np.where(tB_1 == 1)[0]\n",
    "tB_1_neg = np.where(tB_1 == 0)[0]\n",
    "n_tA_1_pos, n_tA_1_neg = len(tA_1_pos), len(tA_1_neg)\n",
    "n_tB_1_pos, n_tB_1_neg = len(tB_1_pos), len(tB_1_neg)\n",
    "\n",
    "n_pos_pos_1 = len(set(tA_1_pos).intersection(tB_1_pos))\n",
    "n_pos_neg_1 = len(set(tA_1_pos).intersection(tB_1_neg))\n",
    "n_neg_pos_1 = len(set(tA_1_neg).intersection(tB_1_pos))\n",
    "n_neg_neg_1 = len(set(tA_1_neg).intersection(tB_1_neg))\n",
    "\n",
    "tA_2_pos = np.where(tA_2 == 1)[0]\n",
    "tA_2_neg = np.where(tA_2 == 0)[0]\n",
    "tB_2_pos = np.where(tB_2 == 1)[0]\n",
    "tB_2_neg = np.where(tB_2 == 0)[0]\n",
    "n_tA_2_pos, n_tA_2_neg = len(tA_2_pos), len(tA_2_neg)\n",
    "n_tB_2_pos, n_tB_2_neg = len(tB_2_pos), len(tB_2_neg)\n",
    "\n",
    "n_pos_pos_2 = len(set(tA_2_pos).intersection(tB_2_pos))\n",
    "n_pos_neg_2 = len(set(tA_2_pos).intersection(tB_2_neg))\n",
    "n_neg_pos_2 = len(set(tA_2_neg).intersection(tB_2_pos))\n",
    "n_neg_neg_2 = len(set(tA_2_neg).intersection(tB_2_neg))\n",
    "\n",
    "print(n_pos_pos_1, n_pos_neg_1, n_neg_pos_1, n_neg_neg_1)\n",
    "print(n_pos_pos_2, n_pos_neg_2, n_neg_pos_2, n_neg_neg_2)\n",
    "\n",
    "\n",
    "learning_rate = .01\n",
    "optim = torch.optim.Adam([theta1_v, theta2_v, beta2_v, beta1_v, alpha2_v, alpha1_v], lr = learning_rate)\n",
    "losses, beta2s = [], []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    optim.zero_grad()\n",
    "    alpha1, alpha2 = torch.sigmoid(alpha1_v), torch.sigmoid(alpha2_v)\n",
    "    beta1, beta2 = torch.sigmoid(beta1_v), torch.sigmoid(beta2_v)\n",
    "    theta1, theta2 = torch.sigmoid(theta1_v), torch.sigmoid(theta2_v)\n",
    "    n_1 = len(tA_1)\n",
    "    n_2 = len(tA_2)\n",
    "    l = n_pos_pos_1*torch.log(((theta1 * (1-beta1) * (1 - beta2)) + ((1-theta1)*alpha1*alpha2)))/n_1\n",
    "    l += n_pos_neg_1*torch.log((theta1*(1-beta1)*beta2) + ((1-theta1)*alpha1*(1-alpha2)))/n_1\n",
    "    l += n_neg_pos_1*torch.log(theta1*beta1*(1-beta2) +  (1-theta1)*(1-alpha1)*alpha2)/n_1\n",
    "    l += n_neg_neg_1*torch.log(theta1*beta1*beta2 + (1-theta1)*(1-alpha1)*(1-alpha2))/n_1\n",
    "\n",
    "    l += n_pos_pos_2*torch.log(((theta2 * (1-beta1) * (1 - beta2)) + ((1-theta2)*alpha1*alpha2)))/n_2\n",
    "    l += n_pos_neg_2*torch.log((theta2*(1-beta1)*beta2) + ((1-theta2)*alpha1*(1-alpha2)))/n_2\n",
    "    l += n_neg_pos_2*torch.log(theta2*beta1*(1-beta2) +  (1-theta2)*(1-alpha1)*alpha2)/n_2\n",
    "    l += n_neg_neg_2*torch.log(theta2*beta1*beta2 + (1-theta2)*(1-alpha1)*(1-alpha2))/n_2\n",
    "    \n",
    "    loss = -l\n",
    "    loss.backward()\n",
    "    \n",
    "    optim.step()\n",
    "    if  epoch % log_interval == 0:\n",
    "        print(\"Test A alpha: \", alpha1.item(), \"\\tbeta: \", beta1.item())\n",
    "        print(\"Test B alpha: \", alpha2.item(), \"\\tbeta: \", beta2.item())\n",
    "        print(\"Theta1: \", theta1.item(), \"Theta2: \", theta2.item())\n",
    "        print()\n",
    "    beta2s.append(beta2.item())\n",
    "    losses.append(loss.item())\n",
    "duration = time.time() - start\n",
    "print(\"Job duration: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "931afd9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1269 292 207 432\n",
      "1526 1503 958 3813\n",
      "Test A alpha:  0.10000000149011612 \tbeta:  0.8999999761581421\n",
      "Test B alpha:  0.10000000149011612 \tbeta:  0.8999999761581421\n",
      "Test C alpha:  0.8319999575614929 \tbeta:  0.30000001192092896\n",
      "Theta1:  0.3333333432674408\n",
      "\n",
      "Test A alpha:  0.1644781231880188 \tbeta:  0.21776451170444489\n",
      "Test B alpha:  0.09197968244552612 \tbeta:  0.2665822207927704\n",
      "Test C alpha:  0.08020162582397461 \tbeta:  0.6210876107215881\n",
      "Theta1:  0.4661020338535309\n",
      "\n",
      "Test A alpha:  0.2637564539909363 \tbeta:  0.08498536050319672\n",
      "Test B alpha:  0.1817132532596588 \tbeta:  0.10345783829689026\n",
      "Test C alpha:  0.09913703799247742 \tbeta:  0.49768972396850586\n",
      "Theta1:  0.29975438117980957\n",
      "\n",
      "Test A alpha:  0.26394543051719666 \tbeta:  0.08433707803487778\n",
      "Test B alpha:  0.18197789788246155 \tbeta:  0.10293000936508179\n",
      "Test C alpha:  0.09929819405078888 \tbeta:  0.49741220474243164\n",
      "Theta1:  0.29929304122924805\n",
      "\n",
      "Test A alpha:  0.26394572854042053 \tbeta:  0.08433634787797928\n",
      "Test B alpha:  0.18197818100452423 \tbeta:  0.1029292643070221\n",
      "Test C alpha:  0.09929844737052917 \tbeta:  0.49741196632385254\n",
      "Theta1:  0.2992924749851227\n",
      "\n",
      "True A alpha1:  0.2694285714285714 \t beta:  0.09866666666666668\n",
      "True B alpha2:  0.17814285714285713 \t beta:  0.09566666666666668\n",
      "True C alpha2:  0.1 \t betat:  0.5\n",
      "True theta1:  0.3\n",
      "Job duration:  14.018127202987671\n"
     ]
    }
   ],
   "source": [
    "# Three tests, one population\n",
    "n_epochs = 10000\n",
    "log_interval = 2000\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "alpha1_v = Variable(torch.Tensor([inv_sigm(.1)]), requires_grad=True)\n",
    "alpha2_v = Variable(torch.Tensor([inv_sigm(.1)]), requires_grad=True)\n",
    "alpha3_v = Variable(torch.Tensor([inv_sigm(.832)]), requires_grad=True)\n",
    "beta1_v = Variable(torch.Tensor([inv_sigm(.9)]), requires_grad=True)\n",
    "beta2_v = Variable(torch.Tensor([inv_sigm(.9)]), requires_grad=True)\n",
    "beta3_v = Variable(torch.Tensor([inv_sigm(.3)]), requires_grad=True)\n",
    "theta1_v = Variable(torch.Tensor([inv_sigm(1/3)]), requires_grad=True)\n",
    "\n",
    "tA_1_pos = np.where(tA_1 == 1)[0]\n",
    "tA_1_neg = np.where(tA_1 == 0)[0]\n",
    "tB_1_pos = np.where(tB_1 == 1)[0]\n",
    "tB_1_neg = np.where(tB_1 == 0)[0]\n",
    "tC_1_pos = np.where(tC_1 == 1)[0]\n",
    "tC_1_neg = np.where(tC_1 == 0)[0]\n",
    "n_tA_1_pos, n_tA_1_neg = len(tA_1_pos), len(tA_1_neg)\n",
    "n_tB_1_pos, n_tB_1_neg = len(tB_1_pos), len(tB_1_neg)\n",
    "\n",
    "n_pos_pos_pos_1 = len(set(tA_1_pos).intersection(tB_1_pos).intersection(tC_1_pos))\n",
    "n_pos_pos_neg_1 = len(set(tA_1_pos).intersection(tB_1_pos).intersection(tC_1_neg))\n",
    "\n",
    "n_pos_neg_pos_1 = len(set(tA_1_pos).intersection(tB_1_neg).intersection(tC_1_pos))\n",
    "n_pos_neg_neg_1 = len(set(tA_1_pos).intersection(tB_1_neg).intersection(tC_1_neg))\n",
    "\n",
    "n_neg_pos_pos_1 = len(set(tA_1_neg).intersection(tB_1_pos).intersection(tC_1_pos))\n",
    "n_neg_pos_neg_1 = len(set(tA_1_neg).intersection(tB_1_pos).intersection(tC_1_neg))\n",
    "\n",
    "n_neg_neg_pos_1 = len(set(tA_1_neg).intersection(tB_1_neg).intersection(tC_1_pos))\n",
    "n_neg_neg_neg_1 = len(set(tA_1_neg).intersection(tB_1_neg).intersection(tC_1_neg))\n",
    "\n",
    "print(n_pos_pos_pos_1, n_pos_neg_pos_1, n_neg_pos_pos_1, n_neg_neg_pos_1)\n",
    "print(n_pos_pos_neg_1, n_pos_neg_neg_1, n_neg_pos_neg_1, n_neg_neg_neg_1)\n",
    "\n",
    "\n",
    "learning_rate = .01\n",
    "optim = torch.optim.Adam([theta1_v, beta3_v, beta2_v, beta1_v, alpha3_v, alpha2_v, alpha1_v], lr = learning_rate)\n",
    "\n",
    "losses, beta2s = [], []\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    optim.zero_grad()\n",
    "    alpha1, alpha2, alpha3 = torch.sigmoid(alpha1_v), torch.sigmoid(alpha2_v), torch.sigmoid(alpha3_v)\n",
    "    beta1, beta2, beta3 = torch.sigmoid(beta1_v), torch.sigmoid(beta2_v), torch.sigmoid(beta3_v)\n",
    "    theta1 = torch.sigmoid(theta1_v)\n",
    "    n_1 = len(tA_1)\n",
    "    n_2 = len(tA_2)\n",
    "    l = n_pos_pos_pos_1*torch.log(((theta1 * (1-beta1) * (1-beta2) * (1-beta3)) + ((1-theta1)*alpha1*alpha2*alpha3)))/n_1\n",
    "    l += n_pos_pos_neg_1*torch.log(((theta1 * (1-beta1) * (1-beta2) * (beta3)) + ((1-theta1)*alpha1*alpha2*(1-alpha3))))/n_1\n",
    "\n",
    "    l += n_pos_neg_pos_1*torch.log((theta1*(1-beta1)*beta2*(1-beta3)) + ((1-theta1)*alpha1*(1-alpha2)*alpha3))/n_1\n",
    "    l += n_pos_neg_neg_1*torch.log((theta1*(1-beta1)*beta2*(beta3)) + ((1-theta1)*alpha1*(1-alpha2)*(1-alpha3)))/n_1\n",
    "\n",
    "    l += n_neg_pos_pos_1*torch.log(theta1*beta1*(1-beta2)*(1-beta3) +  (1-theta1)*(1-alpha1)*alpha2*alpha3)/n_1\n",
    "    l += n_neg_pos_neg_1*torch.log(theta1*beta1*(1-beta2)*(beta3) +  (1-theta1)*(1-alpha1)*alpha2*(1-alpha3))/n_1\n",
    "    \n",
    "    l += n_neg_neg_pos_1*torch.log(theta1*beta1*beta2*(1-beta3) + (1-theta1)*(1-alpha1)*(1-alpha2)*alpha3)/n_1\n",
    "    l += n_neg_neg_neg_1*torch.log(theta1*beta1*beta2*beta3 + (1-theta1)*(1-alpha1)*(1-alpha2)*(1-alpha3))/n_1\n",
    "\n",
    "    \n",
    "    loss = -l\n",
    "    loss.backward()\n",
    "    \n",
    "    optim.step()\n",
    "    if  epoch % log_interval == 0:\n",
    "        print(\"Test A alpha: \", alpha1.item(), \"\\tbeta: \", beta1.item())\n",
    "        print(\"Test B alpha: \", alpha2.item(), \"\\tbeta: \", beta2.item())\n",
    "        print(\"Test C alpha: \", alpha3.item(), \"\\tbeta: \", beta3.item())\n",
    "        print(\"Theta1: \", theta1.item())\n",
    "        print()\n",
    "    beta2s.append(beta2.item())\n",
    "    losses.append(loss.item())\n",
    "\n",
    "print(\"True A alpha1: \",np.mean(tA_1[y1 == 0]), '\\t beta: ', 1-np.mean(tA_1[y1 == 1]))\n",
    "print(\"True B alpha2: \", np.mean(tB_1[y1 == 0]), '\\t beta: ',  1 - np.mean(tB_1[y1 == 1]))\n",
    "print(\"True C alpha2: \", np.mean(tC_1[y1 == 0]), '\\t betat: ',  1 - np.mean(tC_1[y1 == 1]))\n",
    "\n",
    "print(\"True theta1: \", np.mean(y1))\n",
    "\n",
    "duration = time.time() - start\n",
    "print(\"Job duration: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf74e0fa",
   "metadata": {},
   "source": [
    "## Testing on logistic regressions (introducing correlated errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a24937e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ddmg/prism/.conda/envs/frank/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/data/ddmg/prism/.conda/envs/frank/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/data/ddmg/prism/.conda/envs/frank/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(penalty=&#x27;none&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(penalty=&#x27;none&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(penalty='none')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate x \n",
    "train_size = 20000\n",
    "pos = np.random.multivariate_normal((1, 1), [[1, 1], [1, 1]], size=train_size)\n",
    "neg = np.random.multivariate_normal((-1, -1), [[1, 1], [1, 1]], size=train_size)\n",
    "\n",
    "x_train = np.concatenate([pos, neg], axis=0)\n",
    "y_train = np.concatenate([np.ones(pos.shape[0]), np.zeros(neg.shape[0])], axis=0)\n",
    "\n",
    "# Train Model 1 \n",
    "mA_alpha, mA_beta = .2, .1\n",
    "xA_train, yA_train = x_train.copy(), apply_alpha_beta(y_train, mA_alpha, mA_beta)\n",
    "lrA = LogisticRegression(penalty='none')\n",
    "lrA.fit(xA_train, yA_train)\n",
    "\n",
    "# Train Model 2 \n",
    "mB_alpha, mB_beta = .3, .1\n",
    "xB_train, yB_train = x_train.copy(), apply_alpha_beta(y_train, mB_alpha, mB_beta)\n",
    "lrB = LogisticRegression(penalty='none')\n",
    "lrB.fit(xB_train, yB_train)\n",
    "\n",
    "# Train Model 3\n",
    "mC_alpha, mC_beta = .3, .2\n",
    "xC_train, yC_train = x_train.copy(), apply_alpha_beta(y_train, mC_alpha, mC_beta)\n",
    "lrC = LogisticRegression(penalty='none')\n",
    "lrC.fit(xC_train, yC_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0042b5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two test populations\n",
    "n_samples = 10000\n",
    "pos_1 = np.random.multivariate_normal((1, 1), [[1, 1], [1, 1]], size=n_samples)\n",
    "neg_1 = np.random.multivariate_normal((-1, -1), [[1, 1], [1, 1]], size=2*n_samples)\n",
    "\n",
    "pos_2 = np.random.multivariate_normal((1, 1), [[1, 1], [1, 1]], size=2*n_samples)\n",
    "neg_2 = np.random.multivariate_normal((-1, -1), [[1, 1], [1, 1]], size=2*n_samples)\n",
    "\n",
    "x1 = np.concatenate([pos_1, neg_1], axis=0)\n",
    "y1 = np.concatenate([np.ones(n_samples), np.zeros(2*n_samples)])\n",
    "x2 = np.concatenate([pos_2, neg_2], axis=0)\n",
    "y2 = np.concatenate([np.ones(2*n_samples), np.zeros(2*n_samples)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe9ae32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test A alpha1:  0.21995 \tbeta1:  0.10540000000000005\n",
      "Test B alpha2:  0.324 \tbeta2:  0.05800000000000005\n",
      "Test C alpha3:  0.1942 \tbeta3:  0.12260000000000004\n",
      "True theta1:  0.3333333333333333 \tTrue theta2:  0.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tA_1 = lrA.predict(x1)\n",
    "tB_1 = lrB.predict(x1)\n",
    "tC_1 = lrC.predict(x1)\n",
    "tA_2 = lrA.predict(x2)\n",
    "tB_2 = lrB.predict(x2)\n",
    "tC_2 = lrC.predict(x2)\n",
    "\n",
    "\n",
    "print(\"Test A alpha1: \",np.mean(tA_1[y1 == 0]), \"\\tbeta1: \", 1-np.mean(tA_1[y1 == 1]))\n",
    "print(\"Test B alpha2: \", np.mean(tB_1[y1 == 0]), \"\\tbeta2: \", 1 - np.mean(tB_1[y1 == 1]))\n",
    "print(\"Test C alpha3: \", np.mean(tC_1[y1 == 0]), \"\\tbeta3: \", 1 - np.mean(tC_1[y1 == 1]))\n",
    "\n",
    "print(\"True theta1: \", np.mean(y1), \"\\tTrue theta2: \", np.mean(y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9ccb902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13345 0 2555 14100\n",
      "22301 0 3100 14599\n",
      "Test A alpha:  0.36000001430511475 \tbeta:  0.2600000202655792\n",
      "Test B alpha:  0.42000001668930054 \tbeta:  0.10000000149011612\n",
      "Theta1:  0.3333333432674408 Theta2:  0.5\n",
      "\n",
      "Test A alpha:  0.17896504700183868 \tbeta:  0.1299770474433899\n",
      "Test B alpha:  0.22748129069805145 \tbeta:  0.04136713966727257\n",
      "Theta1:  0.45279955863952637 Theta2:  0.5795118808746338\n",
      "\n",
      "Test A alpha:  0.0941072627902031 \tbeta:  0.08277413249015808\n",
      "Test B alpha:  0.14549896121025085 \tbeta:  0.02058461308479309\n",
      "Theta1:  0.46798205375671387 Theta2:  0.5890932679176331\n",
      "\n",
      "Test A alpha:  0.053936149924993515 \tbeta:  0.06658267229795456\n",
      "Test B alpha:  0.1165434867143631 \tbeta:  0.011663191951811314\n",
      "Theta1:  0.47142255306243896 Theta2:  0.5913093090057373\n",
      "\n",
      "Test A alpha:  0.03276977688074112 \tbeta:  0.061992302536964417\n",
      "Test B alpha:  0.10899136960506439 \tbeta:  0.0070810336619615555\n",
      "Theta1:  0.47238612174987793 Theta2:  0.5918567776679993\n",
      "\n",
      "Test A alpha:  0.02062249928712845 \tbeta:  0.0605410598218441\n",
      "Test B alpha:  0.10783009976148605 \tbeta:  0.00446277717128396\n",
      "Theta1:  0.47260430455207825 Theta2:  0.5918721556663513\n",
      "\n",
      "Test A alpha:  0.01324254646897316 \tbeta:  0.059395525604486465\n",
      "Test B alpha:  0.1080145537853241 \tbeta:  0.0028705999720841646\n",
      "Theta1:  0.4724588096141815 Theta2:  0.5916036367416382\n",
      "\n",
      "Test A alpha:  0.00860226433724165 \tbeta:  0.05807046964764595\n",
      "Test B alpha:  0.10884612798690796 \tbeta:  0.0018674761522561312\n",
      "Theta1:  0.4720200300216675 Theta2:  0.5910788774490356\n",
      "\n",
      "Test A alpha:  0.005625558085739613 \tbeta:  0.05646636709570885\n",
      "Test B alpha:  0.11019939184188843 \tbeta:  0.0012226833496242762\n",
      "Theta1:  0.47132551670074463 Theta2:  0.5903121829032898\n",
      "\n",
      "Test A alpha:  0.0036934479139745235 \tbeta:  0.05465683713555336\n",
      "Test B alpha:  0.11188975721597672 \tbeta:  0.00080344412708655\n",
      "Theta1:  0.4704630374908447 Theta2:  0.5893781185150146\n",
      "\n",
      "True A alpha1:  0.21995 \t beta:  0.10540000000000005\n",
      "True B alpha2:  0.324 \t beta:  0.05800000000000005\n",
      "True theta1:  0.3333333333333333 \t  theta2:  0.5\n",
      "Job duration:  11.426288604736328\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "n_epochs = 10000\n",
    "log_interval = 1000\n",
    "\n",
    "alpha1_v = Variable(torch.Tensor([inv_sigm(.36)]), requires_grad=True)\n",
    "alpha2_v = Variable(torch.Tensor([inv_sigm(.42)]), requires_grad=True)\n",
    "beta1_v = Variable(torch.Tensor([inv_sigm(.26)]), requires_grad=True)\n",
    "beta2_v = Variable(torch.Tensor([inv_sigm(.1)]), requires_grad=True)\n",
    "theta1_v = Variable(torch.Tensor([inv_sigm(1/3)]), requires_grad=True)\n",
    "theta2_v = Variable(torch.Tensor([inv_sigm(.5)]), requires_grad=True)\n",
    "\n",
    "tA_1_pos = np.where(tA_1 == 1)[0]\n",
    "tA_1_neg = np.where(tA_1 == 0)[0]\n",
    "tB_1_pos = np.where(tB_1 == 1)[0]\n",
    "tB_1_neg = np.where(tB_1 == 0)[0]\n",
    "n_tA_1_pos, n_tA_1_neg = len(tA_1_pos), len(tA_1_neg)\n",
    "n_tB_1_pos, n_tB_1_neg = len(tB_1_pos), len(tB_1_neg)\n",
    "\n",
    "n_pos_pos_1 = len(set(tA_1_pos).intersection(tB_1_pos))\n",
    "n_pos_neg_1 = len(set(tA_1_pos).intersection(tB_1_neg))\n",
    "n_neg_pos_1 = len(set(tA_1_neg).intersection(tB_1_pos))\n",
    "n_neg_neg_1 = len(set(tA_1_neg).intersection(tB_1_neg))\n",
    "\n",
    "tA_2_pos = np.where(tA_2 == 1)[0]\n",
    "tA_2_neg = np.where(tA_2 == 0)[0]\n",
    "tB_2_pos = np.where(tB_2 == 1)[0]\n",
    "tB_2_neg = np.where(tB_2 == 0)[0]\n",
    "n_tA_2_pos, n_tA_2_neg = len(tA_2_pos), len(tA_2_neg)\n",
    "n_tB_2_pos, n_tB_2_neg = len(tB_2_pos), len(tB_2_neg)\n",
    "\n",
    "n_pos_pos_2 = len(set(tA_2_pos).intersection(tB_2_pos))\n",
    "n_pos_neg_2 = len(set(tA_2_pos).intersection(tB_2_neg))\n",
    "n_neg_pos_2 = len(set(tA_2_neg).intersection(tB_2_pos))\n",
    "n_neg_neg_2 = len(set(tA_2_neg).intersection(tB_2_neg))\n",
    "\n",
    "print(n_pos_pos_1, n_pos_neg_1, n_neg_pos_1, n_neg_neg_1)\n",
    "print(n_pos_pos_2, n_pos_neg_2, n_neg_pos_2, n_neg_neg_2)\n",
    "\n",
    "\n",
    "learning_rate = .001\n",
    "### I've played around with fixing certain variables, optimizing others, optimizing all: \n",
    "optim = torch.optim.Adam([alpha1_v, alpha2_v, beta1_v, beta2_v, theta1_v, theta2_v], lr = learning_rate)\n",
    "# optim = torch.optim.Adam([theta2_v, theta1_v, beta1_v,  beta2_v], lr = learning_rate)\n",
    "\n",
    "losses = []\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    optim.zero_grad()\n",
    "    alpha1, alpha2 = torch.sigmoid(alpha1_v), torch.sigmoid(alpha2_v)\n",
    "    beta1, beta2 = torch.sigmoid(beta1_v), torch.sigmoid(beta2_v)\n",
    "    theta1, theta2 = torch.sigmoid(theta1_v), torch.sigmoid(theta2_v)\n",
    "    n_1 = len(tA_1)\n",
    "    n_2 = len(tA_2)\n",
    "    \n",
    "    l = n_pos_pos_1*torch.log(((theta1 * (1-beta1) * (1 - beta2)) + ((1-theta1)*alpha1*alpha2)))/n_1\n",
    "    l += n_pos_neg_1*torch.log((theta1*(1-beta1)*beta2) + ((1-theta1)*alpha1*(1-alpha2)))/n_1\n",
    "    l += n_neg_pos_1*torch.log(theta1*beta1*(1-beta2) +  (1-theta1)*(1-alpha1)*alpha2)/n_1\n",
    "    l += n_neg_neg_1*torch.log(theta1*beta1*beta2 + (1-theta1)*(1-alpha1)*(1-alpha2))/n_1\n",
    "\n",
    "    l += n_pos_pos_2*torch.log(((theta2 * (1-beta1) * (1 - beta2)) + ((1-theta2)*alpha1*alpha2)))/n_2\n",
    "    l += n_pos_neg_2*torch.log((theta2*(1-beta1)*beta2) + ((1-theta2)*alpha1*(1-alpha2)))/n_2\n",
    "    l += n_neg_pos_2*torch.log(theta2*beta1*(1-beta2) +  (1-theta2)*(1-alpha1)*alpha2)/n_2\n",
    "    l += n_neg_neg_2*torch.log(theta2*beta1*beta2 + (1-theta2)*(1-alpha1)*(1-alpha2))/n_2\n",
    "    \n",
    "    loss = -l\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    if  epoch % log_interval == 0:\n",
    "        print(\"Test A alpha: \", alpha1.item(), \"\\tbeta: \", beta1.item())\n",
    "        print(\"Test B alpha: \", alpha2.item(), \"\\tbeta: \", beta2.item())\n",
    "        print(\"Theta1: \", theta1.item(), \"Theta2: \", theta2.item())\n",
    "        print()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "print(\"True A alpha1: \",np.mean(tA_1[y1 == 0]), '\\t beta: ', 1-np.mean(tA_1[y1 == 1]))\n",
    "print(\"True B alpha2: \", np.mean(tB_1[y1 == 0]), '\\t beta: ',  1 - np.mean(tB_1[y1 == 1]))\n",
    "\n",
    "print(\"True theta1: \", np.mean(y1), \"\\t  theta2: \", np.mean(y2))\n",
    "duration = time.time() - start\n",
    "print(\"Job duration: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7961432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12658 0 0 0\n",
      "687 0 2555 14100\n",
      "Test A alpha:  0.10000000149011612 \tbeta:  0.10000000149011612\n",
      "Test B alpha:  0.10000000149011612 \tbeta:  0.10000000149011612\n",
      "Test C alpha:  0.8100000023841858 \tbeta:  0.005999999586492777\n",
      "Theta1:  0.3333333432674408\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ddmg/prism/.conda/envs/frank/lib/python3.9/site-packages/torch/optim/adam.py:33: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  super().__init__(params, defaults)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test A alpha:  0.012054597027599812 \tbeta:  0.000975200382526964\n",
      "Test B alpha:  0.16339828073978424 \tbeta:  0.0031913816928863525\n",
      "Test C alpha:  0.046702791005373 \tbeta:  0.005999999586492777\n",
      "Theta1:  0.43820613622665405\n",
      "\n",
      "Test A alpha:  0.011768273077905178 \tbeta:  0.00022631694446317852\n",
      "Test B alpha:  0.16338802874088287 \tbeta:  0.0009912565583363175\n",
      "Test C alpha:  0.013472754508256912 \tbeta:  0.005999999586492777\n",
      "Theta1:  0.4382098913192749\n",
      "\n",
      "Test A alpha:  0.012030350044369698 \tbeta:  7.244275911943987e-05\n",
      "Test B alpha:  0.1635950356721878 \tbeta:  0.0004451171844266355\n",
      "Test C alpha:  0.005899460520595312 \tbeta:  0.005999999586492777\n",
      "Theta1:  0.4380711615085602\n",
      "\n",
      "Test A alpha:  0.012111231684684753 \tbeta:  2.5434730559936725e-05\n",
      "Test B alpha:  0.16366130113601685 \tbeta:  0.00023058652004692703\n",
      "Test C alpha:  0.003020346397534013 \tbeta:  0.005999999586492777\n",
      "Theta1:  0.4380268156528473\n",
      "\n",
      "Test A alpha:  0.012145968154072762 \tbeta:  9.213799785356969e-06\n",
      "Test B alpha:  0.16369029879570007 \tbeta:  0.00012806142331101\n",
      "Test C alpha:  0.0016667393501847982 \tbeta:  0.005999999586492777\n",
      "Theta1:  0.4380072057247162\n",
      "\n",
      "Test A alpha:  0.012163642793893814 \tbeta:  3.375599817445618e-06\n",
      "Test B alpha:  0.16370508074760437 \tbeta:  7.383476622635499e-05\n",
      "Test C alpha:  0.0009573833667673171 \tbeta:  0.005999999586492777\n",
      "Theta1:  0.4379972815513611\n",
      "\n",
      "Test A alpha:  0.01217332947999239 \tbeta:  1.2417654033924919e-06\n",
      "Test B alpha:  0.163713276386261 \tbeta:  4.348132279119454e-05\n",
      "Test C alpha:  0.0005625160411000252 \tbeta:  0.005999999586492777\n",
      "Theta1:  0.4379916489124298\n",
      "\n",
      "Test A alpha:  0.0121788764372468 \tbeta:  4.577859158416686e-07\n",
      "Test B alpha:  0.16371795535087585 \tbeta:  2.5923909561242908e-05\n",
      "Test C alpha:  0.0003348926256876439 \tbeta:  0.005999999586492777\n",
      "Theta1:  0.43798866868019104\n",
      "\n",
      "Test A alpha:  0.012182105332612991 \tbeta:  1.6903246091715118e-07\n",
      "Test B alpha:  0.16372071206569672 \tbeta:  1.556908682687208e-05\n",
      "Test C alpha:  0.00020094169303774834 \tbeta:  0.005999999586492777\n",
      "Theta1:  0.4379870295524597\n",
      "\n",
      "True A alpha1:  0.21995 \t beta:  0.10540000000000005\n",
      "True B alpha2:  0.324 \t beta:  0.05800000000000005\n",
      "True C alpha3:  0.1942 \t beta:  0.12260000000000004\n",
      "True theta1:  0.3333333333333333\n",
      "Job duration:  14.068296670913696\n"
     ]
    }
   ],
   "source": [
    "## 3 models, 1 population\n",
    "n_epochs = 10000\n",
    "log_interval = 1000\n",
    "start = time.time()\n",
    "\n",
    "alpha1_v = Variable(torch.Tensor([inv_sigm(.1)]), requires_grad=True)\n",
    "alpha2_v = Variable(torch.Tensor([inv_sigm(.1)]), requires_grad=True)\n",
    "alpha3_v = Variable(torch.Tensor([inv_sigm(.81)]), requires_grad=True)\n",
    "beta1_v = Variable(torch.Tensor([inv_sigm(.1)]), requires_grad=True)\n",
    "beta2_v = Variable(torch.Tensor([inv_sigm(.1)]), requires_grad=True)\n",
    "beta3_v = Variable(torch.Tensor([inv_sigm(.006)]), requires_grad=True)\n",
    "theta1_v = Variable(torch.Tensor([inv_sigm(1/3)]), requires_grad=True)\n",
    "\n",
    "tA_1_pos = np.where(tA_1 == 1)[0]\n",
    "tA_1_neg = np.where(tA_1 == 0)[0]\n",
    "tB_1_pos = np.where(tB_1 == 1)[0]\n",
    "tB_1_neg = np.where(tB_1 == 0)[0]\n",
    "tC_1_pos = np.where(tC_1 == 1)[0]\n",
    "tC_1_neg = np.where(tC_1 == 0)[0]\n",
    "n_tA_1_pos, n_tA_1_neg = len(tA_1_pos), len(tA_1_neg)\n",
    "n_tB_1_pos, n_tB_1_neg = len(tB_1_pos), len(tB_1_neg)\n",
    "\n",
    "n_pos_pos_pos_1 = len(set(tA_1_pos).intersection(tB_1_pos).intersection(tC_1_pos))\n",
    "n_pos_pos_neg_1 = len(set(tA_1_pos).intersection(tB_1_pos).intersection(tC_1_neg))\n",
    "\n",
    "n_pos_neg_pos_1 = len(set(tA_1_pos).intersection(tB_1_neg).intersection(tC_1_pos))\n",
    "n_pos_neg_neg_1 = len(set(tA_1_pos).intersection(tB_1_neg).intersection(tC_1_neg))\n",
    "\n",
    "n_neg_pos_pos_1 = len(set(tA_1_neg).intersection(tB_1_pos).intersection(tC_1_pos))\n",
    "n_neg_pos_neg_1 = len(set(tA_1_neg).intersection(tB_1_pos).intersection(tC_1_neg))\n",
    "\n",
    "n_neg_neg_pos_1 = len(set(tA_1_neg).intersection(tB_1_neg).intersection(tC_1_pos))\n",
    "n_neg_neg_neg_1 = len(set(tA_1_neg).intersection(tB_1_neg).intersection(tC_1_neg))\n",
    "\n",
    "print(n_pos_pos_pos_1, n_pos_neg_pos_1, n_neg_pos_pos_1, n_neg_neg_pos_1)\n",
    "print(n_pos_pos_neg_1, n_pos_neg_neg_1, n_neg_pos_neg_1, n_neg_neg_neg_1)\n",
    "\n",
    "\n",
    "learning_rate = .01\n",
    "optim = torch.optim.Adam([theta1_v, beta3_v, beta2_v, beta1_v, alpha3_v, alpha2_v, alpha1_v], lr = learning_rate)\n",
    "optim = torch.optim.Adam([theta1_v, beta1_v, beta2_v, beta1_v, alpha3_v,  alpha2_v, alpha1_v], lr = learning_rate)\n",
    "\n",
    "losses, beta2s = [], []\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    optim.zero_grad()\n",
    "    alpha1, alpha2, alpha3 = torch.sigmoid(alpha1_v), torch.sigmoid(alpha2_v), torch.sigmoid(alpha3_v)\n",
    "    beta1, beta2, beta3 = torch.sigmoid(beta1_v), torch.sigmoid(beta2_v), torch.sigmoid(beta3_v)\n",
    "    theta1 = torch.sigmoid(theta1_v)\n",
    "    n_1 = len(tA_1)\n",
    "    n_2 = len(tA_2)\n",
    "    l = n_pos_pos_pos_1*torch.log(((theta1 * (1-beta1) * (1-beta2) * (1-beta3)) + ((1-theta1)*alpha1*alpha2*alpha3)))/n_1\n",
    "    l += n_pos_pos_neg_1*torch.log(((theta1 * (1-beta1) * (1-beta2) * (beta3)) + ((1-theta1)*alpha1*alpha2*(1-alpha3))))/n_1\n",
    "\n",
    "    l += n_pos_neg_pos_1*torch.log((theta1*(1-beta1)*beta2*(1-beta3)) + ((1-theta1)*alpha1*(1-alpha2)*alpha3))/n_1\n",
    "    l += n_pos_neg_neg_1*torch.log((theta1*(1-beta1)*beta2*(beta3)) + ((1-theta1)*alpha1*(1-alpha2)*(1-alpha3)))/n_1\n",
    "\n",
    "    l += n_neg_pos_pos_1*torch.log(theta1*beta1*(1-beta2)*(1-beta3) +  (1-theta1)*(1-alpha1)*alpha2*alpha3)/n_1\n",
    "    l += n_neg_pos_neg_1*torch.log(theta1*beta1*(1-beta2)*(beta3) +  (1-theta1)*(1-alpha1)*alpha2*(1-alpha3))/n_1\n",
    "    \n",
    "    l += n_neg_neg_pos_1*torch.log(theta1*beta1*beta2*(1-beta3) + (1-theta1)*(1-alpha1)*(1-alpha2)*alpha3)/n_1\n",
    "    l += n_neg_neg_neg_1*torch.log(theta1*beta1*beta2*beta3 + (1-theta1)*(1-alpha1)*(1-alpha2)*(1-alpha3))/n_1\n",
    "\n",
    "    \n",
    "    loss = -l\n",
    "    loss.backward()\n",
    "    \n",
    "    optim.step()\n",
    "    if  epoch % log_interval == 0:\n",
    "        print(\"Test A alpha: \", alpha1.item(), \"\\tbeta: \", beta1.item())\n",
    "        print(\"Test B alpha: \", alpha2.item(), \"\\tbeta: \", beta2.item())\n",
    "        print(\"Test C alpha: \", alpha3.item(), \"\\tbeta: \", beta3.item())\n",
    "        print(\"Theta1: \", theta1.item())\n",
    "        print()\n",
    "    beta2s.append(beta2.item())\n",
    "    losses.append(loss.item())\n",
    "\n",
    "print(\"True A alpha1: \",np.mean(tA_1[y1 == 0]), '\\t beta: ', 1-np.mean(tA_1[y1 == 1]))\n",
    "print(\"True B alpha2: \", np.mean(tB_1[y1 == 0]), '\\t beta: ',  1 - np.mean(tB_1[y1 == 1]))\n",
    "print(\"True C alpha3: \", np.mean(tC_1[y1 == 0]), '\\t beta: ',  1 - np.mean(tC_1[y1 == 1]))\n",
    "\n",
    "print(\"True theta1: \", np.mean(y1))\n",
    "duration = time.time() - start\n",
    "print(\"Job duration: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7697ea3",
   "metadata": {},
   "source": [
    "### Experiments - No Labeled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "179a2411",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a04c90a9fa704cfcbde50870e09e3875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We can't get accurate estimates of alpha or beta for any classifier without additional assumptions about the errors. \n",
    "# Also, the assumption that the errors are independent  of x is broken.\n",
    "# Can we get an ordering of classifiers if we know 1 classifiers specificity?\n",
    "# But maybe you can get an ordering of classifiers on your data\n",
    "import itertools\n",
    "from tqdm.notebook import tqdm\n",
    "# Sample different alpha, beta, and theta values \n",
    "# Train different models \n",
    "\n",
    "alpha_opts = [.1, .2, .3]\n",
    "beta_opts = [.1, .2, .3]\n",
    "theta_opts = [.1, .3, .5, .7]\n",
    "\n",
    "alpha_sets = []\n",
    "for i, alphaA in enumerate(alpha_opts):\n",
    "    if i > 1:\n",
    "        continue\n",
    "    for alphaB in alpha_opts[i+1:]:\n",
    "        for alphaC in alpha_opts[i+2:]:\n",
    "            alpha_sets.append((alphaA, alphaB, alphaC))\n",
    "beta_sets = []\n",
    "for i,betaA in enumerate(beta_opts):\n",
    "    for betaB in beta_opts[i+1:]:\n",
    "        for betaC in beta_opts[i+2:]:\n",
    "            if betaC == betaB:\n",
    "                continue\n",
    "            beta_sets.append((betaA, betaB, betaC))\n",
    "\n",
    "train_size = 10000\n",
    "test_size = 20000\n",
    "n_epochs = 10000\n",
    "results = []\n",
    "configs = list(itertools.product(alpha_sets, beta_sets, theta_opts))\n",
    "for alpha_set, beta_set, true_theta in tqdm(configs):\n",
    "    true_alphaA, true_alphaB, true_alphaC = alpha_set\n",
    "    true_betaA, true_betaB, true_betaC = beta_set\n",
    "    \n",
    "    pos = np.random.multivariate_normal((1, 1), [[1, 1], [1, 1]], size=train_size)\n",
    "    neg = np.random.multivariate_normal((-1, -1), [[1, 1], [1, 1]], size=train_size)\n",
    "    x_train = np.concatenate([pos, neg], axis=0)\n",
    "    y_train = np.concatenate([np.ones(train_size), np.zeros(train_size)], axis=0)\n",
    "    models = []\n",
    "    \n",
    "    for m_params in [(true_alphaA, true_betaA), \n",
    "                     (true_alphaB, true_betaB), \n",
    "                     (true_alphaC, true_betaC)]:\n",
    "        xM_train, yM_train = x_train.copy(), apply_alpha_beta(y_train, m_params[0], m_params[1])\n",
    "        lrM = LogisticRegression(penalty='none')\n",
    "        lrM.fit(xM_train, yM_train)\n",
    "        models.append(lrM)\n",
    "\n",
    "    # Create test population\n",
    "    pos_1 = np.random.multivariate_normal((1, 1), [[1, 1], [1, 1]], size=int(true_theta*test_size))\n",
    "    neg_1 = np.random.multivariate_normal((-1, -1), [[1, 1], [1, 1]], size=int((1-true_theta)*test_size))\n",
    "    \n",
    "    x1 = np.concatenate([pos_1, neg_1], axis=0)\n",
    "    y1 = np.concatenate([np.ones(pos_1.shape[0]), np.zeros(neg_1.shape[0])])\n",
    "    \n",
    "    model_preds = [gibbs_classification(model, x1) for model in models]\n",
    "    nPPP, nPPN, nPNP, nPNN,  nNPP, nNPN, nNNP, nNNN = get_agreements(*model_preds)\n",
    "\n",
    "    # alpha_ranking\n",
    "    model_names = ['A', 'B', 'C']\n",
    "    true_alpha_ranking = [model_names[i] for i in np.argsort([true_alphaA, true_alphaB, true_alphaC])]\n",
    "    true_beta_ranking = [model_names[i] for i in np.argsort([true_betaA, true_betaB, true_betaC])]\n",
    "    \n",
    "    alphaA_v, alphaB_v, alphaC_v = [Variable(torch.Tensor([inv_sigm(.5)]), requires_grad=True) for i in range(3)]\n",
    "    betaA_v, betaB_v, betaC_v = [Variable(torch.Tensor([inv_sigm(.5)]), requires_grad=True) for i in range(3)]\n",
    "    theta1_v = Variable(torch.Tensor([inv_sigm(.5)]), requires_grad=True)\n",
    "    \n",
    "    # We know betaC_v, or the \n",
    "    betaC_v.requires_grad = False\n",
    "    betaC_v[0] = inv_sigm(1 - np.mean(model_preds[2][y1==1]))\n",
    "    betaC_v.requires_grad = False\n",
    "    learning_rate = .01\n",
    "    optim = torch.optim.Adam([theta1_v, betaA_v, betaB_v, alphaA_v, alphaB_v, alphaC_v], lr = learning_rate)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        optim.zero_grad()\n",
    "        alphaA, alphaB, alphaC = torch.sigmoid(alphaA_v), torch.sigmoid(alphaB_v), torch.sigmoid(alphaC_v)\n",
    "        betaA, betaB, betaC = torch.sigmoid(betaA_v), torch.sigmoid(betaB_v), torch.sigmoid(betaC_v)\n",
    "        theta1 = torch.sigmoid(theta1_v)\n",
    "        n_1 = len(model_preds[0])\n",
    "        l = nPPP*torch.log(((theta1 * (1-betaA) * (1-betaB) * (1-betaC)) + ((1-theta1)*alphaA*alphaB*alphaC)))/n_1\n",
    "        l += nPPN*torch.log(((theta1 * (1-betaA) * (1-betaB) * (betaC)) + ((1-theta1)*alphaA*alphaB*(1-alphaC))))/n_1\n",
    "\n",
    "        l += nPNP*torch.log((theta1*(1-betaA)*betaB*(1-betaC)) + ((1-theta1)*alphaA*(1-alphaB)*alphaC))/n_1\n",
    "        l += nPNN*torch.log((theta1*(1-betaA)*betaB*(betaC)) + ((1-theta1)*alphaA*(1-alphaB)*(1-alphaC)))/n_1\n",
    "\n",
    "        l += nNPP*torch.log(theta1*betaA*(1-betaB)*(1-betaC) +  (1-theta1)*(1-alphaA)*alphaB*alphaC)/n_1\n",
    "        l += nNPN*torch.log(theta1*betaA*(1-betaB)*(betaC) +  (1-theta1)*(1-alphaA)*alphaB*(1-alphaC))/n_1\n",
    "\n",
    "        l += nNNP*torch.log(theta1*betaA*betaB*(1-betaC) + (1-theta1)*(1-alphaA)*(1-alphaB)*alphaC)/n_1\n",
    "        l += nNNN*torch.log(theta1*betaA*betaB*betaC + (1-theta1)*(1-alphaA)*(1-alphaB)*(1-alphaC))/n_1\n",
    "\n",
    "        loss = -l\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    \n",
    "    result = {'pred_alphaA': alphaA.item(), 'pred_alphaB': alphaB.item(), 'pred_alphaC': alphaC.item(),\n",
    "              'pred_betaA': betaA.item(), 'pred_betaB': betaB.item(), 'pred_betaC': betaC.item(),\n",
    "              'pred_theta1': theta1.item(),\n",
    "              'true_alphaA': true_alphaA, 'true_alphaB': true_alphaB, 'true_alphaC': true_alphaC,\n",
    "              'true_betaA': true_betaA, 'true_betaB': true_betaB, 'true_betaC': true_betaC, 'true_theta1': true_theta}\n",
    "    result['true_alpha_ranking'] = true_alpha_ranking\n",
    "    result['true_beta_ranking'] = true_beta_ranking\n",
    "    result['pred_alpha_ranking'] = [model_names[i] for i in np.argsort([result['pred_alphaA'], result['pred_alphaB'],\n",
    "                                                           result['pred_alphaB']])]\n",
    "    result['pred_beta_ranking'] = [model_names[i] for i in np.argsort([result['pred_betaA'], result['pred_betaB'],\n",
    "                                                           result['pred_betaB']])]\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "#  % of time our algorithm ranks the correct method as 1st\n",
    "# % of time our algorithm ranks the correct method as 2nd\n",
    "# % of time our algorithm ranks the correct method as 3rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "485bf99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "for quantity in ['alphaA', 'alphaB', 'alphaC', 'betaA', 'betaB', 'betaC']:\n",
    "    results_df[quantity + '_err'] = results_df['true_' + quantity] - results_df['pred_' + quantity]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53faf861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluating rankings based on sensitivity and specificity\n",
    "print(np.mean(results_df['true_beta_ranking'] == results_df['pred_beta_ranking']))\n",
    "print(np.mean(results_df['true_alpha_ranking'] == results_df['pred_alpha_ranking']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "17ee3ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['correct_beta_order'] = results_df['true_beta_ranking'] ==  results_df['pred_beta_ranking']\n",
    "results_df['correct_alpha_order'] = results_df['true_alpha_ranking'] ==  results_df['pred_alpha_ranking']\n",
    "results_df['mean_alpha'] = (results_df['true_alphaA'] + results_df['true_alphaB'] + results_df['true_alphaC'])/3\n",
    "results_df['mean_beta'] = (results_df['true_betaA'] + results_df['true_betaB'] + results_df['true_betaC'])/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a91bde6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='true_betaC', ylabel='alphaA_err'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "sns.pointplot(x='true_betaC', y='alphaA_err', data=results_df, join=False)\n",
    "# What % of the top ranked model in terms of alpha is correct?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9109f6",
   "metadata": {},
   "source": [
    "### Improvements\n",
    "\n",
    "- Incorporating outputted probabilities?\n",
    "- Modeling how errors are likely correlated?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
