{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "import pyro.distributions as dist\n",
    "import pyro.distributions.constraints as constraints\n",
    "from pyro.infer import MCMC, NUTS, Predictive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "\n",
    "def corrected_dawid_skene_model_with_observed(annotations, observed_z, mask):\n",
    "    # print(\"observed_z\", observed_z.shape)\n",
    "    # print(\"observed_z\", observed_z[:5])\n",
    "    # print(\"mask\", mask)\n",
    "    # mask =   # Add a new dimension at the beginning\n",
    "       \n",
    "    # print(\"simulating...\\r\")\n",
    "    num_items, num_raters = annotations.shape\n",
    "    num_classes = len(torch.unique(annotations))\n",
    "    # mask_repeated = mask.unsqueeze(0).repeat(num_raters, 1)\n",
    "    # observed_z_repeated = observed_z.unsqueeze(0).repeat(num_raters, 1)  \n",
    "    \n",
    "    # Prior for the class proportions\n",
    "    pi = pyro.sample(\"pi\", dist.Dirichlet(torch.ones(num_classes)))\n",
    "\n",
    "    # Priors for the annotator confusion matrices\n",
    "    with pyro.plate(\"raters\", num_raters):\n",
    "        theta = pyro.sample(\"theta\", dist.Dirichlet(0.5 * torch.eye(num_classes) + 0.25).to_event(1))\n",
    "\n",
    "\n",
    "    with pyro.plate(\"items\", num_items):\n",
    "        \n",
    "        # with pyro.poutine.mask(mask=mask):\n",
    "        z = pyro.sample(\"z\", dist.Categorical(pi), obs=observed_z, obs_mask=mask)\n",
    "            # z = pyro.sample(\"z\", dist.Categorical(pi))\n",
    "        # z = pyro.sample(\"z\", dist.Categorical(pi).mask(mask).to_event(1), obs=observed_z)\n",
    "        pdb.set_trace()\n",
    "        # print(\"observed_z\", observed_z[:5])\n",
    "        # print(\"z shape\", z.shape)\n",
    "        \n",
    "        # Condition on the observed values using a mask\n",
    "        # z = torch.where(mask, observed_z, z)\n",
    "        \n",
    "        for r in pyro.plate(\"raters_loop\", num_raters):\n",
    "            probs = theta[r, z, :]\n",
    "            pyro.sample(f\"y_{r}\", dist.Categorical(probs), obs=annotations[:, r])\n",
    "\n",
    "    # observed_annotations = annotations[mask]\n",
    "    # unobserved_annotations = annotations[~mask]\n",
    "    \n",
    "    # observed_z_values = observed_z[mask]\n",
    "    \n",
    "    # # # For observed items\n",
    "    # with pyro.plate(\"observed_items\", observed_annotations.size(0)):\n",
    "    #     # Condition on the observed z values\n",
    "    #     z_observed = pyro.sample(\"z_observed\", dist.Categorical(pi), obs=observed_z_values)\n",
    "        \n",
    "    #     for r in pyro.plate(\"raters_observed\", num_raters):\n",
    "    #         probs_observed = theta[r, z_observed, :]\n",
    "    #         pyro.sample(f\"y_observed_{r}\", dist.Categorical(probs_observed), obs=observed_annotations[:, r])\n",
    "\n",
    "    # # For unobserved items\n",
    "    # with pyro.plate(\"unobserved_items\", unobserved_annotations.size(0)):\n",
    "    #     z_unobserved = pyro.sample(\"z_unobserved\", dist.Categorical(pi))\n",
    "        \n",
    "    #     for r in pyro.plate(\"raters_unobserved\", num_raters):\n",
    "    #         probs_unobserved = theta[r, z_unobserved, :]\n",
    "    #         pyro.sample(f\"y_unobserved_{r}\", dist.Categorical(probs_unobserved), obs=unobserved_annotations[:, r])\n",
    "\n",
    "def simulate_data_with_observed(num_items, num_raters, true_pi, confusion_diagonal):\n",
    "    num_classes = len(true_pi)\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    off_diag = (1 - confusion_diagonal) / (num_classes - 1)\n",
    "    confusion_matrix = off_diag * torch.ones(num_classes, num_classes) + \\\n",
    "                       (confusion_diagonal - off_diag) * torch.eye(num_classes)\n",
    "    \n",
    "    # Simulate true labels\n",
    "    z = dist.Categorical(true_pi).sample([num_items])\n",
    "    \n",
    "    # Create the observed_z tensor\n",
    "    observed_z = torch.full((num_items,), -1, dtype=torch.int64)\n",
    "    mask = torch.zeros(num_items, dtype=torch.bool)\n",
    "    \n",
    "    # Randomly select 10% of the items to reveal their true labels\n",
    "    num_observed = int(0.5 * num_items)\n",
    "    observed_indices = torch.randperm(num_items)[:num_observed]\n",
    "    observed_z[observed_indices] = z[observed_indices]\n",
    "    mask[observed_indices] = True\n",
    "    \n",
    "    # Simulate annotations based on true labels and confusion matrices\n",
    "    annotations = torch.zeros((num_items, num_raters), dtype=torch.int64)\n",
    "    for i in range(num_items):\n",
    "        for r in range(num_raters):\n",
    "            probs = confusion_matrix[:, z[i]]\n",
    "            annotations[i, r] = dist.Categorical(probs).sample()\n",
    "    \n",
    "    return annotations, observed_z, mask\n",
    "# dummy_annotations = torch.randint(0, 3, (100, 5))\n",
    "# pyro.render_model(corrected_dawid_skene_model_with_observed, model_args=(dummy_annotations, dummy_annotations[:, 0], np.ones(1000)), render_params=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warmup:   0%|          | 0/1500 [04:43, ?it/s]\n",
      "Warmup:   0%|          | 0/1500 [04:04, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling! None {} ()\n",
      "sampling! None {} ()\n",
      "sampling! tensor([ True, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False]) {} ()\n",
      "sampling! None {} ()\n",
      "sampling! None {} ()\n",
      "sampling! None {'infer': {'_deterministic': True}} ()\n",
      "> \u001b[0;32m/tmp/ipykernel_513114/3512955596.py\u001b[0m(36)\u001b[0;36mcorrected_dawid_skene_model_with_observed\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     34 \u001b[0;31m        \u001b[0;31m# z = torch.where(mask, observed_z, z)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     35 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 36 \u001b[0;31m        \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpyro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"raters_loop\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_raters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     37 \u001b[0;31m            \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     38 \u001b[0;31m            \u001b[0mpyro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"y_{r}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mannotations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "tensor([-1,  2,  2,  1,  1,  1,  0,  1,  0,  1,  2,  1,  1,  1,  1,  1,  1,  1,\n",
      "         0,  1,  2,  1,  0,  0,  1,  1,  1,  1,  2,  1,  2,  1,  1,  1,  1,  0,\n",
      "         1,  0,  2,  1,  1,  1,  0,  1,  1,  1,  1,  2,  2,  1,  2,  2,  1,  0,\n",
      "         1,  0,  2,  1,  2,  2,  0,  1,  1,  1,  1,  1,  0,  1,  1,  2,  1,  1,\n",
      "         1,  1,  2,  1,  1,  1,  1,  2,  1,  0,  1,  2,  0,  0,  1,  1,  1,  2,\n",
      "         1,  1,  0,  1,  1,  2,  0,  1,  0,  0,  1,  1,  1,  1,  0,  0,  2,  1,\n",
      "         2,  1,  2,  0,  1,  1,  1,  2,  2,  2,  2,  1,  0,  2,  0,  0,  1,  0,\n",
      "         1,  2,  1,  1,  2,  0,  1,  0,  0,  1,  2,  1,  0,  0,  1,  1,  0,  1,\n",
      "         1,  1,  1,  1,  2,  2,  0,  2,  0,  1,  1,  2,  2,  2,  2,  2,  0,  0,\n",
      "         1,  0,  1,  1,  1,  2,  1,  2,  1,  2,  2,  2,  1,  0,  2,  2,  0,  1,\n",
      "         1,  2,  1,  2,  1,  2,  1,  0,  1,  1,  0,  1,  1,  1,  1,  0,  2,  0,\n",
      "         2,  0,  0,  0,  1,  2,  1,  2,  0,  0,  2,  1,  2,  1,  1,  1,  2,  0,\n",
      "         1,  2,  2,  0,  1,  2,  2,  2,  2,  2,  1,  0,  2,  1,  1,  2,  1,  1,\n",
      "         1,  1,  0,  0,  0,  1,  1,  1,  1,  1,  0,  2,  0,  0,  0,  1,  0,  1,\n",
      "         2,  1,  0,  1,  1,  0,  2,  1,  1,  1,  2,  1,  1,  2,  1,  1,  1,  2,\n",
      "         2,  1,  1,  1,  1,  1,  1,  0,  1,  0,  2,  1,  1,  1,  2,  2,  0,  1,\n",
      "         2,  2,  1,  1,  1,  1,  1,  2,  1,  1,  1,  0,  1,  2,  0,  0,  0,  1,\n",
      "         1,  1,  0,  2,  1,  1,  1,  2,  0,  1,  1,  1,  1,  2,  0,  0,  2,  0,\n",
      "         0,  1,  1,  2,  1,  2,  2,  1,  2,  1,  2,  1,  2,  0,  0,  2,  1,  2,\n",
      "         1,  1,  1,  0,  2,  2,  1,  1,  2,  2,  0,  0,  1,  2,  2,  1,  1,  1,\n",
      "         2,  1,  2,  0,  0,  2,  0,  0,  0,  0,  1,  2,  2,  1,  2,  1,  0,  1,\n",
      "         2,  1,  1,  1,  0,  1,  1,  1,  2,  2,  0,  1,  1,  2,  1,  1,  1,  2,\n",
      "         1,  1,  0,  1,  2,  2,  1,  2,  0,  0,  1,  2,  1,  0,  1,  0,  2,  1,\n",
      "         1,  1,  1,  2,  0,  1,  2,  1,  1,  0,  0,  0,  0,  2,  0,  1,  1,  1,\n",
      "         1,  2,  1,  2,  1,  0,  1,  1,  0,  0,  0,  2,  2,  2,  1,  2,  2,  1,\n",
      "         1,  1,  1,  1,  2,  1,  1,  2,  1,  0,  1,  1,  1,  1,  1,  1,  1,  0,\n",
      "         1,  0,  0,  0,  1,  1,  1,  1,  0,  1,  1,  2,  2,  1,  2,  2,  1,  1,\n",
      "         2,  0,  0,  1,  2,  1,  2,  2,  1,  1,  1,  2,  2,  1,  2,  1,  1,  1,\n",
      "         2,  1,  0,  2,  1,  1,  1,  1,  1,  1,  1,  0,  0,  2,  2,  0,  1,  2,\n",
      "         1,  0,  2,  0,  1,  1,  0,  1,  1,  1,  1,  2,  0,  2,  2,  1,  1,  0,\n",
      "         1,  0,  1,  2,  1,  1,  1,  1,  2,  1,  1,  1,  2,  0,  2,  1,  2,  1,\n",
      "         1,  0,  0,  1,  0,  1,  1,  1,  0,  2,  1,  1,  1,  1,  0,  1,  1,  1,\n",
      "         2,  0,  0,  1,  1,  0,  1,  2,  1,  1,  1,  2,  2,  0,  1,  1,  1,  1,\n",
      "         1,  2,  0,  0,  2,  2,  2,  0,  2,  1,  1,  1,  1,  0,  1,  0,  0,  1,\n",
      "         1,  0,  1,  1,  2,  1,  1,  1,  2,  0,  0,  0,  0,  1,  1,  0,  2,  0,\n",
      "         1,  0,  2,  0,  0,  1,  0,  2,  2,  1,  2,  0,  1,  1,  0,  2,  1,  1,\n",
      "         0,  1,  1,  1,  1,  1,  1,  1,  0,  1,  1,  1,  2,  1,  1,  1,  1,  0,\n",
      "         2,  1,  1,  1,  0,  1,  0,  1,  2,  1,  1,  2,  1,  2,  0,  0,  2,  1,\n",
      "         1,  1,  1,  2,  1,  1,  1,  2,  1,  1,  1,  1,  0,  1,  0,  1,  1,  1,\n",
      "         1,  1,  2,  1,  2,  1,  0,  1,  0,  1,  0,  1,  1,  2,  2,  1,  1,  2,\n",
      "         1,  1,  1,  1,  1,  1,  1,  2,  1,  2,  1,  1,  0,  1,  2,  1,  1,  1,\n",
      "         0,  1,  0,  1,  2,  2,  0,  1,  1,  1,  1,  1,  0,  1,  1,  2,  0,  2,\n",
      "         0,  0,  2,  1,  2,  2,  1,  2,  1,  2,  1,  1,  1,  1,  1,  2,  1,  1,\n",
      "         2,  0,  1,  1,  2,  2,  1,  0,  1,  0,  2,  1,  1,  2,  1,  1,  1,  2,\n",
      "         0,  2,  2,  1,  1,  2,  0,  1,  2,  1,  0,  1,  1,  1,  1,  1,  1,  2,\n",
      "         1,  0,  2,  2,  1,  0,  0,  2,  0,  0,  1,  0,  1,  0,  1,  1,  0,  0,\n",
      "         2,  1,  2,  1,  2,  1,  0,  0,  0,  2,  1,  1,  1,  1,  2,  2,  2,  2,\n",
      "         1,  2,  0,  1,  0,  0,  2,  1,  1,  2,  2,  1,  1,  1,  1,  2,  2,  2,\n",
      "         2,  0,  2,  0,  1,  0,  1,  1,  0,  1,  1,  1,  1,  0,  2,  1,  1,  1,\n",
      "         0,  2,  1,  0,  1,  0,  1,  1,  1,  2,  0,  1,  2,  1,  1,  1,  1,  1,\n",
      "         0,  2,  1,  0,  1,  1,  1,  0,  1,  1,  2,  1,  2,  2,  0,  1,  0,  1,\n",
      "         1,  2,  2,  0,  2,  2,  1,  1,  1,  1,  1,  1,  1,  1,  0,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  0,  1,  2,  1,  1,  0,  1,  1,  2,  1,  1,  0,\n",
      "         2,  2,  2,  2,  1,  2,  0,  2,  1,  2,  1,  0,  2,  1,  0,  1,  2,  0,\n",
      "         2,  1,  1,  1,  1,  0,  0,  0,  1,  1,  1,  1,  1,  1,  2,  0,  0,  1,\n",
      "         1,  2,  0,  0,  0,  2,  1,  2,  1,  1])\n",
      "sampling! None {} ()\n",
      "sampling! None {} ()\n",
      "sampling! None {} ()\n",
      "sampling! None {} ()\n",
      "sampling! tensor([ True, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False]) {} ()\n",
      "sampling! None {} ()\n",
      "sampling! None {} ()\n",
      "sampling! None {'infer': {'_deterministic': True}} ()\n",
      "> \u001b[0;32m/tmp/ipykernel_513114/3512955596.py\u001b[0m(36)\u001b[0;36mcorrected_dawid_skene_model_with_observed\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     34 \u001b[0;31m        \u001b[0;31m# z = torch.where(mask, observed_z, z)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     35 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 36 \u001b[0;31m        \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpyro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"raters_loop\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_raters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     37 \u001b[0;31m            \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     38 \u001b[0;31m            \u001b[0mpyro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"y_{r}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mannotations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "sampling! None {} ()\n",
      "sampling! None {} ()\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "index -1 is out of bounds for dimension 1 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39m# print(observed_z[mask == False])\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39m# nuts_kernel = NUTS(conditioned_model, jit_compile=True, ignore_jit_warnings=True)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m mcmc \u001b[39m=\u001b[39m MCMC(nuts_kernel, num_samples\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m, warmup_steps\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m mcmc\u001b[39m.\u001b[39;49mrun(annotations_simulated, observed_z, mask)\n\u001b[1;32m     25\u001b[0m \u001b[39m# # Extract posterior samples\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39m# posterior_samples_corrected = mcmc.get_samples()\u001b[39;00m\n",
      "File \u001b[0;32m/data/ddmg/frank/.conda/envs/frank/lib/python3.9/site-packages/pyro/poutine/messenger.py:12\u001b[0m, in \u001b[0;36m_context_wrap\u001b[0;34m(context, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_context_wrap\u001b[39m(context, fn, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     11\u001b[0m     \u001b[39mwith\u001b[39;00m context:\n\u001b[0;32m---> 12\u001b[0m         \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/data/ddmg/frank/.conda/envs/frank/lib/python3.9/site-packages/pyro/infer/mcmc/api.py:563\u001b[0m, in \u001b[0;36mMCMC.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[39mwith\u001b[39;00m optional(\n\u001b[1;32m    555\u001b[0m     pyro\u001b[39m.\u001b[39mvalidation_enabled(\u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisable_validation),\n\u001b[1;32m    556\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisable_validation \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[39m# This also resolves \"RuntimeError: Cowardly refusing to serialize non-leaf tensor which\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[39m# requires_grad\", which happens with `jit_compile` under PyTorch 1.7\u001b[39;00m\n\u001b[1;32m    562\u001b[0m     args \u001b[39m=\u001b[39m [arg\u001b[39m.\u001b[39mdetach() \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mis_tensor(arg) \u001b[39melse\u001b[39;00m arg \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m args]\n\u001b[0;32m--> 563\u001b[0m     \u001b[39mfor\u001b[39;00m x, chain_id \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msampler\u001b[39m.\u001b[39mrun(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    564\u001b[0m         \u001b[39mif\u001b[39;00m num_samples[chain_id] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    565\u001b[0m             num_samples[chain_id] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/data/ddmg/frank/.conda/envs/frank/lib/python3.9/site-packages/pyro/infer/mcmc/api.py:223\u001b[0m, in \u001b[0;36m_UnarySampler.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m logger \u001b[39m=\u001b[39m initialize_logger(logger, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, progress_bar)\n\u001b[1;32m    222\u001b[0m hook_w_logging \u001b[39m=\u001b[39m _add_logging_hook(logger, progress_bar, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook)\n\u001b[0;32m--> 223\u001b[0m \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m _gen_samples(\n\u001b[1;32m    224\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel,\n\u001b[1;32m    225\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarmup_steps,\n\u001b[1;32m    226\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_samples,\n\u001b[1;32m    227\u001b[0m     hook_w_logging,\n\u001b[1;32m    228\u001b[0m     i \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_chains \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    229\u001b[0m     \u001b[39m*\u001b[39margs,\n\u001b[1;32m    230\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    231\u001b[0m ):\n\u001b[1;32m    232\u001b[0m     \u001b[39myield\u001b[39;00m sample, i  \u001b[39m# sample, chain_id\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel\u001b[39m.\u001b[39mcleanup()\n",
      "File \u001b[0;32m/data/ddmg/frank/.conda/envs/frank/lib/python3.9/site-packages/pyro/infer/mcmc/api.py:144\u001b[0m, in \u001b[0;36m_gen_samples\u001b[0;34m(kernel, warmup_steps, num_samples, hook, chain_id, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_gen_samples\u001b[39m(kernel, warmup_steps, num_samples, hook, chain_id, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 144\u001b[0m     kernel\u001b[39m.\u001b[39;49msetup(warmup_steps, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    145\u001b[0m     params \u001b[39m=\u001b[39m kernel\u001b[39m.\u001b[39minitial_params\n\u001b[1;32m    146\u001b[0m     save_params \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(kernel, \u001b[39m\"\u001b[39m\u001b[39msave_params\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39msorted\u001b[39m(params))\n",
      "File \u001b[0;32m/data/ddmg/frank/.conda/envs/frank/lib/python3.9/site-packages/pyro/infer/mcmc/hmc.py:345\u001b[0m, in \u001b[0;36mHMC.setup\u001b[0;34m(self, warmup_steps, *args, **kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_warmup_steps \u001b[39m=\u001b[39m warmup_steps\n\u001b[1;32m    344\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 345\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialize_model_properties(args, kwargs)\n\u001b[1;32m    346\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minitial_params:\n\u001b[1;32m    347\u001b[0m     z \u001b[39m=\u001b[39m {k: v\u001b[39m.\u001b[39mdetach() \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minitial_params\u001b[39m.\u001b[39mitems()}\n",
      "File \u001b[0;32m/data/ddmg/frank/.conda/envs/frank/lib/python3.9/site-packages/pyro/infer/mcmc/hmc.py:279\u001b[0m, in \u001b[0;36mHMC._initialize_model_properties\u001b[0;34m(self, model_args, model_kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_initialize_model_properties\u001b[39m(\u001b[39mself\u001b[39m, model_args, model_kwargs):\n\u001b[0;32m--> 279\u001b[0m     init_params, potential_fn, transforms, trace \u001b[39m=\u001b[39m initialize_model(\n\u001b[1;32m    280\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel,\n\u001b[1;32m    281\u001b[0m         model_args,\n\u001b[1;32m    282\u001b[0m         model_kwargs,\n\u001b[1;32m    283\u001b[0m         transforms\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransforms,\n\u001b[1;32m    284\u001b[0m         max_plate_nesting\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_max_plate_nesting,\n\u001b[1;32m    285\u001b[0m         jit_compile\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jit_compile,\n\u001b[1;32m    286\u001b[0m         jit_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jit_options,\n\u001b[1;32m    287\u001b[0m         skip_jit_warnings\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ignore_jit_warnings,\n\u001b[1;32m    288\u001b[0m         init_strategy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init_strategy,\n\u001b[1;32m    289\u001b[0m         initial_params\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initial_params,\n\u001b[1;32m    290\u001b[0m     )\n\u001b[1;32m    291\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpotential_fn \u001b[39m=\u001b[39m potential_fn\n\u001b[1;32m    292\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms \u001b[39m=\u001b[39m transforms\n",
      "File \u001b[0;32m/data/ddmg/frank/.conda/envs/frank/lib/python3.9/site-packages/pyro/infer/mcmc/util.py:455\u001b[0m, in \u001b[0;36minitialize_model\u001b[0;34m(model, model_args, model_kwargs, transforms, max_plate_nesting, jit_compile, jit_options, skip_jit_warnings, num_chains, init_strategy, initial_params)\u001b[0m\n\u001b[1;32m    452\u001b[0m     \u001b[39mif\u001b[39;00m automatic_transform_enabled:\n\u001b[1;32m    453\u001b[0m         transforms[name] \u001b[39m=\u001b[39m biject_to(node[\u001b[39m\"\u001b[39m\u001b[39mfn\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39msupport)\u001b[39m.\u001b[39minv\n\u001b[0;32m--> 455\u001b[0m trace_prob_evaluator \u001b[39m=\u001b[39m TraceEinsumEvaluator(\n\u001b[1;32m    456\u001b[0m     model_trace, has_enumerable_sites, max_plate_nesting\n\u001b[1;32m    457\u001b[0m )\n\u001b[1;32m    459\u001b[0m pe_maker \u001b[39m=\u001b[39m _PEMaker(\n\u001b[1;32m    460\u001b[0m     model, model_args, model_kwargs, trace_prob_evaluator, transforms\n\u001b[1;32m    461\u001b[0m )\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m initial_params \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/data/ddmg/frank/.conda/envs/frank/lib/python3.9/site-packages/pyro/infer/mcmc/util.py:183\u001b[0m, in \u001b[0;36mTraceEinsumEvaluator.__init__\u001b[0;34m(self, model_trace, has_enumerable_sites, max_plate_nesting)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enum_dims \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[1;32m    182\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mordering \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 183\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_populate_cache(model_trace)\n",
      "File \u001b[0;32m/data/ddmg/frank/.conda/envs/frank/lib/python3.9/site-packages/pyro/infer/mcmc/util.py:197\u001b[0m, in \u001b[0;36mTraceEinsumEvaluator._populate_cache\u001b[0;34m(self, model_trace)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_plate_nesting \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    193\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    194\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFinite value required for `max_plate_nesting` when model \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    195\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhas discrete (enumerable) sites.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    196\u001b[0m     )\n\u001b[0;32m--> 197\u001b[0m model_trace\u001b[39m.\u001b[39;49mcompute_log_prob()\n\u001b[1;32m    198\u001b[0m model_trace\u001b[39m.\u001b[39mpack_tensors()\n\u001b[1;32m    199\u001b[0m \u001b[39mfor\u001b[39;00m name, site \u001b[39min\u001b[39;00m model_trace\u001b[39m.\u001b[39mnodes\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[0;32m/data/ddmg/frank/.conda/envs/frank/lib/python3.9/site-packages/pyro/poutine/trace_struct.py:230\u001b[0m, in \u001b[0;36mTrace.compute_log_prob\u001b[0;34m(self, site_filter)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mlog_prob\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m site:\n\u001b[1;32m    229\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 230\u001b[0m         log_p \u001b[39m=\u001b[39m site[\u001b[39m\"\u001b[39;49m\u001b[39mfn\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mlog_prob(\n\u001b[1;32m    231\u001b[0m             site[\u001b[39m\"\u001b[39;49m\u001b[39mvalue\u001b[39;49m\u001b[39m\"\u001b[39;49m], \u001b[39m*\u001b[39;49msite[\u001b[39m\"\u001b[39;49m\u001b[39margs\u001b[39;49m\u001b[39m\"\u001b[39;49m], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msite[\u001b[39m\"\u001b[39;49m\u001b[39mkwargs\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m    232\u001b[0m         )\n\u001b[1;32m    233\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    234\u001b[0m         _, exc_value, traceback \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mexc_info()\n",
      "File \u001b[0;32m/data/ddmg/frank/.conda/envs/frank/lib/python3.9/site-packages/pyro/distributions/torch.py:143\u001b[0m, in \u001b[0;36mCategorical.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[39massert\u001b[39;00m logits\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m value\u001b[39m.\u001b[39mdim()) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m logits\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m value\u001b[39m.\u001b[39mdim(), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlog_prob(value)\n",
      "File \u001b[0;32m/data/ddmg/frank/.conda/envs/frank/lib/python3.9/site-packages/torch/distributions/categorical.py:127\u001b[0m, in \u001b[0;36mCategorical.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    125\u001b[0m value, log_pmf \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbroadcast_tensors(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogits)\n\u001b[1;32m    126\u001b[0m value \u001b[39m=\u001b[39m value[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, :\u001b[39m1\u001b[39m]\n\u001b[0;32m--> 127\u001b[0m \u001b[39mreturn\u001b[39;00m log_pmf\u001b[39m.\u001b[39;49mgather(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, value)\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: index -1 is out of bounds for dimension 1 with size 3"
     ]
    }
   ],
   "source": [
    "num_items = 1000\n",
    "num_raters = 2\n",
    "true_pi = torch.tensor([0.1, 0.3, 0.6])\n",
    "confusion_diagonal = 0.9\n",
    "\n",
    "annotations_simulated, observed_z, mask = simulate_data_with_observed(num_items, num_raters, true_pi, confusion_diagonal)\n",
    "# Condition the model on the observed data\n",
    "# old_mask = mask\n",
    "mask[mask == True] = False \n",
    "observed_z[observed_z < 0] = 0\n",
    "mask[0] = True\n",
    "observed_z[0] = -1\n",
    "\n",
    "# observed_z = observed_z.to(torch.float32)\n",
    "# observed_z[observed_z < 0] = float('nan')\n",
    "\n",
    "# conditioned_model = pyro.condition(corrected_dawid_skene_model_with_observed, data={\"z\": observed_z})\n",
    "nuts_kernel = NUTS(corrected_dawid_skene_model_with_observed, jit_compile=True, ignore_jit_warnings=True)\n",
    "\n",
    "# print(observed_z[mask == False])\n",
    "# nuts_kernel = NUTS(conditioned_model, jit_compile=True, ignore_jit_warnings=True)\n",
    "mcmc = MCMC(nuts_kernel, num_samples=1000, warmup_steps=500)\n",
    "mcmc.run(annotations_simulated, observed_z, mask)\n",
    "\n",
    "# # Extract posterior samples\n",
    "# posterior_samples_corrected = mcmc.get_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1.],\n",
       "       [0., 1., 0., 0., 1.],\n",
       "       [0., 1., 0., 1., 1.],\n",
       "       ...,\n",
       "       [0., 1., 1., 1., 1.],\n",
       "       [0., 0., 0., 1., 1.],\n",
       "       [1., 1., 1., 1., 0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def simulate_models_and_predictions(num_models, N, num_features, external_N=10000):\n",
    "    \"\"\"\n",
    "    Simulate models, get their predictions, and rank them based on accuracy on an external dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - num_models: Number of models to simulate\n",
    "    - N: Number of individuals in the training dataset\n",
    "    - num_features: Number of features per individual\n",
    "    - external_N: Number of individuals in the external dataset for accuracy estimation\n",
    "    \n",
    "    Returns:\n",
    "    - Rankings of models based on accuracy on the external dataset\n",
    "    - Matrix of class predictions for each model on the original dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate the feature matrix X\n",
    "    X = np.random.randn(N, num_features)\n",
    "    \n",
    "    # Generate the true beta coefficients\n",
    "    beta = np.random.randn(num_features)\n",
    "    \n",
    "    # Generate the true y values using a Bernoulli distribution based on the sigmoid probabilities\n",
    "    p = sigmoid(X @ beta)\n",
    "    y = np.random.binomial(1, p)\n",
    "    \n",
    "    # Covariance matrix for beta'\n",
    "    cov_matrix = np.full((num_features, num_features), 0.5)\n",
    "    np.fill_diagonal(cov_matrix, 1)\n",
    "    \n",
    "    # Generate an external dataset X' for accuracy estimation\n",
    "    X_prime = np.random.randn(external_N, num_features)\n",
    "    p_values_prime = sigmoid(X_prime @ beta)\n",
    "    y_prime = np.random.binomial(1, p_values_prime)\n",
    "    \n",
    "    # Matrix to store predictions from each model for both datasets\n",
    "    predictions_on_N = np.zeros((N, num_models))\n",
    "    accuracies = []\n",
    "    \n",
    "    for model_id in range(num_models):\n",
    "        beta_prime = np.random.multivariate_normal(beta, cov_matrix)\n",
    "        \n",
    "        # Predictions on N\n",
    "        y_hat_prob_N = sigmoid(X @ beta_prime)\n",
    "        y_hat_N = np.random.binomial(1, y_hat_prob_N)\n",
    "        predictions_on_N[:, model_id] = y_hat_N\n",
    "        \n",
    "        # Predictions and accuracy on external_N\n",
    "        y_hat_prob_prime = sigmoid(X_prime @ beta_prime)\n",
    "        y_hat_prime = np.random.binomial(1, y_hat_prob_prime)\n",
    "        accuracy = np.mean(y_hat_prime == y_prime)\n",
    "        accuracies.append((model_id, accuracy))\n",
    "    \n",
    "    # Rank models based on accuracy\n",
    "    # ranked_models = sorted(accuracies, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return accuracies, predictions_on_N, y\n",
    "\n",
    "\n",
    "rankings, predictions_on_N, true_y = simulate_models_and_predictions(num_models=5, N=1000, num_features=5)\n",
    "\n",
    "# predictions_on_N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([611, 665, 669, 106,  95])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warmup:   2%|▏         | 37/2000 [00:05, 16.94it/s, step size=4.61e-02, acc. prob=0.766]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/data/ddmg/frank/shuvom/tutorials/23-10-27-debug.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bminotaur.csail.mit.edu/data/ddmg/frank/shuvom/tutorials/23-10-27-debug.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m observed_y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfull((N,), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint64)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bminotaur.csail.mit.edu/data/ddmg/frank/shuvom/tutorials/23-10-27-debug.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m observed_y[indices] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(true_y[indices])\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bminotaur.csail.mit.edu/data/ddmg/frank/shuvom/tutorials/23-10-27-debug.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m mcmc\u001b[39m.\u001b[39;49mrun(torch\u001b[39m.\u001b[39;49mtensor(predictions_on_N), observed_y, torch\u001b[39m.\u001b[39;49mtensor(mask))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bminotaur.csail.mit.edu/data/ddmg/frank/shuvom/tutorials/23-10-27-debug.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m posterior_samples_corrected \u001b[39m=\u001b[39m mcmc\u001b[39m.\u001b[39mget_samples()\n",
      "File \u001b[0;32m/data/ddmg/frank/.conda/envs/frank/lib/python3.9/site-packages/pyro/poutine/messenger.py:12\u001b[0m, in \u001b[0;36m_context_wrap\u001b[0;34m(context, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_context_wrap\u001b[39m(context, fn, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     11\u001b[0m     \u001b[39mwith\u001b[39;00m context:\n\u001b[0;32m---> 12\u001b[0m         \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/data/ddmg/frank/.conda/envs/frank/lib/python3.9/site-packages/pyro/infer/mcmc/api.py:563\u001b[0m, in \u001b[0;36mMCMC.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[39mwith\u001b[39;00m optional(\n\u001b[1;32m    555\u001b[0m     pyro\u001b[39m.\u001b[39mvalidation_enabled(\u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisable_validation),\n\u001b[1;32m    556\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisable_validation \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[39m# This also resolves \"RuntimeError: Cowardly refusing to serialize non-leaf tensor which\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[39m# requires_grad\", which happens with `jit_compile` under PyTorch 1.7\u001b[39;00m\n\u001b[1;32m    562\u001b[0m     args \u001b[39m=\u001b[39m [arg\u001b[39m.\u001b[39mdetach() \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mis_tensor(arg) \u001b[39melse\u001b[39;00m arg \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m args]\n\u001b[0;32m--> 563\u001b[0m     \u001b[39mfor\u001b[39;00m x, chain_id \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msampler\u001b[39m.\u001b[39mrun(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    564\u001b[0m         \u001b[39mif\u001b[39;00m num_samples[chain_id] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    565\u001b[0m             num_samples[chain_id] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/data/ddmg/frank/.conda/envs/frank/lib/python3.9/site-packages/pyro/infer/mcmc/api.py:223\u001b[0m, in \u001b[0;36m_UnarySampler.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m logger \u001b[39m=\u001b[39m initialize_logger(logger, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, progress_bar)\n\u001b[1;32m    222\u001b[0m hook_w_logging \u001b[39m=\u001b[39m _add_logging_hook(logger, progress_bar, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook)\n\u001b[0;32m--> 223\u001b[0m \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m _gen_samples(\n\u001b[1;32m    224\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel,\n\u001b[1;32m    225\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarmup_steps,\n\u001b[1;32m    226\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_samples,\n\u001b[1;32m    227\u001b[0m     hook_w_logging,\n\u001b[1;32m    228\u001b[0m     i \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_chains \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    229\u001b[0m     \u001b[39m*\u001b[39margs,\n\u001b[1;32m    230\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    231\u001b[0m ):\n\u001b[1;32m    232\u001b[0m     \u001b[39myield\u001b[39;00m sample, i  \u001b[39m# sample, chain_id\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel\u001b[39m.\u001b[39mcleanup()\n",
      "File \u001b[0;32m/data/ddmg/frank/.conda/envs/frank/lib/python3.9/site-packages/pyro/infer/mcmc/api.py:150\u001b[0m, in \u001b[0;36m_gen_samples\u001b[0;34m(kernel, warmup_steps, num_samples, hook, chain_id, *args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[39myield\u001b[39;00m {name: params[name]\u001b[39m.\u001b[39mshape \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m save_params}\n\u001b[1;32m    149\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(warmup_steps):\n\u001b[0;32m--> 150\u001b[0m     params \u001b[39m=\u001b[39m kernel\u001b[39m.\u001b[39;49msample(params)\n\u001b[1;32m    151\u001b[0m     hook(\n\u001b[1;32m    152\u001b[0m         kernel,\n\u001b[1;32m    153\u001b[0m         params,\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWarmup [\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(chain_id) \u001b[39mif\u001b[39;00m chain_id \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mWarmup\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    155\u001b[0m         i,\n\u001b[1;32m    156\u001b[0m     )\n\u001b[1;32m    157\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_samples):\n",
      "File \u001b[0;32m/data/ddmg/frank/.conda/envs/frank/lib/python3.9/site-packages/pyro/infer/mcmc/nuts.py:452\u001b[0m, in \u001b[0;36mNUTS.sample\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    450\u001b[0m     z_right_grads \u001b[39m=\u001b[39m new_tree\u001b[39m.\u001b[39mz_right_grads\n\u001b[1;32m    451\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# go the the left, start from the left leaf of current tree\u001b[39;00m\n\u001b[0;32m--> 452\u001b[0m     new_tree \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build_tree(\n\u001b[1;32m    453\u001b[0m         z_left,\n\u001b[1;32m    454\u001b[0m         r_left,\n\u001b[1;32m    455\u001b[0m         z_left_grads,\n\u001b[1;32m    456\u001b[0m         log_slice,\n\u001b[1;32m    457\u001b[0m         direction,\n\u001b[1;32m    458\u001b[0m         tree_depth,\n\u001b[1;32m    459\u001b[0m         energy_current,\n\u001b[1;32m    460\u001b[0m     )\n\u001b[1;32m    461\u001b[0m     z_left \u001b[39m=\u001b[39m new_tree\u001b[39m.\u001b[39mz_left\n\u001b[1;32m    462\u001b[0m     r_left \u001b[39m=\u001b[39m new_tree\u001b[39m.\u001b[39mr_left\n",
      "File \u001b[0;32m/data/ddmg/frank/.conda/envs/frank/lib/python3.9/site-packages/pyro/infer/mcmc/nuts.py:259\u001b[0m, in \u001b[0;36mNUTS._build_tree\u001b[0;34m(self, z, r, z_grads, log_slice, direction, tree_depth, energy_current)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_basetree(\n\u001b[1;32m    255\u001b[0m         z, r, z_grads, log_slice, direction, energy_current\n\u001b[1;32m    256\u001b[0m     )\n\u001b[1;32m    258\u001b[0m \u001b[39m# build the first half of tree\u001b[39;00m\n\u001b[0;32m--> 259\u001b[0m half_tree \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build_tree(\n\u001b[1;32m    260\u001b[0m     z, r, z_grads, log_slice, direction, tree_depth \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m, energy_current\n\u001b[1;32m    261\u001b[0m )\n\u001b[1;32m    262\u001b[0m z_proposal \u001b[39m=\u001b[39m half_tree\u001b[39m.\u001b[39mz_proposal\n\u001b[1;32m    263\u001b[0m z_proposal_pe \u001b[39m=\u001b[39m half_tree\u001b[39m.\u001b[39mz_proposal_pe\n",
      "File \u001b[0;32m/data/ddmg/frank/.conda/envs/frank/lib/python3.9/site-packages/pyro/infer/mcmc/nuts.py:259\u001b[0m, in \u001b[0;36mNUTS._build_tree\u001b[0;34m(self, z, r, z_grads, log_slice, direction, tree_depth, energy_current)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_basetree(\n\u001b[1;32m    255\u001b[0m         z, r, z_grads, log_slice, direction, energy_current\n\u001b[1;32m    256\u001b[0m     )\n\u001b[1;32m    258\u001b[0m \u001b[39m# build the first half of tree\u001b[39;00m\n\u001b[0;32m--> 259\u001b[0m half_tree \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build_tree(\n\u001b[1;32m    260\u001b[0m     z, r, z_grads, log_slice, direction, tree_depth \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m, energy_current\n\u001b[1;32m    261\u001b[0m )\n\u001b[1;32m    262\u001b[0m z_proposal \u001b[39m=\u001b[39m half_tree\u001b[39m.\u001b[39mz_proposal\n\u001b[1;32m    263\u001b[0m z_proposal_pe \u001b[39m=\u001b[39m half_tree\u001b[39m.\u001b[39mz_proposal_pe\n",
      "File \u001b[0;32m/data/ddmg/frank/.conda/envs/frank/lib/python3.9/site-packages/pyro/infer/mcmc/nuts.py:254\u001b[0m, in \u001b[0;36mNUTS._build_tree\u001b[0;34m(self, z, r, z_grads, log_slice, direction, tree_depth, energy_current)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_build_tree\u001b[39m(\n\u001b[1;32m    251\u001b[0m     \u001b[39mself\u001b[39m, z, r, z_grads, log_slice, direction, tree_depth, energy_current\n\u001b[1;32m    252\u001b[0m ):\n\u001b[1;32m    253\u001b[0m     \u001b[39mif\u001b[39;00m tree_depth \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 254\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build_basetree(\n\u001b[1;32m    255\u001b[0m             z, r, z_grads, log_slice, direction, energy_current\n\u001b[1;32m    256\u001b[0m         )\n\u001b[1;32m    258\u001b[0m     \u001b[39m# build the first half of tree\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     half_tree \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_tree(\n\u001b[1;32m    260\u001b[0m         z, r, z_grads, log_slice, direction, tree_depth \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m, energy_current\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m/data/ddmg/frank/.conda/envs/frank/lib/python3.9/site-packages/pyro/infer/mcmc/nuts.py:199\u001b[0m, in \u001b[0;36mNUTS._build_basetree\u001b[0;34m(self, z, r, z_grads, log_slice, direction, energy_current)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_build_basetree\u001b[39m(\u001b[39mself\u001b[39m, z, r, z_grads, log_slice, direction, energy_current):\n\u001b[1;32m    198\u001b[0m     step_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_size \u001b[39mif\u001b[39;00m direction \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_size\n\u001b[0;32m--> 199\u001b[0m     z_new, r_new, z_grads, potential_energy \u001b[39m=\u001b[39m velocity_verlet(\n\u001b[1;32m    200\u001b[0m         z,\n\u001b[1;32m    201\u001b[0m         r,\n\u001b[1;32m    202\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpotential_fn,\n\u001b[1;32m    203\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmass_matrix_adapter\u001b[39m.\u001b[39;49mkinetic_grad,\n\u001b[1;32m    204\u001b[0m         step_size,\n\u001b[1;32m    205\u001b[0m         z_grads\u001b[39m=\u001b[39;49mz_grads,\n\u001b[1;32m    206\u001b[0m     )\n\u001b[1;32m    207\u001b[0m     r_new_unscaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmass_matrix_adapter\u001b[39m.\u001b[39munscale(r_new)\n\u001b[1;32m    208\u001b[0m     energy_new \u001b[39m=\u001b[39m potential_energy \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_kinetic_energy(r_new_unscaled)\n",
      "File \u001b[0;32m/data/ddmg/frank/.conda/envs/frank/lib/python3.9/site-packages/pyro/ops/integrator.py:39\u001b[0m, in \u001b[0;36mvelocity_verlet\u001b[0;34m(z, r, potential_fn, kinetic_grad, step_size, num_steps, z_grads)\u001b[0m\n\u001b[1;32m     37\u001b[0m r_next \u001b[39m=\u001b[39m r\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m     38\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_steps):\n\u001b[0;32m---> 39\u001b[0m     z_next, r_next, z_grads, potential_energy \u001b[39m=\u001b[39m _single_step_verlet(\n\u001b[1;32m     40\u001b[0m         z_next, r_next, potential_fn, kinetic_grad, step_size, z_grads\n\u001b[1;32m     41\u001b[0m     )\n\u001b[1;32m     42\u001b[0m \u001b[39mreturn\u001b[39;00m z_next, r_next, z_grads, potential_energy\n",
      "File \u001b[0;32m/data/ddmg/frank/.conda/envs/frank/lib/python3.9/site-packages/pyro/ops/integrator.py:61\u001b[0m, in \u001b[0;36m_single_step_verlet\u001b[0;34m(z, r, potential_fn, kinetic_grad, step_size, z_grads)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mfor\u001b[39;00m site_name \u001b[39min\u001b[39;00m z:\n\u001b[1;32m     59\u001b[0m     z[site_name] \u001b[39m=\u001b[39m z[site_name] \u001b[39m+\u001b[39m step_size \u001b[39m*\u001b[39m r_grads[site_name]  \u001b[39m# z(n+1)\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m z_grads, potential_energy \u001b[39m=\u001b[39m potential_grad(potential_fn, z)\n\u001b[1;32m     62\u001b[0m \u001b[39mfor\u001b[39;00m site_name \u001b[39min\u001b[39;00m r:\n\u001b[1;32m     63\u001b[0m     r[site_name] \u001b[39m=\u001b[39m r[site_name] \u001b[39m+\u001b[39m \u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m step_size \u001b[39m*\u001b[39m (\u001b[39m-\u001b[39mz_grads[site_name])  \u001b[39m# r(n+1)\u001b[39;00m\n",
      "File \u001b[0;32m/data/ddmg/frank/.conda/envs/frank/lib/python3.9/site-packages/pyro/ops/integrator.py:91\u001b[0m, in \u001b[0;36mpotential_grad\u001b[0;34m(potential_fn, z)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m---> 91\u001b[0m grads \u001b[39m=\u001b[39m grad(potential_energy, z_nodes)\n\u001b[1;32m     92\u001b[0m \u001b[39mfor\u001b[39;00m node \u001b[39min\u001b[39;00m z_nodes:\n\u001b[1;32m     93\u001b[0m     node\u001b[39m.\u001b[39mrequires_grad_(\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/data/ddmg/frank/.conda/envs/frank/lib/python3.9/site-packages/torch/autograd/__init__.py:303\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[39mreturn\u001b[39;00m _vmap_internals\u001b[39m.\u001b[39m_vmap(vjp, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, allow_none_pass_through\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    302\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    304\u001b[0m         t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,\n\u001b[1;32m    305\u001b[0m         allow_unused, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warmup:  10%|▉         | 145/1500 [00:39, 20.65it/s, step size=1.99e-01, acc. prob=0.778]"
     ]
    }
   ],
   "source": [
    "N=1000\n",
    "indices = observed_indices = torch.randperm(N)[:5]\n",
    "print(indices)\n",
    "\n",
    "mask = [False for i in range(N)]\n",
    "for index in indices:\n",
    "    mask[index] = True\n",
    "\n",
    "\n",
    "pyro.clear_param_store()\n",
    "logging.getLogger(\"pyro\").setLevel(logging.DEBUG)\n",
    "nuts_kernel = NUTS(corrected_dawid_skene_model_with_observed, jit_compile=True, ignore_jit_warnings=True)\n",
    "mcmc = MCMC(nuts_kernel, num_samples=500, warmup_steps=1500)\n",
    "\n",
    "observed_y = torch.full((N,), -1, dtype=torch.int64)\n",
    "observed_y[indices] = torch.tensor(true_y[indices])\n",
    "mcmc.run(torch.tensor(predictions_on_N), observed_y, torch.tensor(mask))\n",
    "\n",
    "posterior_samples_corrected = mcmc.get_samples()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta_mean = posterior_samples_corrected[\"theta\"].mean(dim=0) \n",
    "# pi_mean = posterior_samples_corrected[\"pi\"].mean(dim=0)\n",
    "\n",
    "# result = pi_mean[0] * theta_mean[:, 0, 0] + pi_mean[1] * theta_mean[:, 1, 1]\n",
    "# resulting_ranking = torch.argsort(result, descending=True)\n",
    "# resulting_ranking.tolist()\n",
    "# result\n",
    "# posterior_samples_corrected[\"theta\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'posterior_samples_corrected' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pi_reshaped \u001b[39m=\u001b[39m posterior_samples_corrected[\u001b[39m\"\u001b[39m\u001b[39mpi\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m result \u001b[39m=\u001b[39m (pi_reshaped[:, :, \u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m posterior_samples_corrected[\u001b[39m\"\u001b[39m\u001b[39mtheta\u001b[39m\u001b[39m\"\u001b[39m][:, :, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m \n\u001b[1;32m      4\u001b[0m           pi_reshaped[:, :, \u001b[39m1\u001b[39m] \u001b[39m*\u001b[39m posterior_samples_corrected[\u001b[39m\"\u001b[39m\u001b[39mtheta\u001b[39m\u001b[39m\"\u001b[39m][:, :, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m])\n\u001b[1;32m      6\u001b[0m resulting_ranking \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margsort(result\u001b[39m.\u001b[39mmean(dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), descending\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'posterior_samples_corrected' is not defined"
     ]
    }
   ],
   "source": [
    "pi_reshaped = posterior_samples_corrected[\"pi\"].unsqueeze(1)\n",
    "\n",
    "result = (pi_reshaped[:, :, 0] * posterior_samples_corrected[\"theta\"][:, :, 0, 0] + \n",
    "          pi_reshaped[:, :, 1] * posterior_samples_corrected[\"theta\"][:, :, 1, 1])\n",
    "\n",
    "resulting_ranking = torch.argsort(result.mean(dim=0), descending=True)\n",
    "print(result.mean(dim=0).tolist())\n",
    "formatted_list = [\"{:.3f}\".format(item) for item in result.mean(dim=0).tolist()]\n",
    "print(formatted_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6724, 0.5668, 0.6522, 0.6039, 0.5811]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs_only = [tup[1] for tup in rankings]\n",
    "accs_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignificanceResult(statistic=0.8999999999999998, pvalue=0.03738607346849874)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import kendalltau\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Compute Kendall's tau using scipy's implementation\n",
    "# spearmanr([0,1,2,3,4], [2,0,4,1,3])\n",
    "spearmanr([1,0,2,4,3], [0,1,2,4,3])\n",
    "# tau_scipy\n",
    "\n",
    "# kendalltau(result.mean(dim=0).tolist(), accs_only)\n",
    "# tau_scipy\n",
    "# kendalltau([0,1,2,3,4], accs_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.]],\n",
       "\n",
       "       [[0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.]],\n",
       "\n",
       "       [[0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rankings_only\n",
    "from dawid_skene import run\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Sample tensor\n",
    "tensor = np.array([[1., 0., 0., 1., 1.],\n",
    "                   [0., 0., 0., 0., 0.],\n",
    "                   [0., 0., 1., 0., 0.],\n",
    "                   # ... (For demonstration purposes, I've truncated the data.)\n",
    "                   [0., 1., 1., 0., 0.],\n",
    "                   [1., 1., 1., 1., 1.],\n",
    "                   [1., 1., 1., 0., 0.]])\n",
    "\n",
    "n_examples = predictions_on_N.shape[0]\n",
    "n_classes = 2\n",
    "n_models = predictions_on_N.shape[1]\n",
    "\n",
    "one_hot_predictions = np.zeros((n_examples, n_models, n_classes))\n",
    "\n",
    "# Loop over both classes\n",
    "# Loop over both classes\n",
    "for class_idx in range(n_classes):\n",
    "    # Loop over each model (column)\n",
    "    for i in range(n_models):\n",
    "        one_class_preds = (predictions_on_N[:,i] == class_idx).astype(int)\n",
    "        one_hot_predictions[np.arange(n_examples), i, class_idx] = one_class_preds\n",
    "\n",
    "\n",
    "one_hot_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result\n",
    "# _, _, _, _, blah, error_rates, _ = run(one_hot_predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import dawid_skene\n",
    "importlib.reload(dawid_skene)\n",
    "from dawid_skene import run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open('kendalls_labeled.pkl', 'rb') as f:\n",
    "#     kendalls_labeled = pickle.load(f)\n",
    "\n",
    "# with open('spearmans_labeled.pkl', 'rb') as f:\n",
    "#     spearmans_labeled = pickle.load(f)\n",
    "\n",
    "\n",
    "# spearmans_labeled\n",
    "# my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warmup:   0%|          | 0/600 [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:38, 15.45it/s, step size=5.69e-01, acc. prob=0.871]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.635, 0.698, 0.753, 0.66, 0.73] tensor([0.6578, 0.7836, 0.8934, 0.7230, 0.8496])\n",
      "Labeled:  10 0.9999999999999999 0.9999999999999999\n",
      "50 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:42, 14.21it/s, step size=4.63e-01, acc. prob=0.921]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.673, 0.507, 0.628, 0.507, 0.696] tensor([0.8442, 0.6013, 0.7263, 0.5725, 0.8217])\n",
      "Labeled:  50 0.8720815992723809 0.7378647873726218\n",
      "100 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:41, 14.44it/s, step size=5.12e-01, acc. prob=0.884]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.678, 0.674, 0.543, 0.733, 0.593] tensor([0.7345, 0.7555, 0.5299, 0.8695, 0.5799])\n",
      "Labeled:  100 0.8999999999999998 0.7999999999999999\n",
      "500 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:36, 16.23it/s, step size=6.13e-01, acc. prob=0.890]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.657, 0.61, 0.694, 0.656, 0.703] tensor([0.6884, 0.6433, 0.7476, 0.6655, 0.7250])\n",
      "Labeled:  500 0.8999999999999998 0.7999999999999999\n",
      "10 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:49, 12.20it/s, step size=4.54e-01, acc. prob=0.895]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.669, 0.615, 0.715, 0.644, 0.676] tensor([0.7028, 0.7396, 0.8699, 0.7259, 0.7491])\n",
      "Labeled:  10 0.6 0.39999999999999997\n",
      "50 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:46, 12.86it/s, step size=4.44e-01, acc. prob=0.905]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.632, 0.533, 0.597, 0.636, 0.567] tensor([0.7058, 0.5940, 0.7287, 0.7392, 0.6391])\n",
      "Labeled:  50 0.8999999999999998 0.7999999999999999\n",
      "100 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:42, 14.25it/s, step size=5.58e-01, acc. prob=0.905]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.68, 0.649, 0.782, 0.553, 0.651] tensor([0.8224, 0.8205, 0.7298, 0.5814, 0.7357])\n",
      "Labeled:  100 0.3 0.19999999999999998\n",
      "500 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:35, 16.76it/s, step size=6.27e-01, acc. prob=0.897]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.746, 0.712, 0.825, 0.639, 0.818] tensor([0.7964, 0.7469, 0.8384, 0.6614, 0.8534])\n",
      "Labeled:  500 0.8999999999999998 0.7999999999999999\n",
      "10 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:40, 14.91it/s, step size=5.43e-01, acc. prob=0.890]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.648, 0.727, 0.658, 0.689, 0.671] tensor([0.6239, 0.8959, 0.7710, 0.7164, 0.7897])\n",
      "Labeled:  10 0.7 0.6\n",
      "50 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:38, 15.43it/s, step size=5.15e-01, acc. prob=0.908]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.779, 0.744, 0.662, 0.79, 0.752] tensor([0.8432, 0.8248, 0.7130, 0.8550, 0.8572])\n",
      "Labeled:  50 0.7 0.6\n",
      "100 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:47, 12.57it/s, step size=5.23e-01, acc. prob=0.914]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.672, 0.645, 0.653, 0.598, 0.635] tensor([0.8543, 0.6763, 0.5721, 0.5303, 0.8034])\n",
      "Labeled:  100 0.6 0.39999999999999997\n",
      "500 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:41, 14.61it/s, step size=6.55e-01, acc. prob=0.900]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.766, 0.787, 0.758, 0.751, 0.541] tensor([0.8171, 0.8517, 0.7899, 0.7830, 0.5609])\n",
      "Labeled:  500 0.9999999999999999 0.9999999999999999\n",
      "10 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:43, 13.91it/s, step size=5.89e-01, acc. prob=0.892]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.535, 0.542, 0.577, 0.515, 0.539] tensor([0.7782, 0.7752, 0.8446, 0.9137, 0.6875])\n",
      "Labeled:  10 -0.3 -0.19999999999999998\n",
      "50 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:41, 14.29it/s, step size=5.89e-01, acc. prob=0.911]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.439, 0.436, 0.6, 0.625, 0.624] tensor([0.7810, 0.7418, 0.5684, 0.8926, 0.8142])\n",
      "Labeled:  50 0.7 0.6\n",
      "100 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:38, 15.55it/s, step size=7.00e-01, acc. prob=0.851]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.457, 0.57, 0.587, 0.622, 0.616] tensor([0.6426, 0.8013, 0.7545, 0.8351, 0.5972])\n",
      "Labeled:  100 0.3 0.19999999999999998\n",
      "500 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:36, 16.25it/s, step size=6.83e-01, acc. prob=0.879]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.655, 0.645, 0.583, 0.519, 0.693] tensor([0.7214, 0.7238, 0.6345, 0.5328, 0.7710])\n",
      "Labeled:  500 0.8999999999999998 0.7999999999999999\n",
      "10 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:40, 14.71it/s, step size=6.38e-01, acc. prob=0.832]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.684, 0.715, 0.671, 0.662, 0.698] tensor([0.7335, 0.8041, 0.8506, 0.6792, 0.8620])\n",
      "Labeled:  10 0.49999999999999994 0.39999999999999997\n",
      "50 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:38, 15.62it/s, step size=6.23e-01, acc. prob=0.854]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.72, 0.542, 0.585, 0.664, 0.707] tensor([0.8415, 0.5350, 0.5939, 0.7994, 0.8717])\n",
      "Labeled:  50 0.8999999999999998 0.7999999999999999\n",
      "100 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:40, 14.77it/s, step size=4.94e-01, acc. prob=0.913]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.588, 0.612, 0.622, 0.567, 0.642] tensor([0.6411, 0.8231, 0.7908, 0.7581, 0.6679])\n",
      "Labeled:  100 0.09999999999999999 0.0\n",
      "500 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:35, 17.00it/s, step size=6.36e-01, acc. prob=0.896]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.526, 0.734, 0.768, 0.754, 0.655] tensor([0.5204, 0.7825, 0.8394, 0.8168, 0.6583])\n",
      "Labeled:  500 0.9999999999999999 0.9999999999999999\n",
      "10 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [01:27,  6.83it/s, step size=2.04e-01, acc. prob=0.938]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.502, 0.453, 0.644, 0.5, 0.53] tensor([0.5269, 0.8854, 0.6513, 0.6726, 0.5200])\n",
      "Labeled:  10 -0.7 -0.6\n",
      "50 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:38, 15.62it/s, step size=5.86e-01, acc. prob=0.902]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.689, 0.771, 0.709, 0.699, 0.667] tensor([0.7892, 0.8328, 0.8002, 0.7516, 0.6091])\n",
      "Labeled:  50 0.8999999999999998 0.7999999999999999\n",
      "100 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:41, 14.58it/s, step size=5.13e-01, acc. prob=0.881]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.607, 0.579, 0.552, 0.605, 0.575] tensor([0.6509, 0.5384, 0.7750, 0.5200, 0.7815])\n",
      "Labeled:  100 -0.6 -0.39999999999999997\n",
      "500 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:36, 16.64it/s, step size=6.28e-01, acc. prob=0.877]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.758, 0.639, 0.743, 0.714, 0.77] tensor([0.7966, 0.6580, 0.7735, 0.7708, 0.8354])\n",
      "Labeled:  500 0.9999999999999999 0.9999999999999999\n",
      "10 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [02:00,  4.96it/s, step size=1.95e-01, acc. prob=0.430]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.568, 0.617, 0.636, 0.595, 0.539] tensor([0.5328, 0.8972, 0.6159, 0.6912, 0.5392])\n",
      "Labeled:  10 0.6 0.39999999999999997\n",
      "50 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:37, 16.13it/s, step size=5.88e-01, acc. prob=0.892]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.629, 0.632, 0.603, 0.643, 0.639] tensor([0.5394, 0.8268, 0.8309, 0.5770, 0.8338])\n",
      "Labeled:  50 0.0 0.0\n",
      "100 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:37, 15.82it/s, step size=6.89e-01, acc. prob=0.876]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.78, 0.763, 0.705, 0.759, 0.697] tensor([0.8145, 0.8227, 0.7082, 0.7693, 0.6505])\n",
      "Labeled:  100 0.8999999999999998 0.7999999999999999\n",
      "500 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:37, 15.91it/s, step size=5.89e-01, acc. prob=0.916]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.703, 0.712, 0.548, 0.676, 0.749] tensor([0.7115, 0.7821, 0.5301, 0.7354, 0.7884])\n",
      "Labeled:  500 0.8999999999999998 0.7999999999999999\n",
      "10 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:44, 13.40it/s, step size=4.58e-01, acc. prob=0.902]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.676, 0.677, 0.695, 0.714, 0.544] tensor([0.7996, 0.7413, 0.7390, 0.8575, 0.5372])\n",
      "Labeled:  10 0.6 0.39999999999999997\n",
      "50 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:40, 14.99it/s, step size=5.51e-01, acc. prob=0.869]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.735, 0.614, 0.673, 0.578, 0.634] tensor([0.7512, 0.8030, 0.5499, 0.5995, 0.7728])\n",
      "Labeled:  50 -0.19999999999999998 -0.19999999999999998\n",
      "100 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:40, 14.82it/s, step size=5.97e-01, acc. prob=0.910]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.667, 0.712, 0.726, 0.719, 0.757] tensor([0.6737, 0.6793, 0.8436, 0.8496, 0.8511])\n",
      "Labeled:  100 0.8999999999999998 0.7999999999999999\n",
      "500 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:40, 14.92it/s, step size=5.64e-01, acc. prob=0.910]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.586, 0.663, 0.634, 0.773, 0.643] tensor([0.5497, 0.7189, 0.6545, 0.7849, 0.6943])\n",
      "Labeled:  500 0.9999999999999999 0.9999999999999999\n",
      "10 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:42, 14.24it/s, step size=5.92e-01, acc. prob=0.878]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.682, 0.725, 0.66, 0.719, 0.685] tensor([0.7563, 0.8096, 0.8517, 0.8862, 0.6065])\n",
      "Labeled:  10 0.09999999999999999 0.0\n",
      "50 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:50, 11.93it/s, step size=4.01e-01, acc. prob=0.924]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.548, 0.497, 0.508, 0.633, 0.597] tensor([0.5291, 0.5891, 0.6729, 0.5869, 0.8911])\n",
      "Labeled:  50 -0.09999999999999999 0.0\n",
      "100 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:46, 12.78it/s, step size=5.72e-01, acc. prob=0.886]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.677, 0.644, 0.636, 0.67, 0.587] tensor([0.7262, 0.7846, 0.7859, 0.7662, 0.5467])\n",
      "Labeled:  100 0.0 -0.19999999999999998\n",
      "500 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:37, 15.90it/s, step size=6.60e-01, acc. prob=0.882]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.478, 0.657, 0.72, 0.604, 0.552] tensor([0.5260, 0.6867, 0.7455, 0.6484, 0.5930])\n",
      "Labeled:  500 0.9999999999999999 0.9999999999999999\n",
      "10 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:58, 10.32it/s, step size=3.66e-01, acc. prob=0.914]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.488, 0.475, 0.526, 0.532, 0.497] tensor([0.6876, 0.7875, 0.6715, 0.5496, 0.6933])\n",
      "Labeled:  10 -0.8999999999999998 -0.7999999999999999\n",
      "50 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:39, 15.14it/s, step size=5.38e-01, acc. prob=0.907]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.713, 0.787, 0.664, 0.761, 0.611] tensor([0.7570, 0.7594, 0.7431, 0.8096, 0.6310])\n",
      "Labeled:  50 0.8999999999999998 0.7999999999999999\n",
      "100 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:45, 13.17it/s, step size=5.17e-01, acc. prob=0.884]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.53, 0.571, 0.527, 0.558, 0.524] tensor([0.5527, 0.5607, 0.7124, 0.7023, 0.6814])\n",
      "Labeled:  100 -0.3 -0.19999999999999998\n",
      "500 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:37, 16.21it/s, step size=6.06e-01, acc. prob=0.884]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.674, 0.692, 0.697, 0.668, 0.645] tensor([0.6889, 0.7612, 0.7562, 0.7307, 0.7294])\n",
      "Labeled:  500 0.6 0.39999999999999997\n",
      "10 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:41, 14.57it/s, step size=5.32e-01, acc. prob=0.906]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.615, 0.592, 0.646, 0.623, 0.627] tensor([0.7935, 0.6545, 0.7277, 0.7773, 0.8539])\n",
      "Labeled:  10 0.3 0.19999999999999998\n",
      "50 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:42, 14.28it/s, step size=5.41e-01, acc. prob=0.885]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.549, 0.649, 0.623, 0.584, 0.679] tensor([0.6350, 0.7042, 0.6941, 0.6885, 0.8885])\n",
      "Labeled:  50 0.9999999999999999 0.9999999999999999\n",
      "100 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:38, 15.44it/s, step size=5.93e-01, acc. prob=0.898]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.639, 0.634, 0.5, 0.582, 0.419] tensor([0.6887, 0.7686, 0.6094, 0.6514, 0.7674])\n",
      "Labeled:  100 0.19999999999999998 0.19999999999999998\n",
      "500 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:36, 16.40it/s, step size=7.20e-01, acc. prob=0.862]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71, 0.637, 0.625, 0.704, 0.7] tensor([0.7803, 0.7252, 0.6878, 0.6649, 0.7331])\n",
      "Labeled:  500 0.39999999999999997 0.39999999999999997\n",
      "10 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:46, 12.99it/s, step size=4.95e-01, acc. prob=0.880]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.576, 0.5, 0.493, 0.653, 0.728] tensor([0.8355, 0.7860, 0.7814, 0.5750, 0.5570])\n",
      "Labeled:  10 -0.6 -0.39999999999999997\n",
      "50 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:38, 15.76it/s, step size=6.38e-01, acc. prob=0.874]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7, 0.673, 0.657, 0.625, 0.732] tensor([0.8048, 0.7024, 0.7201, 0.5941, 0.8318])\n",
      "Labeled:  50 0.8999999999999998 0.7999999999999999\n",
      "100 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:40, 14.97it/s, step size=6.07e-01, acc. prob=0.886]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.651, 0.687, 0.649, 0.625, 0.644] tensor([0.8233, 0.8647, 0.7719, 0.7637, 0.5846])\n",
      "Labeled:  100 0.8999999999999998 0.7999999999999999\n",
      "500 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:37, 15.81it/s, step size=7.12e-01, acc. prob=0.873]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.714, 0.672, 0.736, 0.702, 0.74] tensor([0.7859, 0.7472, 0.7701, 0.7221, 0.7341])\n",
      "Labeled:  500 0.09999999999999999 0.0\n",
      "10 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:38, 15.52it/s, step size=5.86e-01, acc. prob=0.873]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.725, 0.764, 0.715, 0.723, 0.683] tensor([0.8104, 0.8434, 0.8529, 0.6929, 0.7990])\n",
      "Labeled:  10 0.19999999999999998 0.19999999999999998\n",
      "50 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:39, 15.27it/s, step size=5.42e-01, acc. prob=0.897]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.618, 0.704, 0.71, 0.675, 0.659] tensor([0.5672, 0.8532, 0.8310, 0.7191, 0.7937])\n",
      "Labeled:  50 0.7999999999999999 0.6\n",
      "100 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:40, 14.86it/s, step size=7.22e-01, acc. prob=0.840]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.487, 0.603, 0.61, 0.565, 0.643] tensor([0.5212, 0.8032, 0.7539, 0.7783, 0.8460])\n",
      "Labeled:  100 0.7 0.6\n",
      "500 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:36, 16.27it/s, step size=6.55e-01, acc. prob=0.898]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.671, 0.55, 0.679, 0.554, 0.609] tensor([0.7162, 0.5396, 0.7452, 0.5375, 0.6612])\n",
      "Labeled:  500 0.8999999999999998 0.7999999999999999\n",
      "10 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:45, 13.07it/s, step size=5.11e-01, acc. prob=0.903]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.731, 0.64, 0.712, 0.635, 0.5] tensor([0.9044, 0.6750, 0.8358, 0.6718, 0.5181])\n",
      "Labeled:  10 0.9999999999999999 0.9999999999999999\n",
      "50 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:47, 12.68it/s, step size=6.07e-01, acc. prob=0.890]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.554, 0.419, 0.639, 0.648, 0.624] tensor([0.6428, 0.7317, 0.8756, 0.8194, 0.7754])\n",
      "Labeled:  50 0.7999999999999999 0.6\n",
      "100 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:42, 14.02it/s, step size=5.12e-01, acc. prob=0.907]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.705, 0.669, 0.633, 0.651, 0.652] tensor([0.7448, 0.6992, 0.6274, 0.6639, 0.6817])\n",
      "Labeled:  100 0.9999999999999999 0.9999999999999999\n",
      "500 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:39, 15.32it/s, step size=6.57e-01, acc. prob=0.907]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.784, 0.797, 0.678, 0.743, 0.8] tensor([0.8164, 0.8405, 0.7252, 0.8069, 0.8618])\n",
      "Labeled:  500 0.9999999999999999 0.9999999999999999\n",
      "10 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [01:05,  9.20it/s, step size=3.40e-01, acc. prob=0.899]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.643, 0.582, 0.549, 0.628, 0.598] tensor([0.8365, 0.6024, 0.6212, 0.5222, 0.8067])\n",
      "Labeled:  10 0.3 0.19999999999999998\n",
      "50 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:40, 14.87it/s, step size=4.94e-01, acc. prob=0.914]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.692, 0.657, 0.704, 0.63, 0.695] tensor([0.7365, 0.6965, 0.7517, 0.6616, 0.7676])\n",
      "Labeled:  50 0.8999999999999998 0.7999999999999999\n",
      "100 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:38, 15.67it/s, step size=6.18e-01, acc. prob=0.896]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.743, 0.734, 0.765, 0.776, 0.73] tensor([0.8295, 0.8411, 0.6716, 0.7445, 0.8364])\n",
      "Labeled:  100 -0.7999999999999999 -0.6\n",
      "500 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:35, 16.95it/s, step size=6.96e-01, acc. prob=0.867]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.594, 0.614, 0.661, 0.565, 0.577] tensor([0.6109, 0.7212, 0.7620, 0.6649, 0.6297])\n",
      "Labeled:  500 0.6 0.39999999999999997\n",
      "10 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:51, 11.60it/s, step size=4.15e-01, acc. prob=0.900]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.596, 0.642, 0.698, 0.601, 0.555] tensor([0.5862, 0.8346, 0.8599, 0.5262, 0.7074])\n",
      "Labeled:  10 0.6 0.39999999999999997\n",
      "50 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:38, 15.52it/s, step size=5.73e-01, acc. prob=0.869]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.571, 0.739, 0.601, 0.686, 0.694] tensor([0.6751, 0.7955, 0.6469, 0.8210, 0.8288])\n",
      "Labeled:  50 0.6 0.39999999999999997\n",
      "100 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:38, 15.70it/s, step size=6.19e-01, acc. prob=0.868]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.72, 0.773, 0.742, 0.702, 0.723] tensor([0.7207, 0.8816, 0.8326, 0.7522, 0.7873])\n",
      "Labeled:  100 0.8999999999999998 0.7999999999999999\n",
      "500 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:36, 16.52it/s, step size=6.64e-01, acc. prob=0.865]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.637, 0.537, 0.775, 0.693, 0.699] tensor([0.6260, 0.5317, 0.8043, 0.7229, 0.7499])\n",
      "Labeled:  500 0.9999999999999999 0.9999999999999999\n",
      "10 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:50, 11.94it/s, step size=5.02e-01, acc. prob=0.864]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.617, 0.681, 0.623, 0.603, 0.624] tensor([0.6851, 0.8432, 0.7537, 0.7242, 0.7474])\n",
      "Labeled:  10 0.7999999999999999 0.6\n",
      "50 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:41, 14.50it/s, step size=5.36e-01, acc. prob=0.882]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.594, 0.714, 0.613, 0.654, 0.786] tensor([0.7336, 0.8372, 0.5942, 0.7598, 0.7354])\n",
      "Labeled:  50 0.6 0.39999999999999997\n",
      "100 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:38, 15.65it/s, step size=5.50e-01, acc. prob=0.915]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.704, 0.714, 0.687, 0.703, 0.586] tensor([0.7284, 0.7787, 0.7824, 0.7887, 0.5413])\n",
      "Labeled:  100 0.19999999999999998 0.19999999999999998\n",
      "500 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:36, 16.60it/s, step size=6.11e-01, acc. prob=0.907]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.587, 0.712, 0.673, 0.675, 0.733] tensor([0.6316, 0.7577, 0.7254, 0.7392, 0.7489])\n",
      "Labeled:  500 0.8999999999999998 0.7999999999999999\n",
      "10 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [01:15,  7.94it/s, step size=2.54e-01, acc. prob=0.921]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.588, 0.495, 0.579, 0.604, 0.542] tensor([0.8142, 0.5560, 0.5709, 0.5814, 0.7210])\n",
      "Labeled:  10 0.49999999999999994 0.39999999999999997\n",
      "50 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:40, 14.76it/s, step size=6.26e-01, acc. prob=0.886]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.742, 0.692, 0.645, 0.726, 0.732] tensor([0.8404, 0.7724, 0.6936, 0.7464, 0.7701])\n",
      "Labeled:  50 0.7 0.6\n",
      "100 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:43, 13.77it/s, step size=4.70e-01, acc. prob=0.909]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.533, 0.647, 0.596, 0.643, 0.649] tensor([0.6126, 0.7538, 0.7165, 0.6963, 0.7176])\n",
      "Labeled:  100 0.7999999999999999 0.6\n",
      "500 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:37, 15.99it/s, step size=6.38e-01, acc. prob=0.893]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.652, 0.692, 0.716, 0.734, 0.727] tensor([0.6454, 0.7282, 0.7508, 0.7778, 0.7432])\n",
      "Labeled:  500 0.8999999999999998 0.7999999999999999\n",
      "10 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:44, 13.62it/s, step size=4.73e-01, acc. prob=0.920]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.544, 0.618, 0.526, 0.638, 0.402] tensor([0.6871, 0.8287, 0.6139, 0.8164, 0.7887])\n",
      "Labeled:  10 0.6 0.39999999999999997\n",
      "50 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:43, 13.73it/s, step size=5.66e-01, acc. prob=0.867]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.549, 0.537, 0.668, 0.703, 0.612] tensor([0.5306, 0.5838, 0.8558, 0.8543, 0.7201])\n",
      "Labeled:  50 0.7999999999999999 0.6\n",
      "100 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:38, 15.64it/s, step size=6.24e-01, acc. prob=0.905]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73, 0.707, 0.712, 0.633, 0.683] tensor([0.7562, 0.7819, 0.7884, 0.5862, 0.7211])\n",
      "Labeled:  100 0.7 0.6\n",
      "500 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:46, 12.77it/s, step size=6.10e-01, acc. prob=0.897]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.573, 0.575, 0.627, 0.516, 0.603] tensor([0.5995, 0.5797, 0.6216, 0.5285, 0.6168])\n",
      "Labeled:  500 0.8999999999999998 0.7999999999999999\n",
      "10 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:41, 14.63it/s, step size=5.89e-01, acc. prob=0.891]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76, 0.72, 0.757, 0.731, 0.776] tensor([0.7352, 0.8000, 0.8666, 0.8045, 0.8358])\n",
      "Labeled:  10 0.19999999999999998 0.19999999999999998\n",
      "50 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:44, 13.37it/s, step size=5.85e-01, acc. prob=0.877]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.697, 0.633, 0.688, 0.637, 0.666] tensor([0.7272, 0.6402, 0.8470, 0.7009, 0.8161])\n",
      "Labeled:  50 0.7 0.6\n",
      "100 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:46, 12.95it/s, step size=5.83e-01, acc. prob=0.903]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.64, 0.689, 0.753, 0.748, 0.667] tensor([0.7587, 0.8051, 0.6844, 0.8661, 0.7900])\n",
      "Labeled:  100 0.0 0.19999999999999998\n",
      "500 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:41, 14.51it/s, step size=6.07e-01, acc. prob=0.907]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.625, 0.658, 0.618, 0.581, 0.589] tensor([0.6499, 0.6557, 0.6392, 0.5859, 0.6043])\n",
      "Labeled:  500 0.9999999999999999 0.9999999999999999\n",
      "10 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:51, 11.72it/s, step size=4.63e-01, acc. prob=0.870]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.617, 0.638, 0.666, 0.675, 0.633] tensor([0.7131, 0.7009, 0.8070, 0.7341, 0.7329])\n",
      "Labeled:  10 0.6 0.39999999999999997\n",
      "50 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:40, 14.95it/s, step size=5.74e-01, acc. prob=0.860]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.615, 0.71, 0.695, 0.668, 0.598] tensor([0.7778, 0.6660, 0.7085, 0.7537, 0.7462])\n",
      "Labeled:  50 -0.7 -0.6\n",
      "100 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:42, 13.98it/s, step size=6.05e-01, acc. prob=0.893]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.623, 0.608, 0.575, 0.625, 0.602] tensor([0.8000, 0.5820, 0.5229, 0.6248, 0.8600])\n",
      "Labeled:  100 0.3 0.19999999999999998\n",
      "500 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:36, 16.27it/s, step size=5.82e-01, acc. prob=0.907]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.686, 0.578, 0.494, 0.688, 0.559] tensor([0.7500, 0.5896, 0.5227, 0.7460, 0.5510])\n",
      "Labeled:  500 0.8999999999999998 0.7999999999999999\n",
      "10 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:58, 10.32it/s, step size=3.39e-01, acc. prob=0.928]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.573, 0.574, 0.6, 0.583, 0.54] tensor([0.6547, 0.6319, 0.5633, 0.8947, 0.5502])\n",
      "Labeled:  10 0.3 0.19999999999999998\n",
      "50 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:42, 14.26it/s, step size=4.79e-01, acc. prob=0.911]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.444, 0.463, 0.539, 0.61, 0.598] tensor([0.6814, 0.5854, 0.8233, 0.6258, 0.7097])\n",
      "Labeled:  50 0.09999999999999999 0.0\n",
      "100 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:39, 15.32it/s, step size=6.64e-01, acc. prob=0.872]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.721, 0.681, 0.714, 0.7, 0.636] tensor([0.7598, 0.7616, 0.6496, 0.7992, 0.5613])\n",
      "Labeled:  100 0.19999999999999998 0.19999999999999998\n",
      "500 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:37, 15.97it/s, step size=6.82e-01, acc. prob=0.867]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.772, 0.705, 0.777, 0.711, 0.658] tensor([0.7765, 0.7329, 0.8207, 0.7910, 0.7247])\n",
      "Labeled:  500 0.8999999999999998 0.7999999999999999\n",
      "10 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:43, 13.90it/s, step size=4.83e-01, acc. prob=0.892]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.583, 0.61, 0.689, 0.648, 0.471] tensor([0.5618, 0.8811, 0.8378, 0.7709, 0.7872])\n",
      "Labeled:  10 0.3 0.19999999999999998\n",
      "50 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:41, 14.53it/s, step size=5.78e-01, acc. prob=0.904]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.738, 0.697, 0.6, 0.701, 0.69] tensor([0.8656, 0.6830, 0.5348, 0.8316, 0.7177])\n",
      "Labeled:  50 0.8999999999999998 0.7999999999999999\n",
      "100 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:38, 15.76it/s, step size=6.26e-01, acc. prob=0.875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.697, 0.669, 0.703, 0.708, 0.713] tensor([0.8199, 0.6606, 0.7697, 0.6999, 0.8704])\n",
      "Labeled:  100 0.6 0.39999999999999997\n",
      "500 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:35, 16.98it/s, step size=5.91e-01, acc. prob=0.928]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.612, 0.675, 0.642, 0.68, 0.662] tensor([0.6354, 0.7095, 0.6793, 0.7434, 0.6919])\n",
      "Labeled:  500 0.9999999999999999 0.9999999999999999\n",
      "10 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warmup:   7%|▋         | 44/600 [1:08:10, 92.96s/it, step size=6.42e-02, acc. prob=0.773]\n",
      "Sample: 100%|██████████| 600/600 [00:48, 12.45it/s, step size=2.56e-01, acc. prob=0.856]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.698, 0.711, 0.676, 0.749, 0.708] tensor([0.7279, 0.7223, 0.6694, 0.8943, 0.7277])\n",
      "Labeled:  10 0.6 0.39999999999999997\n",
      "50 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:40, 14.92it/s, step size=5.89e-01, acc. prob=0.896]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.671, 0.607, 0.695, 0.675, 0.668] tensor([0.7952, 0.5557, 0.8212, 0.7523, 0.6954])\n",
      "Labeled:  50 0.8999999999999998 0.7999999999999999\n",
      "100 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:37, 16.05it/s, step size=6.03e-01, acc. prob=0.911]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.766, 0.757, 0.741, 0.732, 0.669] tensor([0.7039, 0.6880, 0.7795, 0.8029, 0.7360])\n",
      "Labeled:  100 -0.6 -0.39999999999999997\n",
      "500 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:36, 16.55it/s, step size=6.55e-01, acc. prob=0.881]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.719, 0.703, 0.682, 0.708, 0.658] tensor([0.7258, 0.7791, 0.7584, 0.7716, 0.7233])\n",
      "Labeled:  500 0.3 0.19999999999999998\n",
      "10 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:46, 12.97it/s, step size=5.20e-01, acc. prob=0.842]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.59, 0.649, 0.611, 0.641, 0.596] tensor([0.5497, 0.7986, 0.7416, 0.8612, 0.6653])\n",
      "Labeled:  10 0.8999999999999998 0.7999999999999999\n",
      "50 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:47, 12.71it/s, step size=5.80e-01, acc. prob=0.889]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.547, 0.674, 0.631, 0.712, 0.659] tensor([0.5218, 0.8227, 0.7827, 0.8425, 0.7988])\n",
      "Labeled:  50 0.9999999999999999 0.9999999999999999\n",
      "100 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:44, 13.59it/s, step size=5.23e-01, acc. prob=0.893]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.606, 0.625, 0.528, 0.539, 0.629] tensor([0.8169, 0.5597, 0.7232, 0.5597, 0.7022])\n",
      "Labeled:  100 -0.09999999999999999 0.0\n",
      "500 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:38, 15.44it/s, step size=6.78e-01, acc. prob=0.884]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.701, 0.739, 0.691, 0.737, 0.718] tensor([0.7488, 0.7666, 0.7385, 0.7730, 0.7653])\n",
      "Labeled:  500 0.8999999999999998 0.7999999999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# print(spearmans_labeled)\n",
    "nsims = 25\n",
    "# labels = [5, 10, 25, 50, 100]\n",
    "\n",
    "# spearmans_unlabeled = []\n",
    "# kendalls_unlabeled = []\n",
    "\n",
    "labels = [10, 50, 100, 500]\n",
    "# spearmans_labeled = {key: [] for key in labels}\n",
    "# kendalls_labeled = {key: [] for key in labels}\n",
    "\n",
    "# spearmans_labeled = [[] for i in range(len(labels))]\n",
    "# kendalls_labeled = [[] for i in range(len(labels))]\n",
    "i = 1\n",
    "\n",
    "for sim in range(nsims):\n",
    "    for i, label in enumerate(labels):\n",
    "        print(label, sim)\n",
    "        # label = 999\n",
    "        rankings, predictions_on_N, true_y = simulate_models_and_predictions(num_models=5, N=1000, num_features=5)\n",
    "        accs_only = [tup[1] for tup in rankings]\n",
    "\n",
    "        match_rates = []\n",
    "        for i in range(predictions_on_N.shape[1]):\n",
    "            match_rate = np.mean(predictions_on_N[:, i] == true_y)\n",
    "            match_rates.append(match_rate)\n",
    "\n",
    "        N=1000\n",
    "        indices = observed_indices = torch.randperm(N)[:label]\n",
    "        # print(indices)\n",
    "\n",
    "        mask = [False for i in range(N)]\n",
    "        for index in indices:\n",
    "            mask[index] = True\n",
    "\n",
    "\n",
    "        pyro.clear_param_store()\n",
    "        logging.getLogger(\"pyro\").setLevel(logging.DEBUG)\n",
    "        nuts_kernel = NUTS(corrected_dawid_skene_model_with_observed, jit_compile=True, ignore_jit_warnings=True)\n",
    "        mcmc = MCMC(nuts_kernel, num_samples=100, warmup_steps=500)\n",
    "\n",
    "        observed_y = torch.full((N,), -1, dtype=torch.int64)\n",
    "        observed_y[indices] = torch.tensor(true_y[indices])\n",
    "        mcmc.run(torch.tensor(predictions_on_N), observed_y, torch.tensor(mask))\n",
    "\n",
    "        posterior_samples_corrected = mcmc.get_samples()\n",
    "\n",
    "        theta = posterior_samples_corrected[\"theta\"]\n",
    "        pi = posterior_samples_corrected[\"pi\"]\n",
    "        # 1. & 3. Take max values from rows 0 and 1 respectively\n",
    "        max_row_0 = torch.max(theta[:,:,0,:], dim=2)[0]  # Shape: (500, 5)\n",
    "        max_row_1 = torch.max(theta[:,:,1,:], dim=2)[0]  # Shape: (500, 5)\n",
    "\n",
    "        # 2. & 4. Multiply with pi values\n",
    "        result_row_0 = max_row_0 * pi[:,0].unsqueeze(1)  # Shape: (500, 5)\n",
    "        result_row_1 = max_row_1 * pi[:,1].unsqueeze(1)  # Shape: (500, 5)\n",
    "\n",
    "        # 5. Sum the results\n",
    "        result = result_row_0 + result_row_1  # Shape: (500, 5)\n",
    "\n",
    "        # result = final_result.mean(dim=0)\n",
    "\n",
    "        \n",
    "        # resulting_ranking = torch.argsort(result.mean(dim=0), descending=True)\n",
    "        print(match_rates, result.mean(dim=0))\n",
    "\n",
    "        sp_l = spearmanr(result.mean(dim=0), match_rates)[0]\n",
    "        kend_l = kendalltau(result.mean(dim=0), match_rates)[0]\n",
    "        print(\"Labeled: \", label, sp_l, kend_l)\n",
    "\n",
    "        spearmans_labeled[label].append(sp_l)\n",
    "        kendalls_labeled[label].append(kend_l)\n",
    "\n",
    "        # n_examples = predictions_on_N.shape[0]\n",
    "        # n_classes = 2\n",
    "        # n_models = predictions_on_N.shape[1]\n",
    "\n",
    "        # one_hot_predictions = np.zeros((n_examples, n_models, n_classes))\n",
    "\n",
    "        # # Loop over both classes\n",
    "        # # Loop over both classes\n",
    "        # for class_idx in range(n_classes):\n",
    "        #     # Loop over each model (column)\n",
    "        #     for i in range(n_models):\n",
    "        #         one_class_preds = (predictions_on_N[:,i] == class_idx).astype(int)\n",
    "        #         one_hot_predictions[np.arange(n_examples), i, class_idx] = one_class_preds\n",
    "            \n",
    "\n",
    "        # _, _, _, _, prevalence, error_rates, _ = run(one_hot_predictions)\n",
    "        # weighted_diagonal_sums = np.array([np.sum(matrix.diagonal() * prevalence)/2 for matrix in error_rates])\n",
    "        # print(weighted_diagonal_sums, match_rates)\n",
    "        # sp_u = spearmanr(weighted_diagonal_sums, match_rates)[0]\n",
    "        # kend_u = kendalltau(weighted_diagonal_sums, match_rates)[0]\n",
    "        # print(\"Unlabeled: \", sp_u, kend_u)\n",
    "        # spearmans_unlabeled.append(sp_u)\n",
    "        # kendalls_unlabeled.append(kend_u)\n",
    "\n",
    "\n",
    "# with open('kendalls_unlabeled.pkl', 'wb') as f:\n",
    "#     pickle.dump(kendalls_unlabeled, f)\n",
    "\n",
    "        # with open('kendalls_labeled.pkl', 'wb') as f:\n",
    "        #     pickle.dump(kendalls_labeled, f)\n",
    "\n",
    "# with open('spearmans_unlabeled.pkl', 'wb') as f:\n",
    "#     pickle.dump(spearmans_unlabeled, f)\n",
    "\n",
    "        # with open('spearmans_labeled.pkl', 'wb') as f:\n",
    "        #     pickle.dump(spearmans_labeled, f)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = [key for key in kendalls_labeled.keys()]\n",
    "values = list(kendalls_labeled.values())\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(values, vert=True, patch_artist=True, labels=labels)\n",
    "plt.title(\"Box Plot\")\n",
    "plt.xlabel(\"Keys\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4204 0.4566 0.3873 0.4539 0.4199] [0.783, 0.805, 0.709, 0.815, 0.763]\n",
      "[0.449  0.2849 0.345  0.3642 0.4091] [0.691, 0.635, 0.633, 0.658, 0.671]\n",
      "[0.3519 0.3611 0.3862 0.4051 0.4316] [0.663, 0.648, 0.7, 0.708, 0.736]\n",
      "[0.2797 0.4676 0.2816 0.4022 0.389 ] [0.637, 0.663, 0.619, 0.6, 0.666]\n",
      "[0.4417 0.3543 0.4123 0.396  0.3871] [0.751, 0.649, 0.743, 0.746, 0.733]\n",
      "[0.4189 0.3335 0.3009 0.3627 0.3673] [0.622, 0.689, 0.665, 0.556, 0.573]\n",
      "[0.1163 0.2584 0.4072 0.4744 0.2823] [0.421, 0.572, 0.676, 0.671, 0.639]\n",
      "[0.3726 0.2962 0.4987 0.2607 0.2451] [0.659, 0.653, 0.675, 0.555, 0.631]\n",
      "[0.3661 0.4243 0.348  0.3541 0.326 ] [0.666, 0.69, 0.64, 0.629, 0.609]\n",
      "[0.3659 0.3202 0.3823 0.3959 0.4294] [0.681, 0.712, 0.636, 0.653, 0.68]\n",
      "[0.3133 0.4284 0.2814 0.3468 0.4675] [0.626, 0.747, 0.537, 0.666, 0.779]\n",
      "[0.4156 0.4163 0.4186 0.3011 0.2366] [0.66, 0.649, 0.694, 0.579, 0.563]\n",
      "[0.2708 0.4389 0.3562 0.2283 0.3367] [0.597, 0.643, 0.623, 0.462, 0.625]\n",
      "[0.4322 0.4181 0.3766 0.4146 0.4009] [0.691, 0.69, 0.641, 0.676, 0.675]\n",
      "[0.3121 0.3821 0.3561 0.3813 0.4271] [0.474, 0.688, 0.534, 0.665, 0.656]\n",
      "[0.2815 0.3713 0.3652 0.4314 0.2583] [0.492, 0.602, 0.63, 0.633, 0.585]\n",
      "[0.4348 0.417  0.3804 0.3241 0.4664] [0.711, 0.748, 0.681, 0.732, 0.724]\n",
      "[0.375  0.3861 0.3904 0.4704 0.3458] [0.635, 0.775, 0.791, 0.769, 0.563]\n",
      "[0.3291 0.3593 0.3228 0.299  0.4536] [0.665, 0.646, 0.622, 0.582, 0.691]\n",
      "[0.197  0.3938 0.3621 0.3926 0.4812] [0.662, 0.519, 0.694, 0.661, 0.617]\n",
      "[0.315  0.4531 0.2249 0.4373 0.3068] [0.679, 0.627, 0.576, 0.633, 0.704]\n",
      "[0.3862 0.313  0.3949 0.3912 0.3827] [0.752, 0.697, 0.703, 0.666, 0.658]\n",
      "[0.4268 0.2663 0.4358 0.2072 0.4197] [0.733, 0.652, 0.726, 0.577, 0.718]\n",
      "[0.3022 0.3519 0.3988 0.3037 0.2485] [0.513, 0.674, 0.643, 0.602, 0.567]\n",
      "[0.3395 0.3448 0.4591 0.416  0.347 ] [0.588, 0.638, 0.706, 0.712, 0.684]\n",
      "[0.3592 0.3228 0.3734 0.3811 0.4459] [0.509, 0.604, 0.508, 0.666, 0.648]\n",
      "[0.349  0.3254 0.3703 0.4073 0.4197] [0.619, 0.631, 0.737, 0.675, 0.69]\n",
      "[0.3824 0.3703 0.3292 0.4722 0.2968] [0.502, 0.627, 0.761, 0.587, 0.746]\n",
      "[0.4038 0.4612 0.2331 0.3905 0.3853] [0.74, 0.781, 0.475, 0.706, 0.743]\n",
      "[0.3633 0.3658 0.4018 0.3774 0.3859] [0.603, 0.657, 0.646, 0.639, 0.599]\n",
      "[0.4183 0.2705 0.4297 0.4552 0.4104] [0.678, 0.637, 0.721, 0.707, 0.668]\n",
      "[0.4316 0.383  0.362  0.4069 0.3717] [0.725, 0.717, 0.695, 0.702, 0.698]\n",
      "[0.3937 0.2343 0.1988 0.3087 0.4315] [0.621, 0.599, 0.539, 0.562, 0.612]\n",
      "[0.3895 0.3326 0.2079 0.4418 0.3772] [0.707, 0.602, 0.444, 0.695, 0.65]\n",
      "[0.2525 0.3674 0.4147 0.3835 0.2744] [0.74, 0.593, 0.567, 0.505, 0.689]\n",
      "[0.3824 0.3462 0.3539 0.4515 0.2791] [0.541, 0.564, 0.53, 0.511, 0.582]\n",
      "[0.3856 0.471  0.2622 0.4077 0.2862] [0.756, 0.74, 0.608, 0.681, 0.658]\n",
      "[0.3766 0.4054 0.3575 0.4058 0.4522] [0.673, 0.735, 0.739, 0.758, 0.78]\n",
      "[0.4176 0.4304 0.4412 0.378  0.4151] [0.747, 0.754, 0.767, 0.71, 0.735]\n",
      "[0.4334 0.341  0.3893 0.2884 0.4025] [0.734, 0.72, 0.712, 0.619, 0.672]\n",
      "[0.3992 0.3643 0.4025 0.3639 0.3961] [0.726, 0.681, 0.704, 0.644, 0.69]\n",
      "[0.446  0.3817 0.2854 0.3448 0.368 ] [0.616, 0.572, 0.668, 0.542, 0.594]\n",
      "[0.3091 0.1418 0.3638 0.4304 0.4746] [0.57, 0.427, 0.702, 0.724, 0.698]\n",
      "[0.4136 0.2662 0.3835 0.3775 0.3612] [0.673, 0.519, 0.672, 0.619, 0.661]\n",
      "[0.412  0.3737 0.4242 0.4244 0.3875] [0.666, 0.716, 0.7, 0.687, 0.753]\n",
      "[0.4016 0.4055 0.4001 0.4444 0.394 ] [0.684, 0.749, 0.739, 0.752, 0.693]\n",
      "[0.4445 0.2956 0.4445 0.3962 0.3385] [0.732, 0.606, 0.731, 0.704, 0.61]\n",
      "[0.4195 0.4453 0.4253 0.1506 0.3588] [0.709, 0.732, 0.683, 0.444, 0.638]\n",
      "[0.2391 0.3442 0.4107 0.4474 0.4545] [0.581, 0.744, 0.658, 0.788, 0.753]\n",
      "[0.3828 0.3396 0.3743 0.3603 0.4021] [0.687, 0.653, 0.669, 0.631, 0.663]\n",
      "[0.4303 0.3581 0.347  0.346  0.4434] [0.731, 0.619, 0.649, 0.656, 0.758]\n",
      "[0.4292 0.456  0.357  0.3722 0.435 ] [0.744, 0.757, 0.685, 0.699, 0.79]\n",
      "[0.3251 0.2108 0.2845 0.3566 0.4853] [0.65, 0.661, 0.683, 0.609, 0.574]\n",
      "[0.3861 0.4187 0.2525 0.4134 0.3378] [0.657, 0.703, 0.443, 0.675, 0.644]\n",
      "[0.3661 0.1122 0.3318 0.4671 0.3306] [0.584, 0.613, 0.656, 0.486, 0.598]\n",
      "[0.4162 0.3068 0.3216 0.4175 0.3928] [0.626, 0.702, 0.519, 0.672, 0.736]\n",
      "[0.3999 0.4117 0.3142 0.39   0.4468] [0.658, 0.705, 0.623, 0.629, 0.714]\n",
      "[0.3372 0.0523 0.2121 0.3098 0.3163] [0.449, 0.595, 0.535, 0.472, 0.535]\n",
      "[0.3411 0.4097 0.4051 0.4268 0.3749] [0.558, 0.688, 0.681, 0.703, 0.644]\n",
      "[0.3366 0.404  0.3674 0.4047 0.3816] [0.599, 0.707, 0.664, 0.693, 0.658]\n",
      "[0.4153 0.4005 0.3941 0.4559 0.3558] [0.718, 0.712, 0.676, 0.733, 0.743]\n",
      "[0.3094 0.3772 0.3991 0.3072 0.4733] [0.594, 0.649, 0.667, 0.572, 0.712]\n",
      "[0.3085 0.3691 0.4406 0.146  0.3999] [0.593, 0.641, 0.683, 0.448, 0.6]\n",
      "[0.4667 0.346  0.3099 0.3774 0.3926] [0.763, 0.631, 0.668, 0.747, 0.699]\n",
      "[0.3772 0.4215 0.4217 0.3886 0.44  ] [0.648, 0.747, 0.77, 0.701, 0.752]\n",
      "[0.4233 0.2678 0.4021 0.4002 0.29  ] [0.694, 0.531, 0.713, 0.681, 0.59]\n",
      "[0.3425 0.3841 0.1158 0.4223 0.2411] [0.447, 0.514, 0.639, 0.486, 0.547]\n",
      "[0.3771 0.4146 0.3106 0.3796 0.4352] [0.684, 0.725, 0.656, 0.681, 0.728]\n",
      "[0.4196 0.3183 0.3688 0.4145 0.3734] [0.7, 0.574, 0.698, 0.731, 0.681]\n",
      "[0.4033 0.3701 0.3892 0.4517 0.3999] [0.747, 0.701, 0.685, 0.735, 0.706]\n",
      "[0.3635 0.3745 0.3688 0.4163 0.4434] [0.614, 0.592, 0.6, 0.612, 0.648]\n",
      "[0.4112 0.3126 0.2246 0.451  0.4227] [0.649, 0.685, 0.565, 0.665, 0.706]\n",
      "[0.2233 0.3928 0.4154 0.47   0.2918] [0.504, 0.773, 0.71, 0.751, 0.645]\n",
      "[0.2316 0.4538 0.2113 0.3705 0.3841] [0.627, 0.625, 0.634, 0.47, 0.639]\n",
      "[0.4042 0.4082 0.3405 0.21   0.3422] [0.675, 0.695, 0.623, 0.47, 0.626]\n",
      "[0.3693 0.3893 0.4008 0.4351 0.408 ] [0.715, 0.689, 0.724, 0.777, 0.73]\n",
      "[0.1906 0.3452 0.3291 0.3904 0.4147] [0.498, 0.605, 0.584, 0.613, 0.612]\n",
      "[0.3592 0.3991 0.3541 0.395  0.4119] [0.604, 0.71, 0.645, 0.675, 0.68]\n",
      "[0.3382 0.3115 0.3252 0.4616 0.2003] [0.664, 0.617, 0.619, 0.594, 0.651]\n",
      "[0.331  0.3761 0.3651 0.4271 0.4039] [0.681, 0.633, 0.696, 0.657, 0.65]\n",
      "[0.3853 0.3865 0.3959 0.3641 0.3206] [0.657, 0.675, 0.656, 0.619, 0.615]\n",
      "[0.3449 0.3637 0.1303 0.2526 0.2966] [0.471, 0.496, 0.497, 0.607, 0.581]\n",
      "[0.4188 0.4489 0.4401 0.4081 0.3785] [0.771, 0.777, 0.791, 0.771, 0.703]\n",
      "[0.4315 0.3831 0.3607 0.4096 0.4127] [0.761, 0.729, 0.673, 0.75, 0.757]\n",
      "[0.2829 0.4474 0.4689 0.2586 0.4236] [0.685, 0.676, 0.718, 0.653, 0.716]\n",
      "[0.3034 0.3882 0.3231 0.3355 0.4418] [0.687, 0.619, 0.574, 0.7, 0.691]\n",
      "[0.3657 0.4166 0.4445 0.4352 0.3486] [0.729, 0.766, 0.707, 0.716, 0.689]\n",
      "[0.3698 0.3822 0.401  0.2331 0.3369] [0.613, 0.599, 0.613, 0.589, 0.645]\n",
      "[0.2818 0.3513 0.4386 0.4106 0.3168] [0.677, 0.655, 0.662, 0.619, 0.544]\n",
      "[0.4581 0.4111 0.3874 0.3986 0.3083] [0.737, 0.698, 0.716, 0.669, 0.599]\n",
      "[0.3747 0.4567 0.4208 0.3786 0.4205] [0.734, 0.793, 0.79, 0.676, 0.75]\n",
      "[0.2727 0.3977 0.4684 0.3397 0.3067] [0.515, 0.645, 0.665, 0.63, 0.6]\n",
      "[0.4289 0.4176 0.3044 0.4097 0.3252] [0.641, 0.635, 0.717, 0.73, 0.625]\n",
      "[0.3803 0.317  0.4589 0.4234 0.4096] [0.7, 0.602, 0.777, 0.745, 0.789]\n",
      "[0.4408 0.2746 0.3976 0.357  0.3682] [0.686, 0.589, 0.659, 0.617, 0.726]\n",
      "[0.3872 0.3775 0.2312 0.3986 0.2135] [0.478, 0.564, 0.685, 0.491, 0.577]\n",
      "[0.2826 0.345  0.3491 0.4376 0.3911] [0.571, 0.579, 0.666, 0.642, 0.582]\n",
      "[0.4208 0.4064 0.2771 0.3109 0.4084] [0.708, 0.701, 0.531, 0.631, 0.714]\n",
      "[0.202  0.4321 0.4014 0.4118 0.4366] [0.5, 0.674, 0.677, 0.616, 0.655]\n",
      "[0.3866 0.2814 0.3311 0.3701 0.3533] [0.558, 0.545, 0.563, 0.629, 0.54]\n"
     ]
    }
   ],
   "source": [
    "kend_us = []\n",
    "for sim in range(100):\n",
    "\n",
    "    rankings, predictions_on_N, true_y = simulate_models_and_predictions(num_models=5, N=1000, num_features=5)\n",
    "    accs_only = [tup[1] for tup in rankings]\n",
    "\n",
    "    match_rates = []\n",
    "    for i in range(predictions_on_N.shape[1]):\n",
    "        match_rate = np.mean(predictions_on_N[:, i] == true_y)\n",
    "        match_rates.append(match_rate)\n",
    "\n",
    "\n",
    "    n_examples = predictions_on_N.shape[0]\n",
    "    n_classes = 2\n",
    "    n_models = predictions_on_N.shape[1]\n",
    "\n",
    "    one_hot_predictions = np.zeros((n_examples, n_models, n_classes))\n",
    "\n",
    "    # Loop over both classes\n",
    "    # Loop over both classes\n",
    "    for class_idx in range(n_classes):\n",
    "        # Loop over each model (column)\n",
    "        for i in range(n_models):\n",
    "            one_class_preds = (predictions_on_N[:,i] == class_idx).astype(int)\n",
    "            one_hot_predictions[np.arange(n_examples), i, class_idx] = one_class_preds\n",
    "        \n",
    "\n",
    "    _, _, _, _, prevalence, error_rates, _ = run(one_hot_predictions)\n",
    "    weighted_diagonal_sums = np.array([np.sum(matrix.diagonal() * prevalence)/2 for matrix in error_rates])\n",
    "    print(weighted_diagonal_sums, match_rates)\n",
    "    sp_u = spearmanr(weighted_diagonal_sums, match_rates)[0]\n",
    "    kend_u = kendalltau(weighted_diagonal_sums, match_rates)[0]\n",
    "    kend_us.append(kend_u)\n",
    "\n",
    "# kend_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2IAAAIjCAYAAABh3KjvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB8IklEQVR4nO3deVyU9fr/8fcwyKqACgqKCooFJmKKgno4aXnCyo64luVxyWy1xbVd00rLtCyzPK3qyVYjSutYplmUhImah8IdMmVJXEARRZj790c/5usIKCjOwPh6Ph48aq77c3/muua+HbjmXsZkGIYhAAAAAIDduDg6AQAAAAC41NCIAQAAAICd0YgBAAAAgJ3RiAEAAACAndGIAQAAAICd0YgBAAAAgJ3RiAEAAACAndGIAQAAAICd0YgBAAAAgJ3RiAGAk8nKypLJZNLcuXNrbc5169bJZDJp3bp1tTanszKZTBo/fvxZx5Rvo8WLF9snKQBAnUMjBgB1wOLFi2UymbRx40ZHp1Lv9e7dWyaTyfrj6empTp06af78+bJYLI5Or0758ssvZTKZ1KJFC14bALAzV0cnAABAbQsODtbs2bMlSfn5+Xrvvfc0YcIEHThwQM8884yDs5PatGmj4uJiNWjQwKF5LFu2TCEhIcrKytLatWvVt29fh+YDAJcSjogBAJyOr6+vRowYoREjRujBBx/U999/rzZt2mjBggUqKytzdHoymUzy8PCQ2Wx2WA5FRUX67LPPNHHiRF155ZVatmyZw3I5l6KiIkenAAC1jkYMAOqJkpISTZs2TV27dpWvr6+8vb0VFxenb7/9tsp1XnzxRbVp00aenp666qqrlJ6eXmHMtm3bNGTIEDVp0kQeHh6Kjo7W559/fs58du7cqcGDByswMFAeHh4KDg7WzTffrIKCgirXGT9+vBo2bKjjx49XWDZ8+HAFBgZaG6WNGzcqPj5e/v7+8vT0VGhoqG677bZz5lUZDw8PdevWTUePHtWff/5pjW/dulWjR49W27Zt5eHhocDAQN122206ePCgzfpPPvmkTCaTdu3apdGjR8vPz0++vr4aM2ZMpbWc6emnn5aLi4sWLFggqfJrxEaPHq2GDRtq//79SkhIUMOGDRUQEKDJkydXaB4PHjyof/3rX/Lx8ZGfn59GjRqlX375pUbXnX366acqLi7W0KFDdfPNNysxMVEnTpyoMO7EiRN68sknddlll8nDw0NBQUEaNGiQdu/ebR1jsVj00ksvKTIyUh4eHgoICFC/fv2sp9qe7Zo4k8mkJ5980vq4/LX+7bffdMstt6hx48b629/+Jqn620uS9u/fr7Fjx6pFixZyd3dXaGio7r77bpWUlGjPnj0ymUx68cUXK6y3fv16mUwmvf/++9V6HQHgfHFqIgDUE4WFhXrzzTc1fPhwjRs3TkePHtVbb72l+Ph4bdiwQZ07d7YZv3TpUh09elT33nuvTpw4oZdeeklXX321/ve//6l58+aSpF9//VW9evVSy5Yt9fDDD8vb21sfffSREhIS9Mknn2jgwIGV5lJSUqL4+HidPHlS9913nwIDA7V//36tXLlSR44cka+vb6Xr3XTTTVq4cKG++OILDR061Bo/fvy4VqxYodGjR8tsNuvPP//Utddeq4CAAD388MPy8/NTVlaWEhMTz/v1K28G/Pz8rLHVq1drz549GjNmjAIDA/Xrr7/q9ddf16+//qqffvpJJpPJZo5hw4YpNDRUs2fP1qZNm/Tmm2+qWbNmeu6556p83scff1yzZs3Sv//9b40bN+6sOZaVlSk+Pl4xMTGaO3euvvnmG82bN0/t2rXT3XffLemvpufGG2/Uhg0bdPfddys8PFyfffaZRo0aVaPXY9myZerTp48CAwN188036+GHH9aKFStstktZWZn69++vNWvW6Oabb9YDDzygo0ePavXq1UpPT1e7du0kSWPHjtXixYt13XXX6fbbb1dpaamSk5P1008/KTo6ukZ5lRs6dKjat2+vWbNmyTAMSdXfXtnZ2erevbuOHDmiO+64Q+Hh4dq/f7+WL1+u48ePq23bturVq5eWLVumCRMmVHhdGjVqpAEDBpxX3gBQbQYAwOHeeecdQ5Lx888/VzmmtLTUOHnypE3s8OHDRvPmzY3bbrvNGsvMzDQkGZ6ensa+ffus8dTUVEOSMWHCBGvsmmuuMSIjI40TJ05YYxaLxejZs6fRvn17a+zbb781JBnffvutYRiGsXnzZkOS8fHHH9eoTovFYrRs2dIYPHiwTfyjjz4yJBnff/+9YRiG8emnn57z9ajKVVddZYSHhxsHDhwwDhw4YGzbts2YMmWKIcm44YYbbMYeP368wvrvv/++TS6GYRjTp083JNm8zoZhGAMHDjSaNm1qE5Nk3HvvvYZhGMakSZMMFxcXY/HixTZjyrfRO++8Y42NGjXKkGTMnDnTZuyVV15pdO3a1fr4k08+MSQZ8+fPt8bKysqMq6++usKcVcnLyzNcXV2NN954wxrr2bOnMWDAAJtxb7/9tiHJeOGFFyrMYbFYDMMwjLVr1xqSjPvvv7/KMZXVW06SMX36dOvj8td6+PDhFcZWd3uNHDnScHFxqXT/Kc/p3//+tyHJyMjIsC4rKSkx/P39jVGjRlVYDwBqG6cmAkA9YTab5ebmJumvoyKHDh1SaWmpoqOjtWnTpgrjExIS1LJlS+vj7t27KyYmRl9++aUk6dChQ1q7dq2GDRumo0ePKj8/X/n5+Tp48KDi4+O1c+dO7d+/v9Jcyo94ffXVV9U6Na+cyWTS0KFD9eWXX+rYsWPW+IcffqiWLVtaT0ErP2q1cuVKnTp1qtrzl9u2bZsCAgIUEBCg8PBwPf/88/rnP/9Z4dQ4T09P6/+fOHFC+fn5io2NlaRKX9O77rrL5nFcXJwOHjyowsJCm7hhGBo/frxeeuklvfvuuzU6WlXZc+zZs8f6eNWqVWrQoIHN0TUXFxfde++91X6ODz74QC4uLho8eLA1Nnz4cP33v//V4cOHrbFPPvlE/v7+uu+++yrMUX706ZNPPpHJZNL06dOrHHM+znwdpOptL4vFoqSkJN14442VHo0rz2nYsGHy8PCwuTbuq6++Un5+vkaMGHHeeQNAddGIAUA9smTJEnXq1EkeHh5q2rSpAgIC9MUXX1R6XVb79u0rxC677DJlZWVJknbt2iXDMPTEE09Ym5byn/I/qk+/nup0oaGhmjhxot588035+/srPj5eCxcuPOv1YeVuuukmFRcXW69DO3bsmL788ksNHTrU+kfyVVddpcGDB2vGjBny9/fXgAED9M477+jkyZPVep1CQkK0evVqffXVV3r11VfVsmVLHThwQB4eHjbjDh06pAceeEDNmzeXp6enAgICFBoaKkmV1tK6dWubx40bN5Ykm+ZF+uu00IULF2rBggUaPnx4tXKWZL2+6sznOH3+33//XUFBQfLy8rIZFxYWVu3neffdd9W9e3cdPHhQu3bt0q5du3TllVeqpKREH3/8sXXc7t27dfnll8vVteorGXbv3q0WLVqoSZMm1X7+6ijfDqerzvY6cOCACgsL1bFjx7PO7+fnpxtvvFHvvfeeNbZs2TK1bNlSV199dS1WAgCV4xoxAKgn3n33XY0ePVoJCQmaMmWKmjVrJrPZrNmzZ9vcOKG6yr83avLkyYqPj690zNn+uJ83b55Gjx6tzz77TF9//bXuv/9+zZ49Wz/99JOCg4OrXC82NlYhISH66KOPdMstt2jFihUqLi7WTTfdZB1jMpm0fPly/fTTT1qxYoW++uor3XbbbZo3b55++uknNWzY8Ky1eXt729yKvVevXurSpYseffRRvfzyy9b4sGHDtH79ek2ZMkWdO3dWw4YNZbFY1K9fv0q/V6uquxwa//8aptOfb8uWLXrllVc0bNiwajcp9riL4s6dO/Xzzz9LqrxZX7Zsme64445afc6qjoyd7Q6Wpx/9KlfT7XUuI0eO1Mcff6z169crMjJSn3/+ue655x65uPA5NYCLj0YMAOqJ5cuXq23btkpMTLT5w7ayU8Kkv/7gPtOOHTsUEhIiSWrbtq0kqUGDBuf9/VGRkZGKjIzU448/rvXr16tXr15atGiRnn766bOuN2zYML300ksqLCzUhx9+qJCQEOspZqeLjY1VbGysnnnmGb333nu69dZb9cEHH+j222+vUZ6dOnXSiBEj9O9//1uTJ09W69atdfjwYa1Zs0YzZszQtGnTrGMre91qKiwsTHPmzFHv3r3Vr18/rVmzRo0aNbrgeaW/voPs22+/1fHjx22Oiu3atata6y9btkwNGjTQf/7znwqN3w8//KCXX35Ze/fuVevWrdWuXTulpqbq1KlTVX7nWbt27fTVV1/p0KFDVTac5UcOjxw5YhP//fffq5WzpGpvr4CAAPn4+FR6h9Az9evXTwEBAVq2bJliYmJ0/Phx/etf/6p2TgBwIfjIBwDqifI/mk8/+pKamqqUlJRKxyclJdlc47VhwwalpqbquuuukyQ1a9ZMvXv31r///W/l5ORUWP/AgQNV5lJYWKjS0lKbWGRkpFxcXKp1+uBNN92kkydPasmSJVq1apWGDRtms/zw4cMVjjKV3xWyuqcnnmnq1Kk6deqUXnjhBUmVv56SNH/+/POa/0ydOnXSl19+qYyMDN14440qLi6ulXnj4+N16tQpvfHGG9aYxWLRwoULq7X+smXLFBcXp5tuuklDhgyx+ZkyZYokWW/dPnjwYOXn5+uVV16pME/56zZ48GAZhqEZM2ZUOcbHx0f+/v76/vvvbZa/+uqr1cpZqv72cnFxUUJCglasWGG9fX5lOUmSq6urhg8fro8++kiLFy9WZGSkOnXqVO2cAOBCcEQMAOqQt99+W6tWraoQf+CBB9S/f38lJiZq4MCBuuGGG5SZmalFixapQ4cONje+KBcWFqa//e1vuvvuu3Xy5EnNnz9fTZs21dSpU61jFi5cqL/97W+KjIzUuHHj1LZtW+Xl5SklJUX79u3TL7/8Ummea9eu1fjx4zV06FBddtllKi0ttR5hOf0GEFXp0qWLwsLC9Nhjj+nkyZM2pyVKf10L9+qrr2rgwIFq166djh49qjfeeEM+Pj66/vrrzzl/ZTp06KDrr79eb775pp544gk1bdpUf//73zVnzhydOnVKLVu21Ndff63MzMzzmr8ysbGx+uyzz3T99ddryJAhSkpKqvLIUnUlJCSoe/fumjRpknbt2qXw8HB9/vnnOnTokKSz3yAjNTVVu3bt0vjx4ytd3rJlS3Xp0kXLli3TQw89pJEjR2rp0qWaOHGiNmzYoLi4OBUVFembb77RPffcowEDBqhPnz7617/+pZdfflk7d+60niaYnJysPn36WJ/r9ttv17PPPqvbb79d0dHR+v7777Vjx45q1+3j41Pt7TVr1ix9/fXXuuqqq3THHXcoIiJCOTk5+vjjj/XDDz/YfIXByJEj9fLLL+vbb78969cQAECtc9DdGgEApym/fX1VP3/88YdhsViMWbNmGW3atDHc3d2NK6+80li5cqUxatQoo02bNta5ym8V/vzzzxvz5s0zWrVqZbi7uxtxcXHGL7/8UuG5d+/ebYwcOdIIDAw0GjRoYLRs2dLo37+/sXz5cuuYM29fv2fPHuO2224z2rVrZ3h4eBhNmjQx+vTpY3zzzTfVrvmxxx4zJBlhYWEVlm3atMkYPny40bp1a8Pd3d1o1qyZ0b9/f2Pjxo3nnPeqq64yrrjiikqXrVu3zuZ26fv27TMGDhxo+Pn5Gb6+vsbQoUON7OzsKm+pfuDAAZv5yrdbZmamNabTbl9f7rPPPjNcXV2Nm266ySgrK6vy9vXe3t4Vci5/7tMdOHDAuOWWW4xGjRoZvr6+xujRo40ff/zRkGR88MEHVb429913nyHJ2L17d5VjnnzySUOSdV85fvy48dhjjxmhoaFGgwYNjMDAQGPIkCE2c5SWlhrPP/+8ER4ebri5uRkBAQHGddddZ6SlpVnHHD9+3Bg7dqzh6+trNGrUyBg2bJjx559/Vvu1Nozqby/DMIzff//dGDlypBEQEGC4u7sbbdu2Ne69994KXwFhGIZxxRVXGC4uLjZf9wAAF5vJMM44xg8AAOqdpKQkDRw4UD/88IN69erl6HTqlSuvvFJNmjTRmjVrHJ0KgEsI14gBAFDPnHm9WVlZmRYsWCAfHx916dLFQVnVTxs3btSWLVs0cuRIR6cC4BLDNWIAANQz9913n4qLi9WjRw+dPHlSiYmJWr9+vWbNmlXpbd9RUXp6utLS0jRv3jwFBQVVuE4RAC42GjEAAOqZq6++WvPmzdPKlSt14sQJhYWFacGCBVXehAMVLV++XDNnztTll1+u999/v8KXfQPAxcY1YgAAAABgZ1wjBgAAAAB2RiMGAAAAAHbGNWK1wGKxKDs7W40aNTrrF2kCAAAAcG6GYejo0aNq0aKFXFyqPu5FI1YLsrOz1apVK0enAQAAAKCO+OOPPxQcHFzlchqxWtCoUSNJf73YPj4+Ds4GAAAAgKMUFhaqVatW1h6hKjRitaD8dEQfHx8aMQAAAADnvGSJm3UAAAAAgJ3RiAEAAACAndGIAQAAAICd0YgBAAAAgJ3RiAEAAACAndGIAQAAAICd0YgBAAAAgJ3RiAEAAACAndGIAQAAAICd0YgBAAAAgJ3RiAEAAACAndGIAQAAAICd0YgBAAAAgJ25OjoBALhYSkpK9Oqrr2r37t1q166d7rnnHrm5uTk6LQAAgPp1ROz777/XjTfeqBYtWshkMikpKemc66xbt05dunSRu7u7wsLCtHjx4gpjFi5cqJCQEHl4eCgmJkYbNmyo/eQB2NXUqVPl7e2tCRMm6JVXXtGECRPk7e2tqVOnOjo1AACA+tWIFRUVKSoqSgsXLqzW+MzMTN1www3q06ePtmzZogcffFC33367vvrqK+uYDz/8UBMnTtT06dO1adMmRUVFKT4+Xn/++efFKgPARTZ16lQ9//zzatq0qd544w3l5OTojTfeUNOmTfX888/TjAEAAIczGYZhODqJ82EymfTpp58qISGhyjEPPfSQvvjiC6Wnp1tjN998s44cOaJVq1ZJkmJiYtStWze98sorkiSLxaJWrVrpvvvu08MPP1ytXAoLC+Xr66uCggL5+Picf1EALlhJSYm8vb3VtGlT7du3T66u/3cGdmlpqYKDg3Xw4EEVFRVxmiIAAKh11e0NnPoasZSUFPXt29cmFh8frwcffFDSX3+wpaWl6ZFHHrEud3FxUd++fZWSklLlvCdPntTJkyetjwsLCyX99UdeaWmpdR4XFxdZLBZZLBab+V1cXFRWVqbTe+Cq4mazWSaTyTrv6XFJKisrq1bc1dVVhmHYxE0mk8xmc4Ucq4qfOHFCO3bsqJWaTpw4od27d1tPCa2NmsLDw+Xt7V2jmpxxO13qNS1YsEClpaV6+umnK+RvNps1c+ZM3XnnnVqwYIEeeOCBelHT2eL1dTvVlZqOHz+ubdu21UpNJ06c0N69exUaGio3N7daqal9+/by8vKqUU3nitfH7eQMNRUXF2vnzp21UtPJkye1Z88etWnTxvo79EJrCg8Pl5eX1yW/nRxV0/79+3Xw4MFaqam4uFi7d++2yd3FxUWGYVSoqbK4i4uLTCZTlXGLxVKhpsriVeXYrl07eXh41Mp2atKkiYKDgyvEHb3vnbm8Kk7diOXm5qp58+Y2sebNm6uwsFDFxcU6fPiwysrKKh2zbdu2KuedPXu2ZsyYUSG+efNmeXt7S5ICAgLUrl07ZWZm6sCBA9YxwcHBCg4O1o4dO1RQUGCNt23bVs2aNVN6erqKi4ut8fDwcPn5+Wnz5s02O2CnTp3k5uamjRs32uQQHR2tkpISbd261Rozm83q1q2bCgoKbOry9PRUVFSU8vPztWfPHmvc19dXERERys7O1r59+6zxvLy8sx6BdLTFixerc+fONarJGbfTpV7TTz/9JEnq379/pTX179/fOq5Xr171oibJ+bZTXalp+/btGj16tOqqxYsX6/LLL5d0aW8nZ6gpKytLw4cPV11Vvq9d6tvJETXl5ubq5uG36OSJ/6sR1ePu4akP3n9PgYGBkurOvldUVFSt/J361MTLLrtMY8aMsTni9eWXX+qGG27Q8ePHdfjwYbVs2VLr169Xjx49rGOmTp2q7777TqmpqZXOW9kRsVatWungwYPWw4/O+KlPbR4R27Ztm0aMGKElS5YoPDy8VmriiBg1SdJLL72kyZMn64033tCYMWMq1PTGG2/ozjvv1Ny5czkiRk21ekRs27ZtGjVqlN59911ddtllHBFj37toR8S2b99e4XcoR8Tq7763adMmxcTEqGn/SWrQtJUulFFaotKCvAue52Jx9W0uk+uFXxpw6uAfOrhynlJTU9WlSxdJdWffKywsVNOmTS/tUxMDAwOVl2e7I+bl5cnHx0eenp4ym80ym82VjinvrCvj7u4ud3f3CnFXV1eb61Gk/9uQZyrfYNWNnznv+cRNJlOl8apyPDPesGFD645+oUwmkySpY8eOtTbn6apb07ni9XE7nSvu7DWVX9/5+OOPa/To0RWuEZs2bZpcXV113333VciprtZUnXh9207VidujJh8fH3Xv3r3S+Wqq/HkiIiIuyvtauUtxO11ovC7U1KhRo1rbL8prvFi/Q6VLdzudK8eLUVP58zdo2krugWGVjq+x4A61M089UJO/v+2171W1vEI+1RpVT/Xo0UNr1qyxia1evdp69MvNzU1du3a1GWOxWLRmzRqbI2QA6g83NzdNmDBBeXl5Cg4O1uuvv67s7Gy9/vrrCg4OVl5eniZMmMCNOgAAgEPVqyNix44d065du6yPMzMztWXLFjVp0kStW7fWI488ov3792vp0qWSpLvuukuvvPKKpk6dqttuu01r167VRx99pC+++MI6x8SJEzVq1ChFR0ere/fumj9/voqKijRmzBi71wegdsyZM0eS9OKLL+rOO++0xl1dXTVlyhTrcgAAAEepV43Yxo0b1adPH+vjiRMnSpJGjRqlxYsXKycnR3v37rUuDw0N1RdffKEJEybopZdeUnBwsN58803Fx8dbx9x00006cOCApk2bptzcXHXu3FmrVq2qcAMPAPXLnDlz9PTTT+vVV1/V7t271a5dO91zzz0cCQMAAHVCvWrEevfubXPB3JkWL15c6TqbN28+67zjx4/X+PHjLzQ9AHWMm5ub9esqAAAA6hKnvkYMAAAAAOoiGjEAAAAAsDMaMQAAAACwMxoxAAAAALAzGjEAAAAAsDMaMQAAAACwMxoxAAAAALAzGjEAAAAAsDMaMQAAAACwMxoxAAAAALAzGjEAAAAAsDMaMQAAAACwMxoxAAAAALAzGjEAAAAAsDMaMQAAAACwMxoxAAAAALAzGjEAAAAAsDMaMQAAAACwMxoxAAAAALAzGjEAAAAAsDMaMQAAAACwMxoxAAAAALAzGjEAAAAAsDMaMQAAAACwMxoxAAAAALAzGjEAAAAAsDMaMQAAAACwMxoxAAAAALAzGjEAAAAAsDMaMQAAAACwMxoxAAAAALAzGjEAAAAAsDMaMQAAAACwMxoxAAAAALAzV0cnAAAXS1lZmZKTk5WTk6OgoCDFxcXJbDY7Oi0AAACOiAFwTomJiQoLC1OfPn10yy23qE+fPgoLC1NiYqKjUwMAAKARA+B8EhMTNWTIEEVGRiolJUVHjx5VSkqKIiMjNWTIEJoxAADgcDRiAJxKWVmZJk2apP79+yspKUmxsbFq2LChYmNjlZSUpP79+2vy5MkqKytzdKoAAOASxjViTm7ftjQdzd7p6DQqyM3M1JWBLsrd/JUyjuxwdDoVNGrRXsHhXR2dBs5DcnKysrKy9P7778vFxfazJhcXFz3yyCPq2bOnkpOT1bt3b8ckiQuyd+9e5efnOzqNCjIyMmz+W9f4+/urdevWjk4DQCUCG5oUcXyzXA9lX/BcRtkplR09VAtZXRzmRk1kMje44HlKj+dJDU21kJHj0Ig5sb1792rpfb31eK+6d+AzQtL1dzaU/nhW+sPR2VT09I8WjXzrV/5oqYdycnIkSR07dqx0eXm8fBzql7179+ry8AidKD7u6FSqNGLECEenUCkPTy9t35bB+xpQx/j7++veWG893uFTR6dSv7SQnj7sLX9/f0dnct5oxJxYfn6+Fv5UpLWNR8rVr7mj07FR/mlNbX0qUptKj+Qp46eluj4/nz9Y6qGgoCBJUnp6umJjYyssT09PtxmH+iU/P18nio+raf9JatC0laPTsWGUlqi0IE+uvs1lcnVzdDo2Th38QwdXzlM+72tAndO6dWuNXrBOGbV0BtPJkyeVnX3hR9YulhYtWsjd3b1W5hp9c3sF1+P3NBoxJ5d7zJC8rpR7kzBHp1JRgKMTqNzJkl3KPbbE0WngPMXFxSkkJESzZs1SUlKSzemJFotFs2fPVmhoqOLi4hyYJS5Ug6at5B5YB9/Xgjs4OgMA9VBweFepFi+J6FxrM+FiqnvnrAHABTCbzZo3b55WrlyphIQEm7smJiQkaOXKlZo7dy7fJwYAAByKI2IAnM6gQYO0fPlyTZo0ST179rTGQ0NDtXz5cg0aNMiB2QEAANCIAXBSgwYN0oABA5ScnKycnBwFBQUpLi6OI2EAAKBOoBED4LTMZjO3qAcAAHUS14gBAAAAgJ3Vu0Zs4cKFCgkJkYeHh2JiYrRhw4Yqx/bu3Vsmk6nCzw033GAdM3r06ArL+/XrZ49SAAAAAFyi6tWpiR9++KEmTpyoRYsWKSYmRvPnz1d8fLy2b9+uZs2aVRifmJiokpIS6+ODBw8qKipKQ4cOtRnXr18/vfPOO9bHtfXdBgAAAABQmXp1ROyFF17QuHHjNGbMGHXo0EGLFi2Sl5eX3n777UrHN2nSRIGBgdaf1atXy8vLq0Ij5u7ubjOucePG9igHAAAAwCWq3hwRKykpUVpamh555BFrzMXFRX379lVKSkq15njrrbd08803y9vb2ya+bt06NWvWTI0bN9bVV1+tp59+Wk2bNq1ynpMnT+rkyZPWx4WFhZKk0tJSlZaWWnNzcXGRxWKRxWKxydnFxUVlZWUyDOOccbPZLJPJZJ339LgklZWVVRk/cx3UnD220+lcXV1lGIZN3GQyyWw2V9iXqorXhX2PmqjpYtV0+jjUXGlpqSwWC/tePa+pXGV/d9TXmpxxO1HTpVtTdf8GrzeNWH5+vsrKytS8eXObePPmzbVt27Zzrr9hwwalp6frrbfeson369dPgwYNUmhoqHbv3q1HH31U1113nVJSUqq8zfXs2bM1Y8aMCvHNmzdbm7yAgAC1a9dOmZmZOnDggHVMcHCwgoODtWPHDhUUFFjjbdu2VbNmzZSenq7i4mJrPDw8XH5+ftq8ebPNDtipUye5ublp48aNNjlER0erpKREW7du1fbt28/5uuDs7LGdypnNZnXr1k0FBQU2+7Snp6eioqKUn5+vPXv2WOO+vr6KiIhQdna29u3bZ43XhX2PmqjpYtV05i9o1ExGRoaaNm3KvlfPayqXkZFh/aOyvtfkjNuJmi7dmoqKilQdJqOefLyYnZ2tli1bav369erRo4c1PnXqVH333XdKTU096/p33nmnUlJSbDZuZfbs2aN27drpm2++0TXXXFPpmMqOiLVq1UoHDx6Uj4+PpLrxacKmTZsUExOjwFHz5R4Ydta68X9O5u5S7pIHlZaWpqioKD71oSZqqkM1bdq0SdHR0byv1VD5+1pqaqqio6PZ9+p5TVu2bFHXrl2VmpqqLl26OEVNzridqOnSramwsFBNmzZVQUGBtTeoTL05Iubv7y+z2ay8vDybeF5engIDA8+6blFRkT744APNnDnznM/Ttm1b+fv7a9euXVU2Yu7u7pXe0MPV1VWurrYvafmGPFNVR9uqip85b3XiVS1D9dljO53JZDJVGq9qX6ppnJqoqap4fajJZDJVOhbV4+rqat027Hv1u6by5z3zOepzTc64najp0qypun+D15ubdbi5ualr165as2aNNWaxWLRmzRqbI2SV+fjjj3Xy5EmNGDHinM+zb98+HTx4UEFBQRecMwAAAABUpt40YpI0ceJEvfHGG1qyZIkyMjJ09913q6ioSGPGjJEkjRw50uZmHuXeeustJSQkVLgBx7FjxzRlyhT99NNPysrK0po1azRgwACFhYUpPj7eLjUBAAAAuPTUq3PXbrrpJh04cEDTpk1Tbm6uOnfurFWrVllv4LF3794Khxu3b9+uH374QV9//XWF+cxms7Zu3aolS5boyJEjatGiha699lo99dRTfJcYAAAAgIumXjVikjR+/HiNHz++0mXr1q2rELv88survN2xp6envvrqq9pMDwAAAADOqV6dmggAAAAAzoBGDAAAAADsjEYMAAAAAOyMRgwAAAAA7IxGDAAAAADsjEYMAAAAAOyMRgwAAAAA7IxGDAAAAADsjEYMAAAAAOyMRgwAAAAA7IxGDAAAAADsjEYMAAAAAOyMRgwAAAAA7IxGDAAAAADsjEYMAAAAAOyMRgwAAAAA7IxGDAAAAADsjEYMAAAAAOyMRgwAAAAA7IxGDAAAAADsjEYMAAAAAOyMRgwAAAAA7IxGDAAAAADsjEYMAAAAAOyMRgwAAAAA7IxGDAAAAADsjEYMAAAAAOzM1dEJAMCZjh8/rm3bttXKXMXFxcrKylJISIg8PT1rZc7w8HB5eXnVylyoucCGJkW6ZauByezoVOqNU27ZUkOTo9Ood/bu3av8/HxHp1FBRkaGzX/rGn9/f7Vu3drRaQB1Ho0YgDpn27Zt6tq1q6PTqFJaWpq6dOni6DQuWXd2ddOTLRY5Oo36pYX0ZFc3R2dRr+zdu1eXh0foRPFxR6dSpREjRjg6hUp5eHpp+7YMmjHgHGjEANQ54eHhSktLq5W5MjIyNGLECL377ruKiIiolTnDw8NrZR6cn3+nlWj9ZQ+oQdNWjk6l3jh18A/9L+15/dPRidQj+fn5OlF8XE37T6pz+5pRWqLSgjy5+jaXybVuNdinDv6hgyvnKT8/n0YMOAcaMQB1jpeXV60fcYqIiOAolpPIPWZIJS3kboQ6OpV642RJ2V+vG2qsQdNWcg8Mc3QaFQV3cHQGAC4QN+sAAAAAADujEQMAAAAAO6MRAwAAAAA7oxEDAAAAADujEQMAAAAAO6MRAwAAAAA7oxEDAAAAADujEQMAAAAAO6MRAwAAAAA7oxEDAAAAADujEQMAAAAAO6MRAwAAAAA7oxEDAAAAADujEQMAAAAAO6MRAwAAAAA7oxEDAAAAADurd43YwoULFRISIg8PD8XExGjDhg1Vjl28eLFMJpPNj4eHh80YwzA0bdo0BQUFydPTU3379tXOnTsvdhkAAAAALmH1qhH78MMPNXHiRE2fPl2bNm1SVFSU4uPj9eeff1a5jo+Pj3Jycqw/v//+u83yOXPm6OWXX9aiRYuUmpoqb29vxcfH68SJExe7HAAAAACXqHrViL3wwgsaN26cxowZow4dOmjRokXy8vLS22+/XeU6JpNJgYGB1p/mzZtblxmGofnz5+vxxx/XgAED1KlTJy1dulTZ2dlKSkqyQ0UAAAAALkWujk6gukpKSpSWlqZHHnnEGnNxcVHfvn2VkpJS5XrHjh1TmzZtZLFY1KVLF82aNUtXXHGFJCkzM1O5ubnq27evdbyvr69iYmKUkpKim2++udI5T548qZMnT1ofFxYWSpJKS0tVWlpqzc3FxUUWi0UWi8UmZxcXF5WVlckwjHPGzWazTCaTdd7T45JUVlZWZfzMdVBz9thOp3N1dZVhGDZxk8kks9lcYV+qKl4X9r26VFN5/qWlpTIMwylqcsbtVN2aTh+HmistLZXFYmHfq0ZN/A69cLzvUdOlWlN13z/qTSOWn5+vsrIymyNaktS8eXNt27at0nUuv/xyvf322+rUqZMKCgo0d+5c9ezZU7/++quCg4OVm5trnePMOcuXVWb27NmaMWNGhfjmzZvl7e0tSQoICFC7du2UmZmpAwcOWMcEBwcrODhYO3bsUEFBgTXetm1bNWvWTOnp6SouLrbGw8PD5efnp82bN9vsgJ06dZKbm5s2btxok0N0dLRKSkq0detWbd++vcoaUD322E7lzGazunXrpoKCApt92tPTU1FRUcrPz9eePXuscV9fX0VERCg7O1v79u2zxuvCvleXasrIyJAkZWRk6LLLLnOKmpxxO1W3pjN/QaNmMjIy1LRpU/a9atRU/t6B88f7HjVdqjUVFRWpOkxGPfl4MTs7Wy1bttT69evVo0cPa3zq1Kn67rvvlJqaes45Tp06pYiICA0fPlxPPfWU1q9fr169eik7O1tBQUHWccOGDZPJZNKHH35Y6TyVHRFr1aqVDh48KB8fH0l149OETZs2KSYmRoGj5ss9MOycrw/+cjJ3l3KXPKi0tDRFRUXxqU89ryktLU0xMTFKTU1Vt27dnKImZ9xO1a1p06ZNio6O5n2thsrf11JTUxUdHc2+V42a+B16fk7/Hdq5c2fe96jpkqypsLBQTZs2VUFBgbU3qEy9OSLm7+8vs9msvLw8m3heXp4CAwOrNUeDBg105ZVXateuXZJkXS8vL8+mEcvLy1Pnzp2rnMfd3V3u7u4V4q6urnJ1tX1Jyzfkmco3WHXjZ85bnXhVy1B99thOZzKZTJXGq9qXahq/1Goqf15XV1eZTKYa515VnO3kmJrKtyHOj6urq3XbsO+dvSZ+h1443veoqaocaxqvbzVV9/2j3tysw83NTV27dtWaNWusMYvFojVr1tgcITubsrIy/e9//7M2XaGhoQoMDLSZs7CwUKmpqdWeEwAAAABqql593DNx4kSNGjVK0dHR6t69u+bPn6+ioiKNGTNGkjRy5Ei1bNlSs2fPliTNnDlTsbGxCgsL05EjR/T888/r999/1+233y7pr+76wQcf1NNPP6327dsrNDRUTzzxhFq0aKGEhARHlQkAAADAydWrRuymm27SgQMHNG3aNOXm5qpz585atWqV9WYbe/futTncePjwYY0bN065ublq3LixunbtqvXr16tDhw7WMVOnTlVRUZHuuOMOHTlyRH/729+0atWqCl/8DAAAAAC1pV41YpI0fvx4jR8/vtJl69ats3n84osv6sUXXzzrfCaTSTNnztTMmTNrK0UAAAAAOKt6c40YAAAAADgLGjEAAAAAsDMaMQAAAACwMxoxAAAAALAzGjEAAAAAsDMaMQAAAACwMxoxAAAAALAzGjEAAAAAsDMaMQAAAACwMxoxAAAAALAzGjEAAAAAsDMaMQAAAACwMxoxAAAAADaKi4s1fvx4xcfHa/z48SouLnZ0Sk6HRgwAAACAVUJCgry8vLRw4UJ9/fXXWrhwoby8vJSQkODo1JwKjRgAAAAASX81YZ999pnc3Nz08MMPa9euXXr44Yfl5uamzz77jGasFrk6OgEAAAAAjldcXGxtwo4ePSo3NzdJ0uzZszVjxgw1atRIn332mYqLi+Xp6engbOs/GjEAtWLftjQdzd7p6DQqyM3M1JWBLsrd/JUyjuxwdDoVNGrRXsHhXR2dRr1z6uAfjk6hAqO0RKUFeXL1bS6Tq5uj07FRF18vAHXPlClTJEkTJ060NmHl3Nzc9OCDD2rOnDmaMmWKXnnlFUek6FRoxABcsL1792rpfb31eK+6d7ZzhKTr72wo/fGsVAf/Fn36R4tGvvWrWrdu7ehU6gV/f395eHrp4Mp5jk6l3vHw9JK/v7+j0wBQh+3c+dcHqrfffnuly8eOHas5c+ZYx+HC0IgBuGD5+fla+FOR1jYeKVe/5o5Ox4ZRdkplRw/J3KiJTOYGjk7HRumRPGX8tFTX5+fTiFVT69attX1bhvLz8x2dSgUZGRkaMWKE3n33XUVERDg6nQr8/f3ZzwCcVfv27fX111/rzTff1OzZsyssf+utt6zjcOFoxADUitxjhuR1pdybhDk6lYoCHJ1A5U6W7FLusSWOTqPead26dZ1uKCIiItSlSxdHpwEANfb8889r4cKFeuGFFzRjxgyb0xNLSko0f/586zhcuLp3HhEAAAAAu/P09NSAAQNUUlKiRo0a6aGHHtKOHTv00EMPqVGjRiopKdGAAQO4UUctoREDAAAAIElKSkqyNmNz5szR5Zdfrjlz5libsKSkJEen6DQ4NREAAACAVVJSkoqLizVlyhTt3LlT7du31/PPP8+RsFpGIwYAAADAhqenJ7eov8g4NREAAAAA7IxGDAAAAADsjEYMAAAAAOyMRgwAAAAA7IxGDAAAAADsjEYMAAAAAOyMRgwAAAAA7IxGDAAAAADsjEYMAAAAAOyMRgwAAAAA7IxGDAAAAADsjEYMAAAAAOyMRgwAAAAA7IxGDAAAAADsjEYMAAAAAOyMRgwAAAAA7IxGDAAAAADsjEYMAAAAAOyMRgwAAAAA7IxGDAAAAADsjEYMAAAAAOzsghuxsrIybdmyRYcPH66NfAAAAADA6dW4EXvwwQf11ltvSfqrCbvqqqvUpUsXtWrVSuvWravt/AAAAADA6dS4EVu+fLmioqIkSStWrFBmZqa2bdumCRMm6LHHHqv1BAEAAADA2dS4EcvPz1dgYKAk6csvv9TQoUN12WWX6bbbbtP//ve/Wk/wTAsXLlRISIg8PDwUExOjDRs2VDn2jTfeUFxcnBo3bqzGjRurb9++FcaPHj1aJpPJ5qdfv34XuwwAAAAAl7AaN2LNmzfXb7/9prKyMq1atUr/+Mc/JEnHjx+X2Wyu9QRP9+GHH2rixImaPn26Nm3apKioKMXHx+vPP/+sdPy6des0fPhwffvtt0pJSVGrVq107bXXav/+/Tbj+vXrp5ycHOvP+++/f1HrAAAAAHBpq3EjNmbMGA0bNkwdO3aUyWRS3759JUmpqakKDw+v9QRP98ILL2jcuHEaM2aMOnTooEWLFsnLy0tvv/12peOXLVume+65R507d1Z4eLjefPNNWSwWrVmzxmacu7u7AgMDrT+NGze+qHUAAAAAuLS51nSFJ598Uh07dtQff/yhoUOHyt3dXZJkNpv18MMP13qC5UpKSpSWlqZHHnnEGnNxcVHfvn2VkpJSrTmOHz+uU6dOqUmTJjbxdevWqVmzZmrcuLGuvvpqPf3002ratGmV85w8eVInT560Pi4sLJQklZaWqrS01Jqbi4uLLBaLLBaLTc4uLi4qKyuTYRjnjJvNZplMJuu8p8elv26YUlX8zHVQc/bYTqdzdXWVYRg2cZPJJLPZXGFfqiruiH2Pfe3CVPbewb5X/2o6PV9nqelscWevife1C8e+R02Xak3Vff+ocSMmSUOGDKkQGzVq1PlMVW35+fkqKytT8+bNbeLNmzfXtm3bqjXHQw89pBYtWliP4kl/nZY4aNAghYaGavfu3Xr00Ud13XXXKSUlpcpTLWfPnq0ZM2ZUiG/evFne3t6SpICAALVr106ZmZk6cOCAdUxwcLCCg4O1Y8cOFRQUWONt27ZVs2bNlJ6eruLiYms8PDxcfn5+2rx5s80O2KlTJ7m5uWnjxo02OURHR6ukpERbt27V9u3bq/W6oGr22E7lzGazunXrpoKCApt92tPTU1FRUcrPz9eePXuscV9fX0VERCg7O1v79u2zxh2x77GvXZiMjAzrLwZ7vkeUq8/7Xl2q6fR/B85Sk+R826m6NWVkZAgXhn2Pmi7VmoqKilQdJuP0Nq8aZs6cedbl06ZNq8l01Zadna2WLVtq/fr16tGjhzU+depUfffdd0pNTT3r+s8++6zmzJmjdevWqVOnTlWO27Nnj9q1a6dvvvlG11xzTaVjKjsi1qpVKx08eFA+Pj6S6sanCZs2bVJMTIwCR82Xe2DYWV8f/J+TubuUu+RBpaWlKSoqik99qlET+9r5Kd/XUlNT1aVLF0l84lifayr/d1DZe0d9relscWevife183P679DOnTuz71HTJVlTYWGhmjZtqoKCAmtvUJkaHxH79NNPbR6fOnVKmZmZcnV1Vbt27S5aI+bv7y+z2ay8vDybeF5envUujlWZO3eunn32WX3zzTdnbcKkvzpgf39/7dq1q8pGzN3d3XpK5ulcXV3l6mr7kpZvyDNVdbStqviZ81YnXtUyVJ89ttOZTCZTpfGq9qWaxi9GTexrF6ayf6/se/WvptOfx1lqqk7cWWvife3Cse9RU1U51jRe32qq7vtHjd9lNm/eXCFWWFio0aNHa+DAgTWdrtrc3NzUtWtXrVmzRgkJCZJkvfHG+PHjq1xvzpw5euaZZ/TVV18pOjr6nM+zb98+HTx4UEFBQbWVOgAAAADYqPFdEyvj4+OjGTNm6IknnqiN6ao0ceJEvfHGG1qyZIkyMjJ09913q6ioSGPGjJEkjRw50uZmHs8995yeeOIJvf322woJCVFubq5yc3N17NgxSdKxY8c0ZcoU/fTTT8rKytKaNWs0YMAAhYWFKT4+/qLWAgAAAODSVWvH3QsKCmwucrsYbrrpJh04cEDTpk1Tbm6uOnfurFWrVllv4LF3716bw42vvfaaSkpKKtxcZPr06XryySdlNpu1detWLVmyREeOHFGLFi107bXX6qmnnqr01EMAAAAAqA01bsRefvllm8eGYSgnJ0f/+c9/dN1119VaYlUZP358lacirlu3zuZxVlbWWefy9PTUV199VUuZAQAAAM6hrKxMycnJysnJUVBQkOLi4qq8Zgrnp8aN2Isvvmjz2MXFRQEBARo1apTNaYEAAAAA6p/ExERNmjTJ5qBGSEiI5s2bp0GDBjkuMSdT40YsMzPzYuQBAAAAwMESExM1ZMgQ9e/fX++//746duyo9PR0zZo1S0OGDNHy5ctpxmpJrdysAwAAAED9VlZWpkmTJql///5KSkpSbGysGjZsqNjYWCUlJal///6aPHlyhe/6wvk5r5t1bNy4UR999JH27t2rkpISm2WJiYm1khhqz6mDfzg6hQqM0hKVFuTJ1be5TK5ujk7HRl18veqLuvjasa8BOF+BDU2KdMtWAxPXxVTXKbdsqaHJ0WngPCUnJysrK0vvv/9+he/bcnFx0SOPPKKePXsqOTlZvXv3dkySTqTGjdgHH3ygkSNHKj4+Xl9//bWuvfZa7dixQ3l5eRf1e8RQc/7+/vLw9NLBlfMcnUq94+HpJX9/f0enUW+wr50/9jWg7rqzq5uebLHI0WnULy2kJ7vWrQ+9UH05OTmSpI4dO1a6vDxePg4XpsaN2KxZs/Tiiy/q3nvvVaNGjfTSSy8pNDRUd955J1+CXMe0bt1a27dlKD8/39GpVJCRkaERI0bo3XffVUREhKPTqcDf31+tW7d2dBr1Bvva+WNfA+quf6eVaP1lD6hB01aOTqXeOHXwD/0v7Xn909GJ4LyU/y2fnp6u2NjYCsvT09NtxuHC1LgR2717t2644QZJkpubm4qKimQymTRhwgRdffXVmjFjRq0nifPXunXrOv1HXkREhLp06eLoNFAL2NcAOJvcY4ZU0kLuRqijU6k3TpaU/fW6oV6Ki4tTSEiIZs2apaSkJJvTEy0Wi2bPnq3Q0FDFxcU5MEvnUeObdTRu3FhHjx6VJLVs2dLaGR85ckTHjx+v3ewAAAAA2IXZbNa8efO0cuVKJSQkKCUlRUePHlVKSooSEhK0cuVKzZ07l+8TqyXVbsRuu+02HT16VH//+9+1evVqSdLQoUP1wAMPaNy4cRo+fLiuueaai5YoAAAAgItr0KBBWr58uf73v/+pZ8+e8vHxUc+ePZWens6t62tZtU9NXLJkiZ599lm98sorOnHihCTpscceU4MGDbR+/XoNHjxYjz/++EVLFAAAAMDFN2jQIA0YMEDJycnKyclRUFCQ4uLiOBJWy6rdiBnGX+f7NmnSxBpzcXHRww8/XPtZAQAAAHAYs9nMLeovshrdrOPo0aPy8PA46xgfH58LSggAAAAAnF2NGrHLLrusymWGYchkMvFN2wAAAABwDjVqxJYvX25zaiIAAAAAoOZq1Ij16tVLzZo1u1i5AAAAAMAlocbfIwYAAAAAuDDVbsTatGnDLSsBAAAAoBZU+9TEzMzMi5kHAAAAAFwyODURAAAAAOyMRgwAAAAA7IxGDAAAAADsjEYMAAAAAOysWjfrePnll6s94f3333/eyQAAAADApaBajdiLL75YrclMJhONGAAAAACcQ7UaMW5dDwAAAAC1h2vEAAAAAMDOqnVEbOLEidWe8IUXXjjvZAAAAADgUlCtRmzz5s3VmsxkMl1QMgAAAABwKahWI/btt99e7DwAAAAA4JLBNWIAAAAAYGfVOiJ2po0bN+qjjz7S3r17VVJSYrMsMTGxVhIDAAAAAGdV4yNiH3zwgXr27KmMjAx9+umnOnXqlH799VetXbtWvr6+FyNHAAAAAHAqNW7EZs2apRdffFErVqyQm5ubXnrpJW3btk3Dhg1T69atL0aOAAAAAOBUatyI7d69WzfccIMkyc3NTUVFRTKZTJowYYJef/31Wk8QAAAAAJxNjRuxxo0b6+jRo5Kkli1bKj09XZJ05MgRHT9+vHazAwAAAAAnVOObdfz973/X6tWrFRkZqaFDh+qBBx7Q2rVrtXr1al1zzTUXI0cAAAAAcCo1bsReeeUVnThxQpL02GOPqUGDBlq/fr0GDx6sxx9/vNYTBAAAAABnU+NGrEmTJtb/d3Fx0cMPP1yrCQEAAACAs6tWI1ZYWFjtCX18fM47GQAAAAC4FFSrEfPz85PJZKrWhGVlZReUEAAAAAA4u2o1Yt9++631/7OysvTwww9r9OjR6tGjhyQpJSVFS5Ys0ezZsy9OlgAAAADgRKrViF111VXW/585c6ZeeOEFDR8+3Br75z//qcjISL3++usaNWpU7WcJAAAAAE6kxt8jlpKSoujo6Arx6OhobdiwoVaSAgAAAABnVuNGrFWrVnrjjTcqxN988021atWqVpICAAAAAGdW49vXv/jiixo8eLD++9//KiYmRpK0YcMG7dy5U5988kmtJwgAAAAAzqbGR8Suv/567dy5UzfeeKMOHTqkQ4cO6cYbb9SOHTt0/fXXX4wcAQAAAMCp1PiImCQFBwdr1qxZtZ0LAAAAgDqgrKxMycnJysnJUVBQkOLi4mQ2mx2dllOp8RExSTpy5Ii+/vprvfvuu1q6dKnNz8W2cOFChYSEyMPDQzExMee8QcjHH3+s8PBweXh4KDIyUl9++aXNcsMwNG3aNAUFBcnT01N9+/bVzp07L2YJAAAAQJ2VmJiosLAw9enTR7fccov69OmjsLAwJSYmOjo1p1LjRmzFihVq3bq1+vXrp/Hjx+uBBx6w/jz44IMXIcX/8+GHH2rixImaPn26Nm3apKioKMXHx+vPP/+sdPz69es1fPhwjR07Vps3b1ZCQoISEhKUnp5uHTNnzhy9/PLLWrRokVJTU+Xt7a34+HidOHHiotYCAAAA1DWJiYkaMmSIIiMjlZKSoqNHjyolJUWRkZEaMmQIzVgtqnEjNmnSJN122206duyYjhw5osOHD1t/Dh06dDFytHrhhRc0btw4jRkzRh06dNCiRYvk5eWlt99+u9LxL730kvr166cpU6YoIiJCTz31lLp06aJXXnlF0l9Hw+bPn6/HH39cAwYMUKdOnbR06VJlZ2crKSnpotYCAAAA1CVlZWWaNGmS+vfvr6SkJMXGxqphw4aKjY1VUlKS+vfvr8mTJ6usrMzRqTqFGl8jtn//ft1///3y8vK6GPlUqaSkRGlpaXrkkUesMRcXF/Xt21cpKSmVrpOSkqKJEyfaxOLj461NVmZmpnJzc9W3b1/rcl9fX8XExCglJUU333xzpfOePHlSJ0+etD4uLCyUJJWWlqq0tNSam4uLiywWiywWi03OLi4uKisrk2EY54ybzWaZTCbrvKfHJVX4h1BV3NXVVYZh2MRNJpPMZnOFHKuK12ZN5U5/zep7Tc64nRxV07Fjx/Tbb7/VSk0ZGRmSpPT0dBmGUSs1dezYUZ6enpf8dqIm29xPz9dZajpb3NlrKq/r1ME/VNcYpSUqLciTq29zmVzdHJ2OjdNfL/a9+lfTd999p6ysLL3//vuSbN/XXFxc9Mgjj6hnz55at26drrrqqnpR0+k52ms7nbm8KjVuxOLj47Vx40a1bdu2pqtekPz8fJWVlal58+Y28ebNm2vbtm2VrpObm1vp+NzcXOvy8lhVYyoze/ZszZgxo0J88+bN8vb2liQFBASoXbt2yszM1IEDB6xjgoODFRwcrB07dqigoMAab9u2rZo1a6b09HQVFxdb4+Hh4fLz89PmzZttdsBOnTrJzc1NGzdutMkhOjpaJSUl2rp1qzVmNpvVrVs3FRQU2LxWnp6eioqKUn5+vvbs2WON+/r6KiIiQtnZ2dq3b581Xps1lcvIyLD+g6jvNTnjdnJUTcnJybV+F9ZRo0bV2lxpaWlq27btJb+dqMm2pu3bt1uXO0tNkvNtp+rWlJ2dLTd3Dx1cOU+oGXcPD/n7+7Pv1cOafvzxR0l/feBYWU0dO3a0jiv/m7eu1yTZfzsVFRWpOkzG6W1eNbz11luaOXOmxowZo8jISDVo0MBm+T//+c+aTFdt2dnZatmypdavX68ePXpY41OnTtV3332n1NTUCuu4ublpyZIlGj58uDX26quvasaMGcrLy9P69evVq1cvZWdnKygoyDpm2LBhMplM+vDDDyvNpbIjYq1atdLBgwfl4+Mjyfk+IantmjZv3qyuXbsqNTVVXbp0cYqanHE7OcMRseLiYmVlZSkkJETe3t4cEWPfu2g1bdq0STExMUpLS1NUVJRT1HS2+KVQ0969e5Wfn1/natq+fbtGjBihJUuWWD/crEvbKSAgQKGhoex79bCm7777znq2Wffu3SvUlJqaqp49e+qbb77hiNhZci8sLFTTpk1VUFBg7Q0qU+MjYuPGjZMkzZw5s8Iyk8lU4cWrLf7+/jKbzcrLy7OJ5+XlKTAwsNJ1AgMDzzq+/L95eXk2jVheXp46d+5cZS7u7u5yd3evEHd1dZWrq+1LWr4hz1S+waobP3Pe84mbTKZK41XlWNN4TWsqz/PMnOpzTc64nRxRU8OGDdW9e/dK5z8ff//732ttrtNd6tuJmmxzP/15nKWm6sSduaa2bdva/Qyg6iivsWPHjtYPM+si9r36V1Pv3r0VEhKiWbNmKSkpyWYei8Wi2bNnKzQ0VL17965QQ12tqTrx2t5OVS2vkE+1Rp2mvJOs7OdiNWHSX0e3unbtqjVr1tjksmbNGpsjZKfr0aOHzXhJWr16tXV8aGioAgMDbcYUFhYqNTW1yjkBAAAAZ2Q2mzVv3jytXLlSCQkJNndNTEhI0MqVKzV37tyzfsiO6juvL3Qud+LECXl4eNRWLuc0ceJEjRo1StHR0erevbvmz5+voqIijRkzRpI0cuRItWzZUrNnz5YkPfDAA7rqqqs0b9483XDDDfrggw+0ceNGvf7665L+6q4ffPBBPf3002rfvr1CQ0P1xBNPqEWLFkpISLBbXQAAAEBdMGjQIC1fvlyTJk1Sz549rfHQ0FAtX75cgwYNcmB2zqXGjVhZWZlmzZqlRYsWKS8vTzt27FDbtm31xBNPKCQkRGPHjr0YeUqSbrrpJh04cEDTpk1Tbm6uOnfurFWrVllvtrF3716bw409e/bUe++9p8cff1yPPvqo2rdvr6SkJOuFhtJf15gVFRXpjjvu0JEjR/S3v/1Nq1atsmuDCQAAANQVgwYN0oABA5ScnKycnBwFBQUpLi6OI2G1rMaN2DPPPKMlS5Zozpw51uvFpL/OU54/f/5FbcQkafz48Ro/fnyly9atW1chNnToUA0dOrTK+Uwmk2bOnFnpNW8AAADApchsNqt3796OTsOp1fgasaVLl+r111/XrbfeatMVR0VFVXkbeQAAAADA/6lxI7Z//36FhYVViFssFp06dapWkgIAAAAAZ1bjRqxDhw5KTk6uEF++fPlZb/kOAAAAAPhLja8RmzZtmkaNGqX9+/fLYrEoMTFR27dv19KlS7Vy5cqLkSMAAAAAOJUaHxEbMGCAVqxYoW+++Ube3t6aNm2aMjIytGLFCr57CwAAAACqodqN2Isvvmj9/7i4OK1evVp//vmnjh8/rh9++EE9evRQfHz8RUkSAAAAAJxJtRuxRx99VEuXLq10WVFRkfr166eDBw/WWmIAAAAA4Kyq3Yj95z//0Z133qnPP//cJn7s2DHFx8frwIED+vbbb2s9QQAAAABwNtW+WceQIUN05MgRDR8+XF988YV69+6toqIiXXfddcrLy9N3332noKCgi5krAAAAADiFGt018fbbb9ehQ4c0YMAAffbZZ5o2bZqys7P13XffqUWLFhcrRwAAAABwKjW+ff3UqVN16NAhXXPNNQoJCdG6desUHBx8MXIDAAAAAKdU7UZs0KBBNo8bNGggf39/PfDAAzbxxMTE2skMAAAAAJxUtRsxX19fm8fDhw+v9WQAAAAA4FJQ7UbsnXfeuZh5AAAAAMAlo9q3rwcAAAAA1A4aMQAAAACwMxoxAAAAALAzGjEAAAAAsDMaMQAAAACwMxoxAAAAALAzGjEAAAAAsDMaMQAAAACwMxoxAAAAALAzGjEAAAAAsDMaMQAAAACwMxoxAAAAALAzGjEAAAAAsDMaMQAAAACwMxoxAAAAALAzGjEAAAAAsDMaMQAAAACwMxoxAAAAALAzGjEAAAAAsDMaMQAAAACwMxoxAAAAALAzV0cnAAAXS1lZmZKTk5WTk6OgoCDFxcXJbDY7Oi0AAACOiAFwTomJiQoLC1OfPn10yy23qE+fPgoLC1NiYqKjUwMAAKARA+B8EhMTNWTIEEVGRiolJUVHjx5VSkqKIiMjNWTIEJoxAADgcDRiAJxKWVmZJk2apP79+yspKUmxsbFq2LChYmNjlZSUpP79+2vy5MkqKytzdKoAAOASxjViAJxKcnKysrKy9P7778vFxfazJhcXFz3yyCPq2bOnkpOT1bt3b8ckiTrj+PHj2rZtW63MlZGRYfPf2hAeHi4vL69amw8AUHfQiAFwKjk5OZKkjh07Vrq8PF4+Dpe2bdu2qWvXrrU654gRI2ptrrS0NHXp0qXW5gMA1B00YgCcSlBQkCQpPT1dsbGxFZanp6fbjMOlLTw8XGlpabUyV3FxsbKyshQSEiJPT89amTM8PLxW5gEA1D00YgCcSlxcnEJCQjRr1iwlJSXZnJ5osVg0e/ZshYaGKi4uzoFZoq7w8vKq1SNOvXr1qrW5AADOjZt1AHAqZrNZ8+bN08qVK5WQkGBz18SEhAStXLlSc+fO5fvEAACAQ3FEDIDTGTRokJYvX65JkyapZ8+e1nhoaKiWL1+uQYMGOTA7AAAAGjEATmrQoEEaMGCAkpOTlZOTo6CgIMXFxXEkDAAA1Ak0YgCcltls5hb1AACgTqo314gdOnRIt956q3x8fOTn56exY8fq2LFjZx1/33336fLLL5enp6dat26t+++/XwUFBTbjTCZThZ8PPvjgYpcDAAAA4BJWb46I3XrrrcrJydHq1at16tQpjRkzRnfccYfee++9SsdnZ2crOztbc+fOVYcOHfT777/rrrvuUnZ2tpYvX24z9p133lG/fv2sj/38/C5mKQAAAAAucfWiEcvIyNCqVav0888/Kzo6WpK0YMECXX/99Zo7d65atGhRYZ2OHTvqk08+sT5u166dnnnmGY0YMUKlpaVydf2/0v38/BQYGHjxCwEAAAAA1ZNGLCUlRX5+ftYmTJL69u0rFxcXpaamauDAgdWap6CgQD4+PjZNmCTde++9uv3229W2bVvdddddGjNmjEwmU5XznDx5UidPnrQ+LiwslCSVlpaqtLRUkuTi4iIXFxdZLBZZLBbr2PJ4WVmZDMM4Z9xsNstkMlnnPT0uSWVlZdWKu7q6yjAMm7jJZJLZbK6QY1Xx2qyp3OmvWX2vyRm3EzVREzVREzXVvZrKVfZ3R32tyRm3EzVdujWdubwq9aIRy83NVbNmzWxirq6uatKkiXJzc6s1R35+vp566indcccdNvGZM2fq6quvlpeXl77++mvdc889OnbsmO6///4q55o9e7ZmzJhRIb5582Z5e3tLkgICAtSuXTtlZmbqwIED1jHBwcEKDg7Wjh07bK5Xa9u2rZo1a6b09HQVFxdb4+Hh4fLz89PmzZttdsBOnTrJzc1NGzdutMkhOjpaJSUl2rp1qzVmNpvVrVs3FRQUaNu2bda4p6enoqKilJ+frz179ljjvr6+ioiIUHZ2tvbt22eN12ZN5TIyMqz/IOp7Tc64naiJmqiJmqip7tVU7vTfofW9JmfcTtR06dZUVFSk6jAZp7d5dvbwww/rueeeO+uYjIwMJSYmasmSJdq+fbvNsmbNmmnGjBm6++67zzpHYWGh/vGPf6hJkyb6/PPP1aBBgyrHTps2Te+8847++OOPKsdUdkSsVatWOnjwoHx8fCTxacK5atq8ebO6du2q1NRUdenSxSlqcsbtRE3URE3URE11r6YtW7ZU+B1a32tyxu1ETZduTYWFhWratKn1bLyqOPSI2KRJkzR69Oizjmnbtq0CAwP1559/2sRLS0t16NChc17bdfToUfXr10+NGjXSp59+etYmTJJiYmL01FNP6eTJk3J3d690jLu7e6XLXF1dK5z2WL4hz1S+waobP3Pe84mbTKZK41XlWNN4TWsqz/PMnOpzTc64naiJmqiJms4WpybH1FT+vGc+R32uyRm3EzVdmjVVtbzC+GqNukgCAgIUEBBwznE9evTQkSNHlJaWpq5du0qS1q5dK4vFopiYmCrXKywsVHx8vNzd3fX555/Lw8PjnM+1ZcsWNW7cuMomDAAAAAAuVL24RiwiIkL9+vXTuHHjtGjRIp06dUrjx4/XzTffbL1j4v79+3XNNddo6dKl6t69uwoLC3Xttdfq+PHjevfdd1VYWGi9qUZAQIDMZrNWrFihvLw8xcbGysPDQ6tXr9asWbM0efJkR5YLAAAAwMnVi0ZMkpYtW6bx48frmmuukYuLiwYPHqyXX37ZuvzUqVPavn27jh8/LknatGmTUlNTJUlhYWE2c2VmZiokJEQNGjTQwoULNWHCBBmGobCwML3wwgsaN26c/QoDAAAAcMmpN41YkyZNqvzyZkkKCQmxuZiud+/eNo8r069fP5svcgYAAAAAe6j86k8AAAAAwEVDIwYAAAAAdkYjBgAAAAB2RiMGAAAAAHZGIwYAAAAAdkYjBgAAAAB2RiMGAAAAAHZGIwYAAAAAdkYjBgAAAAB2RiMGAAAAAHZGIwYAAAAAdkYjBgAAAAB2RiMGAAAAAHZGIwYAAAAAdkYjBgAAAAB2RiMGAAAAAHZGIwYAAAAAdkYjBgAAAAB2RiMGAAAAAHZGIwYAAAAAdkYjBgAAAAB2RiMGAAAAAHZGIwYAAAAAdubq6AQA4GIpKytTcnKycnJyFBQUpLi4OJnNZkenBQAAwBExAM4pMTFRYWFh6tOnj2655Rb16dNHYWFhSkxMdHRqAAAANGIAnE9iYqKGDBmiyMhIpaSk6OjRo0pJSVFkZKSGDBlCMwYAAByORgyAUykrK9OkSZPUv39/JSUlKTY2Vg0bNlRsbKySkpLUv39/TZ48WWVlZY5OFQAAXMJoxAA4leTkZGVlZenRRx+Vi4vtW5yLi4seeeQRZWZmKjk52UEZAgAA0IgBcDI5OTmSpI4dO1a6vDxePg4AAMARaMQAOJWgoCBJUnp6eqXLy+Pl4wAAAByBRgyAU4mLi1NISIhmzZoli8Vis8xisWj27NkKDQ1VXFycgzIEAACgEQPgZMxms+bNm6eVK1cqISHB5q6JCQkJWrlypebOncv3iQEAAIfiC50BOJ1BgwZp+fLlmjRpknr27GmNh4aGavny5Ro0aJADswMAAKARA+CkBg0apAEDBig5OVk5OTkKCgpSXFwcR8IAAECdQCMGwGmZzWb17t3b0WkAAABUwDViAAAAAGBnNGIAAAAAYGc0YgAAAABgZzRiAAAAAGBnNGIAAAAAYGc0YgAAAABgZzRiAAAAAGBnNGIAAAAAYGc0YgAAAABgZzRiAAAAAGBnNGIAAAAAYGc0YgAAAABgZzRiAAAAAGBn9aYRO3TokG699Vb5+PjIz89PY8eO1bFjx866Tu/evWUymWx+7rrrLpsxe/fu1Q033CAvLy81a9ZMU6ZMUWlp6cUsBQAAAMAlztXRCVTXrbfeqpycHK1evVqnTp3SmDFjdMcdd+i9994763rjxo3TzJkzrY+9vLys/19WVqYbbrhBgYGBWr9+vXJycjRy5Eg1aNBAs2bNumi1AAAAALi01YtGLCMjQ6tWrdLPP/+s6OhoSdKCBQt0/fXXa+7cuWrRokWV63p5eSkwMLDSZV9//bV+++03ffPNN2revLk6d+6sp556Sg899JCefPJJubm5XZR6AAAAAFza6kUjlpKSIj8/P2sTJkl9+/aVi4uLUlNTNXDgwCrXXbZsmd59910FBgbqxhtv1BNPPGE9KpaSkqLIyEg1b97cOj4+Pl533323fv31V1155ZWVznny5EmdPHnS+riwsFCSVFpaaj2t0cXFRS4uLrJYLLJYLNax5fGysjIZhnHOuNlslslkqnC6pNlslvTXUb3qxF1dXWUYhk3cZDLJbDZXyLGqeG3WVO7016y+1+SM24maqImaqIma6l5N5Sr7u6O+1uSM24maLt2aqnuZU71oxHJzc9WsWTObmKurq5o0aaLc3Nwq17vlllvUpk0btWjRQlu3btVDDz2k7du3KzEx0Trv6U2YJOvjs807e/ZszZgxo0J88+bN8vb2liQFBASoXbt2yszM1IEDB6xjgoODFRwcrB07dqigoMAab9u2rZo1a6b09HQVFxdb4+Hh4fLz89PmzZttdsBOnTrJzc1NGzdutMkhOjpaJSUl2rp1qzVmNpvVrVs3FRQUaNu2bda4p6enoqKilJ+frz179ljjvr6+ioiIUHZ2tvbt22eN12ZN5TIyMqz/IOp7Tc64naiJmqiJmqip7tVU7vTfofW9JmfcTtR06dZUVFSk6jAZp7d5dvbwww/rueeeO+uYjIwMJSYmasmSJdq+fbvNsmbNmmnGjBm6++67q/V8a9eu1TXXXKNdu3apXbt2uuOOO/T777/rq6++so45fvy4vL299eWXX+q6666rdJ7Kjoi1atVKBw8elI+PjyQ+TThXTZs3b1bXrl2VmpqqLl26OEVNzridqImaqImaqKnu1bRly5YKv0Pre03OuJ2o6dKtqbCwUE2bNlVBQYG1N6iMQ4+ITZo0SaNHjz7rmLZt2yowMFB//vmnTby0tFSHDh2q8vqvysTExEiStRELDAzUhg0bbMbk5eVJ0lnndXd3l7u7e4W4q6urXF1tX9LyDXmm8g1W3fiZ855P3GQyVRqvKseaxmtaU3meZ+ZUn2tyxu1ETdRETdR0tjg1Oaam8uc98znqc03OuJ2o6dKsqarlFcZXa9RFEhAQoICAgHOO69Gjh44cOaK0tDR17dpV0l9HtywWi7W5qo4tW7ZIkoKCgqzzPvPMM/rzzz+tpz6uXr1aPj4+6tChQw2rAQAAAIDqqRffIxYREaF+/fpp3Lhx2rBhg3788UeNHz9eN998s/WOifv371d4eLj1CNfu3bv11FNPKS0tTVlZWfr88881cuRI/f3vf1enTp0kSddee606dOigf/3rX/rll1/01Vdf6fHHH9e9995b6REvAAAAAKgN9aIRk/66+2F4eLiuueYaXX/99frb3/6m119/3br81KlT2r59u44fPy5JcnNz0zfffKNrr71W4eHhmjRpkgYPHqwVK1ZY1zGbzVq5cqXMZrN69OihESNGaOTIkZp52veOAQAAAEBtqxd3TZSkJk2anPXLm0NCQmwupmvVqpW+++67c87bpk0bffnll7WSIwAAAABUR705IgYAAAAAzoJGDAAAAADsjEYMAAAAAOyMRgwAAAAA7IxGDAAAAADsjEYMAAAAAOyMRgwAAAAA7IxGDAAAAADsjEYMAAAAAOyMRgwAAAAA7IxGDAAAAADszNXRCQDAxVJWVqbk5GTl5OQoKChIcXFxMpvNjk4LToh9DQBQUxwRA+CUEhMTFRYWpj59+uiWW25Rnz59FBYWpsTEREenBifDvgYAOB80YgCcTmJiooYMGaLIyEilpKTo6NGjSklJUWRkpIYMGcIfyKg17GsAgPNlMgzDcHQS9V1hYaF8fX1VUFAgHx8fR6dTL2zatEldu3ZVWlqaunTp4uh04ETKysoUFhamyMhIJSUlycXl/z5vslgsSkhIUHp6unbu3MmpY7gg7GtwFH6HAnVbdXsDrhFDtR0/flzbtm2rlbkyMjJs/lsbwsPD5eXlVWvzoX5KTk5WVlaW3n//fZs/jCXJxcVFjzzyiHr27Knk5GT17t3bMUnCKbCvoSb4HQrgTDRiqLZt27apa9eutTrniBEjam0uPhmEJOXk5EiSOnbsWOny8nj5OOB8sa+hJvgdCuBMNGKotvDwcKWlpdXKXMXFxcrKylJISIg8PT1rZc7w8PBamQf1W1BQkCQpPT1dsbGxFZanp6fbjAPOF/saaoLfoQDOxDVitYBrxIC6g+t2YC/sawCAylS3N+CuiQCcitls1rx587Ry5UolJCTY3MkuISFBK1eu1Ny5c/nDGBeMfQ0AcCE4IlYLOCIG1D2JiYmaNGmSsrKyrLHQ0FDNnTtXgwYNclxicDrsawCA01W3N6ARqwU0YkDdVFZWpuTkZOXk5CgoKEhxcXEcncBFwb4GAChHI2ZHNGIAAAAAJK4RAwAAAIA6i0YMAAAAAOyMRgwAAAAA7IxGDAAAAADsjEYMAAAAAOyMRgwAAAAA7IxGDAAAAADsjEYMAAAAAOyMRgwAAAAA7IxGDAAAAADsjEYMAAAAAOyMRgwAAAAA7IxGDAAAAADsjEYMAAAAAOyMRgwAAAAA7IxGDAAAAADsjEYMAAAAAOyMRgwAAAAA7IxGDAAAAADsjEYMAAAAAOyMRgwAAAAA7IxGDAAAAADsjEYMAAAAAOyMRgwAAAAA7KzeNGKHDh3SrbfeKh8fH/n5+Wns2LE6duxYleOzsrJkMpkq/fn444+t4ypb/sEHH9ijJAAAAACXKFdHJ1Bdt956q3JycrR69WqdOnVKY8aM0R133KH33nuv0vGtWrVSTk6OTez111/X888/r+uuu84m/s4776hfv37Wx35+frWePwAAAACUqxeNWEZGhlatWqWff/5Z0dHRkqQFCxbo+uuv19y5c9WiRYsK65jNZgUGBtrEPv30Uw0bNkwNGza0ifv5+VUYCwAAAAAXS71oxFJSUuTn52dtwiSpb9++cnFxUWpqqgYOHHjOOdLS0rRlyxYtXLiwwrJ7771Xt99+u9q2bau77rpLY8aMkclkqnKukydP6uTJk9bHhYWFkqTS0lKVlpZKklxcXOTi4iKLxSKLxWIdWx4vKyuTYRjnjJvNZplMJuu8p8clqaysrFpxV1dXGYZhEzeZTDKbzRVyrCpOTdRETdRETdRETdRETdRETWev6czlVakXjVhubq6aNWtmE3N1dVWTJk2Um5tbrTneeustRUREqGfPnjbxmTNn6uqrr5aXl5e+/vpr3XPPPTp27Jjuv//+KueaPXu2ZsyYUSG+efNmeXt7S5ICAgLUrl07ZWZm6sCBA9YxwcHBCg4O1o4dO1RQUGCNt23bVs2aNVN6erqKi4ut8fDwcPn5+Wnz5s02O2CnTp3k5uamjRs32uQQHR2tkpISbd261Rozm83q1q2bCgoKtG3bNmvc09NTUVFRys/P1549e6xxX19fRUREKDs7W/v27bPGqYmaqImaqImaqImaqImaqOnsNRUVFak6TMbpbZ6dPfzww3ruuefOOiYjI0OJiYlasmSJtm/fbrOsWbNmmjFjhu6+++6zzlFcXKygoCA98cQTmjRp0lnHTps2Te+8847++OOPKsdUdkSsVatWOnjwoHx8fCTxaQI1URM1URM1URM1URM1UdOlWFNhYaGaNm2qgoICa29QGYc2YgcOHNDBgwfPOqZt27Z69913NWnSJB0+fNgaLy0tlYeHhz7++ONznpr4n//8R2PHjtX+/fsVEBBw1rFffPGF+vfvrxMnTsjd3b1adRQWFsrX1/ecLzYAAAAA51bd3sChpyYGBAScszGSpB49eujIkSNKS0tT165dJUlr166VxWJRTEzMOdd/66239M9//rNaz7VlyxY1bty42k0YAAAAANRUvbhGLCIiQv369dO4ceO0aNEinTp1SuPHj9fNN99svWPi/v37dc0112jp0qXq3r27dd1du3bp+++/15dffllh3hUrVigvL0+xsbHy8PDQ6tWrNWvWLE2ePNlutQEAAAC49NSLRkySli1bpvHjx+uaa66Ri4uLBg8erJdfftm6/NSpU9q+fbuOHz9us97bb7+t4OBgXXvttRXmbNCggRYuXKgJEybIMAyFhYXphRde0Lhx4y56PQAAAAAuXQ69RsxZcI0YAAAAAKmeXCMGABdTWVmZkpOTlZOTo6CgIMXFxVnvbAQAAOBILo5OAAAuhsTERIWFhalPnz665ZZb1KdPH4WFhSkxMdHRqQEAANCIAXA+iYmJGjJkiCIjI5WSkqKjR48qJSVFkZGRGjJkCM0YAABwOK4RqwVcIwbUHWVlZQoLC1NkZKSSkpLk4vJ/nzdZLBYlJCQoPT1dO3fu5DRFAABQ66rbG3BEDIBTSU5OVlZWlh599FGbJkySXFxc9MgjjygzM1PJyckOyhAAAIBGDICTycnJkSR17Nix0uXl8fJxAAAAjkAjBsCpBAUFSZLS09MrXV4eLx8HAADgCDRiAJxKXFycQkJCNGvWLFksFptlFotFs2fPVmhoqOLi4hyUIQAAAI0YACdjNps1b948rVy5UgkJCTZ3TUxISNDKlSs1d+5cbtQBAAAcii90BuB0Bg0apOXLl2vSpEnq2bOnNR4aGqrly5dr0KBBDswOAACA29fXCm5fD9RNZWVlSk5OVk5OjoKCghQXF8eRMAAAcFFVtzfgiBgAp2U2m9W7d29HpwEAAFAB14gBAAAAgJ3RiAEAAACAndGIAQAAAICd0YgBAAAAgJ3RiAEAAACAndGIAQAAAICd0YgBAAAAgJ3RiAEAAACAndGIAQAAAICd0YgBAAAAgJ3RiAEAAACAndGIAQAAAICd0YgBAAAAgJ25OjoBZ2AYhiSpsLDQwZkAAAAAcKTynqC8R6gKjVgtOHr0qCSpVatWDs4EAAAAQF1w9OhR+fr6VrncZJyrVcM5WSwWZWdnq1GjRjKZTI5Op14oLCxUq1at9Mcff8jHx8fR6cCJsa/BXtjXYC/sa7AX9rXzYxiGjh49qhYtWsjFpeorwTgiVgtcXFwUHBzs6DTqJR8fH/5hwy7Y12Av7GuwF/Y12Av7Ws2d7UhYOW7WAQAAAAB2RiMGAAAAAHZGIwaHcHd31/Tp0+Xu7u7oVODk2NdgL+xrsBf2NdgL+9rFxc06AAAAAMDOOCIGAAAAAHZGIwYAAAAAdkYjBgAAAAB2RiMGAAAAAHZGIwaHWLhwoUJCQuTh4aGYmBht2LDB0Smhnvv+++914403qkWLFjKZTEpKSrJZbhiGpk2bpqCgIHl6eqpv377auXOnY5JFvfXkk0/KZDLZ/ISHh1uXnzhxQvfee6+aNm2qhg0bavDgwcrLy3NgxqgvauM97NChQ7r11lvl4+MjPz8/jR07VseOHbNjFagPauN9bO/evbrhhhvk5eWlZs2aacqUKSotLbV3KfUejRjs7sMPP9TEiRM1ffp0bdq0SVFRUYqPj9eff/7p6NRQjxUVFSkqKkoLFy6sdPmcOXP08ssva9GiRUpNTZW3t7fi4+N14sQJO2eK+u6KK65QTk6O9eeHH36wLpswYYJWrFihjz/+WN99952ys7M1aNAgB2aL+qI23sNuvfVW/frrr1q9erVWrlyp77//XnfccYe9SkA9ciHvY2VlZbrhhhtUUlKi9evXa8mSJVq8eLGmTZvmiFLqNwOws+7duxv33nuv9XFZWZnRokULY/bs2Q7MCs5EkvHpp59aH1ssFiMwMNB4/vnnrbEjR44Y7u7uxvvvv++ADFFfTZ8+3YiKiqp02ZEjR4wGDRoYH3/8sTWWkZFhSDJSUlLslCGcwfm8h/3222+GJOPnn3+2jvnvf/9rmEwmY//+/XbLHXXfhb6Pffnll4aLi4uRm5trHfPaa68ZPj4+xsmTJy9q7s6GI2Kwq5KSEqWlpalv377WmIuLi/r27auUlBQHZgZnlpmZqdzcXJv9ztfXVzExMex3qLGdO3eqRYsWatu2rW699Vbt3btXkpSWlqZTp07Z7Gfh4eFq3bo1+xkuSHXew1JSUuTn56fo6GjrmL59+8rFxUWpqal2zxl124W8j6WkpCgyMlLNmze3jomPj1dhYaF+/fVX+xZSz9GIwa7y8/NVVlZm849Xkpo3b67c3FwHZQVnV75vsd/hQsXExGjx4sVatWqVXnvtNWVmZiouLk5Hjx5Vbm6u3Nzc5OfnZ7MO+xkuVHXew3Jzc9WsWTOb5a6urmrSpAn7H2xc6PtYbm5upfti+TJUn6ujEwAAoL647rrrrP/fqVMnxcTEqE2bNvroo4/k6enpwMwAoHp4H6s7OCIGu/L395fZbK5w9528vDwFBgY6KCs4u/J9i/0Otc3Pz0+XXXaZdu3apcDAQJWUlOjIkSM2Y9jPcKGq8x4WGBhY4aZXpaWlOnToEPsfzqqm72OBgYGV7ovly1B9NGKwKzc3N3Xt2lVr1qyxxiwWi9asWaMePXo4MDM4s9DQUAUGBtrsd4WFhUpNTWW/wwU5duyYdu/eraCgIHXt2lUNGjSw2c+2b9+uvXv3sp/hglTnPaxHjx46cuSI0tLSrGPWrl0ri8WimJgYu+eM+qOm72M9evTQ//73P5vGf/Xq1fLx8VGHDh3snn99xqmJsLuJEydq1KhRio6OVvfu3TV//nwVFRVpzJgxjk4N9dixY8e0a9cu6+PMzExt2bJFTZo0UevWrfXggw/q6aefVvv27RUaGqonnnhCLVq0UEJCguOSRr0zefJk3XjjjWrTpo2ys7M1ffp0mc1mDR8+XL6+vho7dqwmTpyoJk2ayMfHR/fdd5969Oih2NhYR6eOOu5C38MiIiLUr18/jRs3TosWLdKpU6c0fvx43XzzzWrRooWDqkJddKHvY9dee606dOigf/3rX5ozZ45yc3P1+OOP695775W7u7uDq6tnHH3bRlyaFixYYLRu3dpwc3Mzunfvbvz000+OTgn13LfffmtIqvAzatQowzD+uv3zE088YTRv3txwd3c3rrnmGmP79u2OTRr1zk033WQEBQUZbm5uRsuWLY2bbrrJ2LVrl3V5cXGxcc899xiNGzc2vLy8jIEDBxo5OTkOzBj1RW28hx08eNAYPny40bBhQ8PHx8cYM2aMcfToUQdUg7qsNt7HsrKyjOuuu87w9PQ0/P39jUmTJhmnTp2ydyn1nskwDMNRTSAAAAAAXIq4RgwAAAAA7IxGDAAAAADsjEYMAAAAAOyMRgwAAAAA7IxGDAAAAADsjEYMAAAAAOyMRgwAAAAA7IxGDAAAAADsjEYMAGpBVlaWTCaTtmzZ4uhUrLZt26bY2Fh5eHioc+fOlY7p3bu3HnzwwWrPuW7dOplMJh05cuSCcgsJCdH8+fMvaI4nn3yyyrpqU23ker6eeOIJ3XHHHQ55bkkaPXq0EhISLvrzmEwmJSUlXfTncVa//fabgoODVVRU5OhUANQAjRgApzB69GiZTCY9++yzNvGkpCSZTCYHZeVY06dPl7e3t7Zv3641a9Y4Op166+eff65RM1RbzWpubq5eeuklPfbYY9ZYTRvnC13PXnJycnTddddVe/zixYvl5+d38RKqhhMnTmj06NGKjIyUq6trlQ3runXr1KVLF7m7uyssLEyLFy+uMGbhwoUKCQmRh4eHYmJitGHDhgrPde+996pp06Zq2LChBg8erLy8POvyDh06KDY2Vi+88EJtlgjgIqMRA+A0PDw89Nxzz+nw4cOOTqXWlJSUnPe6u3fv1t/+9je1adNGTZs2rcWsLi0BAQHy8vKy+/O++eab6tmzp9q0aWP357a3wMBAubu7OzqNGikrK5Onp6fuv/9+9e3bt9IxmZmZuuGGG9SnTx9t2bJFDz74oG6//XZ99dVX1jEffvihJk6cqOnTp2vTpk2KiopSfHy8/vzzT+uYCRMmaMWKFfr444/13XffKTs7W4MGDbJ5rjFjxui1115TaWnpxSkYQK2jEQPgNPr27avAwEDNnj27yjGVnc42f/58hYSEWB+Xn441a9YsNW/eXH5+fpo5c6ZKS0s1ZcoUNWnSRMHBwXrnnXcqzL9t2zb17NlTHh4e6tixo7777jub5enp6bruuuvUsGFDNW/eXP/617+Un59vXd67d2+NHz9eDz74oPz9/RUfH19pHRaLRTNnzlRwcLDc3d3VuXNnrVq1yrrcZDIpLS1NM2fOlMlk0pNPPnmWV+7//Oc//1F0dLQaNWqkwMBA3XLLLTZ/EJb78ccf1alTJ3l4eCg2Nlbp6ek2y3/44QfFxcXJ09NTrVq10v3333/W06aOHDmi22+/XQEBAfLx8dHVV1+tX375xWbMs88+q+bNm6tRo0YaO3asTpw4cdZayo9MffHFF2fN9ZNPPtEVV1whd3d3hYSEaN68eTbLzzw10WQy6c0339TAgQPl5eWl9u3b6/PPP5f01ymqffr0kSQ1btxYJpNJo0ePliQtX75ckZGR8vT0VNOmTdW3b9+zviYffPCBbrzxRuvj0aNH67vvvtNLL70kk8kkk8mkrKwsSdJ3332n7t27y93dXUFBQXr44Yetf5BXtV5ZWZnGjh2r0NBQeXp66vLLL9dLL7101tf0TOVHppKSktS+fXt5eHgoPj5ef/zxh8241157Te3atZObm5suv/xy/ec//7FZfvqpieWn+SYmJqpPnz7y8vJSVFSUUlJSJP21XceMGaOCggJrPeX796uvvmrNo3nz5hoyZEiN6qkJb29vvfbaaxo3bpwCAwMrHbNo0SKFhoZq3rx5ioiI0Pjx4zVkyBC9+OKL1jEvvPCCxo0bpzFjxqhDhw5atGiRvLy89Pbbb0uSCgoK9NZbb+mFF17Q1Vdfra5du+qdd97R+vXr9dNPP1nn+cc//qFDhw5VeM8BUHfRiAFwGmazWbNmzdKCBQu0b9++C5pr7dq1ys7O1vfff68XXnhB06dPV//+/dW4cWOlpqbqrrvu0p133lnheaZMmaJJkyZp8+bN6tGjh2688UYdPHhQ0l/NxtVXX60rr7xSGzdu1KpVq5SXl6dhw4bZzLFkyRK5ubnpxx9/1KJFiyrN76WXXtK8efM0d+5cbd26VfHx8frnP/+pnTt3SvrrVK8rrrhCkyZNUk5OjiZPnlytuk+dOqWnnnpKv/zyi5KSkpSVlWVtJM6sc968efr5558VEBCgG2+8UadOnZL015G4fv36afDgwdq6das+/PBD/fDDDxo/fnyVzzt06FD9+eef+u9//6u0tDR16dJF11xzjQ4dOiRJ+uijj/Tkk09q1qxZ2rhxo4KCgvTqq69Wq6az5ZqWlqZhw4bp5ptv1v/+9z89+eSTeuKJJyo9fex0M2bM0LBhw7R161Zdf/31uvXWW3Xo0CG1atVKn3zyiSRp+/btysnJ0UsvvaScnBwNHz5ct912mzIyMrRu3ToNGjRIhmFUOv+hQ4f022+/KTo62hp76aWX1KNHD40bN045OTnKyclRq1attH//fl1//fXq1q2bfvnlF7322mt666239PTTT591PYvFouDgYH388cf67bffNG3aND366KP66KOPqvW6ljt+/LieeeYZLV26VD/++KOOHDmim2++2br8008/1QMPPKBJkyYpPT1dd955p8aMGaNvv/32rPM+9thjmjx5srZs2aLLLrtMw4cPV2lpqXr27Kn58+fLx8fHWs/kyZO1ceNG3X///Zo5c6a2b9+uVatW6e9//3uV8+/du1cNGzY868+sWbNq9FqcKSUlpcLRsvj4eGtTWVJSorS0NJsxLi4u6tu3r3VMWlqaTp06ZTMmPDxcrVu3to6RJDc3N3Xu3FnJyckXlDMAOzIAwAmMGjXKGDBggGEYhhEbG2vcdttthmEYxqeffmqc/lY3ffp0IyoqymbdF1980WjTpo3NXG3atDHKysqsscsvv9yIi4uzPi4tLTW8vb2N999/3zAMw8jMzDQkGc8++6x1zKlTp4zg4GDjueeeMwzDMJ566inj2muvtXnuP/74w5BkbN++3TAMw7jqqquMK6+88pz1tmjRwnjmmWdsYt26dTPuuece6+OoqChj+vTpZ53nqquuMh544IEql//888+GJOPo0aOGYRjGt99+a0gyPvjgA+uYgwcPGp6ensaHH35oGIZhjB071rjjjjts5klOTjZcXFyM4uJiwzAMo02bNsaLL75oXebj42OcOHHCZp127doZ//73vw3DMIwePXrY1GYYhhETE1NhW56uOrnecsstxj/+8Q+b9aZMmWJ06NDB+vj0XA3DMCQZjz/+uPXxsWPHDEnGf//7X5vnPXz4sHVMWlqaIcnIysqqMt/Tbd682ZBk7N271yZe2fZ69NFHjcsvv9ywWCzW2MKFC42GDRta9+Fzbedy9957rzF48GDr49P/XVXmnXfeMSQZP/30kzWWkZFhSDJSU1MNwzCMnj17GuPGjbNZb+jQocb1119vfSzJ+PTTTw3D+L9/S2+++aZ1+a+//mpIMjIyMqzP6+vrazPnJ598Yvj4+BiFhYXnrNMw/vr3uXPnzrP+HDx4sFpzVfU6tW/f3pg1a5ZN7IsvvjAkGcePHzf2799vSDLWr19vM2bKlClG9+7dDcMwjGXLlhlubm4V5u7WrZsxdepUm9jAgQON0aNHVytnAI7HETEATue5557TkiVLlJGRcd5zXHHFFXJx+b+3yObNmysyMtL62Gw2q2nTphVO2+vRo4f1/11dXRUdHW3N45dfftG3335r84l7eHi4pL+OIpXr2rXrWXMrLCxUdna2evXqZRPv1avXBdUs/fXp+4033qjWrVurUaNGuuqqqyT9dfTgdKfX2aRJE11++eU2dS5evNimzvj4eFksFmVmZlZ4zl9++UXHjh2z3oig/CczM9P6umRkZCgmJqbKHM7mbLlmZGRU+jru3LlTZWVlVc7ZqVMn6/97e3vLx8en0lM4y0VFRemaa65RZGSkhg4dqjfeeOOs1zIWFxdL+uu6x3PJyMhQjx49bG5K06tXLx07duycR4YXLlyorl27KiAgQA0bNtTrr79eYVufi6urq7p162Z9HB4eLj8/v3O+xufaV09/jYOCgiTprK/xP/7xD7Vp00Zt27bVv/71Ly1btkzHjx8/a95hYWFn/WnSpMlZc6xrPD09z1ozgLqFRgyA0/n73/+u+Ph4PfLIIxWWubi4VDgdrPw0tdM1aNDA5rHJZKo0ZrFYqp3XsWPHdOONN2rLli02Pzt37rQ5hcrb27vac9amoqIixcfHy8fHR8uWLdPPP/+sTz/9VFLNbhpy7Ngx3XnnnTY1/vLLL9q5c6fatWtX6figoKAKr8v27ds1ZcqUWquvNtV0XzCbzVq9erX++9//qkOHDlqwYIEuv/zyShtTSfL395eki3rjmQ8++ECTJ0/W2LFj9fXXX2vLli0aM2bMBd0gpjad/hqXN5lne40bNWqkTZs26f3331dQUJCmTZumqKioKu9eaY9TEwMDA23ubihJeXl58vHxkaenp/z9/WU2mysdU37dWWBgoEpKSirUcfqYcocOHVJAQMAF5QzAfmjEADilZ599VitWrLC5hkL66w54ubm5Ns1YbX731+kXz5eWliotLU0RERGSpC5duujXX39VSEhIhU/ea9J8+fj4qEWLFvrxxx9t4j/++KM6dOhw3rlv27ZNBw8e1LPPPqu4uDiFh4dXeQTi9DoPHz6sHTt22NT522+/VXqEwc3NrcJcXbp0UW5ubqVHKMobkoiICKWmplaZw9mcLdeIiIhKX8fLLrtMZrO5WvOfqbzGM4+omUwm9erVSzNmzNDmzZvl5uZmbXTP1K5dO/n4+Oi3336rMPeZ80ZERCglJcVmn/7xxx/VqFEjBQcHV7nejz/+qJ49e+qee+7RlVdeqbCwMJsjs9VVWlqqjRs3Wh9v375dR44cOedrfCH7amX1SH8d5erbt6/mzJmjrVu3KisrS2vXrq10jhYtWlRo/s/8ueuuu847R+mvo7FnfnXE6tWrrUdp3dzc1LVrV5sxFotFa9assY7p2rWrGjRoYDNm+/bt2rt3b4Wjwunp6bryyisvKGcA9uPq6AQA4GKIjIzUrbfeqpdfftkm3rt3bx04cEBz5szRkCFDtGrVKv33v/+Vj49PrTzvwoUL1b59e0VEROjFF1/U4cOHddttt0mS7r33Xr3xxhsaPny4pk6dqiZNmmjXrl364IMP9Oabb9boD/8pU6Zo+vTpateunTp37qx33nlHW7Zs0bJly84799atW8vNzU0LFizQXXfdpfT0dD311FOVjp05c6aaNm2q5s2b67HHHpO/v7/1e5QeeughxcbGavz48br99tvl7e2t3377TatXr9Yrr7xSYa6+ffuqR48eSkhI0Jw5c3TZZZcpOztbX3zxhQYOHKjo6Gg98MADGj16tKKjo9WrVy8tW7ZMv/76q9q2bXvOus6W66RJk9StWzc99dRTuummm5SSkqJXXnml2jcCqUybNm1kMpm0cuVKXX/99fL09NSvv/6qNWvW6Nprr1WzZs2UmpqqAwcOWJuVM5XfsOGHH36w+X6qkJAQpaamKisrSw0bNlSTJk10zz33aP78+brvvvs0fvx4bd++XdOnT9fEiROtp9dWtl779u21dOlSffXVVwoNDdV//vMf/fzzzwoNDa1RvQ0aNNB9992nl19+Wa6urho/frxiY2PVvXt3SX/tq8OGDdOVV16pvn37asWKFUpMTNQ333xzfi/w/6/n2LFjWrNmjaKiouTl5aW1a9dqz549+vvf/67GjRvryy+/lMVi0eWXX17pHOWN/4X47bffVFJSokOHDuno0aPWD3XK78x611136ZVXXtHUqVN12223ae3atfroo4/0xRdfWOeYOHGiRo0apejoaHXv3l3z589XUVGRxowZI0ny9fXV2LFjNXHiRDVp0kQ+Pj6677771KNHD8XGxlrnycrK0v79+6u8lT6AOsjB16gBQK2o7GL5zMxMw83NzTjzre61114zWrVqZXh7exsjR440nnnmmQo36zhzrspudnD6TRzKbzDw3nvvGd27dzfc3NyMDh06GGvXrrVZZ8eOHcbAgQMNPz8/w9PT0wgPDzcefPBB640WqntThbKyMuPJJ580WrZsaTRo0MCIioqy3iyi3PncrOO9994zQkJCDHd3d6NHjx7G559/bkgyNm/ebBjG/92IYsWKFcYVV1xhuLm5Gd27dzd++eUXm3k3bNhg/OMf/zAaNmxoeHt7G506dbK5uciZN8AoLCw07rvvPqNFixZGgwYNjFatWhm33nqrzc0qnnnmGcPf399o2LChMWrUKGPq1KnVulnHuXJdvny50aFDB6NBgwZG69atjeeff95meWU36yi/sUQ5X19f45133rE+njlzphEYGGiYTCZj1KhRxm+//WbEx8cbAQEBhru7u3HZZZcZCxYsqDJ3wzCML7/80mjZsqXNTWO2b99uxMbGGp6enoYkIzMz0zAMw1i3bp3RrVs3w83NzQgMDDQeeugh49SpU2dd78SJE8bo0aMNX19fw8/Pz7j77ruNhx9+2OY1rc7NOnx9fY1PPvnEaNu2reHu7m707dvX+P33323Gvfrqq0bbtm2NBg0aGJdddpmxdOlSm+Wq5GYd5fucYRjG4cOHDUnGt99+a43dddddRtOmTQ1JxvTp043k5GTjqquuMho3bmx4enoanTp1st6U5WJp06aNIanCz+m+/fZbo3Pnzoabm5vRtm1bm/2k3IIFC4zWrVtb99HTb35iGIZRXFxs3HPPPUbjxo0NLy8vY+DAgUZOTo7NmFmzZhnx8fG1XiOAi8dkGFXcOxcAgHps3bp16tOnjw4fPiw/Pz9Hp1NjhmEoJiZGEyZM0PDhwx2dTqUWL16sBx98sMrrsGAfJSUlat++vd57770KN0YBUHdxjRgAAHWQyWTS66+/bv1iZqAqe/fu1aOPPkoTBtQzXCMGAEAd1blzZ+v1RkBVym9uA6B+4dREAAAAALAzTk0EAAAAADujEQMAAAAAO6MRAwAAAAA7oxEDAAAAADujEQMAAAAAO6MRAwAAAAA7oxEDAAAAADujEQMAAAAAO/t/Zs238/sepSsAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = ['0'] + [key for key in kendalls_labeled.keys()]\n",
    "values = [kend_us[:25]] + list(kendalls_labeled.values())\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(values, vert=True, patch_artist=True, labels=labels)\n",
    "plt.title(\"Labels vs Ranking Accuracy\")\n",
    "plt.xlabel(\"Number of labeled points (total points = 1000)\")\n",
    "plt.ylabel(\"Kendall Taus\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len([kend_us] + values)\n",
    "\n",
    "rankings, predictions_on_N, true_y = simulate_models_and_predictions(num_models=5, N=1000, num_features=5)\n",
    "accs_only = [tup[1] for tup in rankings]\n",
    "\n",
    "match_rates = []\n",
    "for i in range(predictions_on_N.shape[1]):\n",
    "    match_rate = np.mean(predictions_on_N[:, i] == true_y)\n",
    "    match_rates.append(match_rate)\n",
    "\n",
    "N=1000\n",
    "label=0\n",
    "indices = observed_indices = torch.randperm(N)[:label]\n",
    "# print(indices)\n",
    "\n",
    "mask = [False for i in range(N)]\n",
    "for index in indices:\n",
    "    mask[index] = True\n",
    "\n",
    "\n",
    "pyro.clear_param_store()\n",
    "logging.getLogger(\"pyro\").setLevel(logging.DEBUG)\n",
    "nuts_kernel = NUTS(corrected_dawid_skene_model_with_observed, jit_compile=True, ignore_jit_warnings=True)\n",
    "mcmc = MCMC(nuts_kernel, num_samples=100, warmup_steps=500)\n",
    "\n",
    "observed_y = torch.full((N,), -1, dtype=torch.int64)\n",
    "observed_y[indices] = torch.tensor(true_y[indices])\n",
    "mcmc.run(torch.tensor(predictions_on_N), observed_y, torch.tensor(mask))\n",
    "\n",
    "posterior_samples_corrected = mcmc.get_samples()\n",
    "\n",
    "theta = posterior_samples_corrected[\"theta\"]\n",
    "pi = posterior_samples_corrected[\"pi\"]\n",
    "# 1. & 3. Take max values from rows 0 and 1 respectively\n",
    "max_row_0 = torch.max(theta[:,:,0,:], dim=2)[0]  # Shape: (500, 5)\n",
    "max_row_1 = torch.max(theta[:,:,1,:], dim=2)[0]  # Shape: (500, 5)\n",
    "\n",
    "# 2. & 4. Multiply with pi values\n",
    "result_row_0 = max_row_0 * pi[:,0].unsqueeze(1)  # Shape: (500, 5)\n",
    "result_row_1 = max_row_1 * pi[:,1].unsqueeze(1)  # Shape: (500, 5)\n",
    "\n",
    "# 5. Sum the results\n",
    "result = result_row_0 + result_row_1  # Shape: (500, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frank",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
