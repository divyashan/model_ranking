{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49eccdf9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinear_model\u001b[39;00m \u001b[39mimport\u001b[39;00m LogisticRegression \n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import pdb\n",
    "\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "\n",
    "# Alpha and beta control class-specific error rates; \n",
    "# alpha is the  false negative rate\n",
    "# beta is the false positive rate\n",
    "# I create classifiers with certain FNRs / FPRs but simulating different alphas & betas on the train set\n",
    "\n",
    "def apply_alpha_beta(y, alpha_opt, beta_opt):\n",
    "    t = y.copy()\n",
    "    for label in [0, 1]:\n",
    "        n_examples = (t == label).sum()\n",
    "        swap_rate = alpha_opt if label == 0 else beta_opt\n",
    "        swap_idxs = np.random.choice(np.where(t == label)[0], size=int(swap_rate * n_examples), replace=False)\n",
    "        t[swap_idxs] = 1 - label\n",
    "    return t\n",
    "\n",
    "def inv_sigm(y):\n",
    "    return np.log(y / (1 - y))\n",
    "\n",
    "def gibbs_classification(model, x):\n",
    "    p_y = model.predict_proba(x)[:,1]\n",
    "    samples = np.random.random(p_y.shape)\n",
    "    return (samples  < p_y).astype(int)\n",
    "\n",
    "def get_agreements(*tests):\n",
    "    # tests are binary outputs from classifiers\n",
    "    # This function calculates the number of agreements between the classifiers\n",
    "    # for all possible combinations of positive and negative results\n",
    "    \n",
    "    # Initialize a dictionary to store the results\n",
    "    agreements = {}\n",
    "    \n",
    "    # Get the indices of positive and negative results for each test\n",
    "    pos_indices = [np.where(test == 1)[0] for test in tests]\n",
    "    neg_indices = [np.where(test == 0)[0] for test in tests]\n",
    "    \n",
    "    # Calculate the number of agreements for all combinations\n",
    "    for i in range(2 ** len(tests)):\n",
    "        # Convert the combination to binary and pad with zeros\n",
    "        combination = bin(i)[2:].zfill(len(tests))\n",
    "        \n",
    "        # Get the indices for this combination\n",
    "        indices = [pos_indices[j] if combination[j] == '1' else neg_indices[j] for j in range(len(tests))]\n",
    "        \n",
    "        # Calculate the number of agreements\n",
    "        agreements[combination] = len(set(indices[0]).intersection(*indices[1:]))\n",
    "    \n",
    "    return agreements\n",
    "\n",
    "'''\n",
    "def get_agreements(tA, tB, tC):\n",
    "    # tA, tB, and tC are binary outputs from classifiers  A, B, and C respectively\n",
    "    tA_pos = np.where(tA == 1)[0]\n",
    "    tA_neg = np.where(tA == 0)[0]\n",
    "    tB_pos = np.where(tB == 1)[0]\n",
    "    tB_neg = np.where(tB == 0)[0]\n",
    "    tC_pos = np.where(tC == 1)[0]\n",
    "    tC_neg = np.where(tC == 0)[0]\n",
    "    n_tA_pos, n_tA_neg = len(tA_pos), len(tA_neg)\n",
    "    n_tB_pos, n_tB_neg = len(tB_pos), len(tB_neg)\n",
    "\n",
    "    n_pos_pos_pos = len(set(tA_pos).intersection(tB_pos).intersection(tC_pos))\n",
    "    n_pos_pos_neg = len(set(tA_pos).intersection(tB_pos).intersection(tC_neg))\n",
    "\n",
    "    n_pos_neg_pos = len(set(tA_pos).intersection(tB_neg).intersection(tC_pos))\n",
    "    n_pos_neg_neg = len(set(tA_pos).intersection(tB_neg).intersection(tC_neg))\n",
    "\n",
    "    n_neg_pos_pos = len(set(tA_neg).intersection(tB_pos).intersection(tC_pos))\n",
    "    n_neg_pos_neg = len(set(tA_neg).intersection(tB_pos).intersection(tC_neg))\n",
    "\n",
    "    n_neg_neg_pos = len(set(tA_neg).intersection(tB_neg).intersection(tC_pos))\n",
    "    n_neg_neg_neg = len(set(tA_neg).intersection(tB_neg).intersection(tC_neg))\n",
    "    \n",
    "    return n_pos_pos_pos, n_pos_pos_neg, n_pos_neg_pos, n_pos_neg_neg, n_neg_pos_pos, n_neg_pos_neg, n_neg_neg_pos, n_neg_neg_neg\n",
    "'''\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9b4ca046",
   "metadata": {},
   "source": [
    "## Testing Hui & Walter Method on completely synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa06cbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_alpha1, true_beta1 = .3, .1\n",
    "true_alpha2, true_beta2 = .2, .1\n",
    "true_alpha3, true_beta3 = .2, .5\n",
    "true_theta1, true_theta2 = .3, .6\n",
    "\n",
    "n_examples = 10000\n",
    "y1 = np.concatenate([np.ones((int(true_theta1*n_examples))), np.zeros((int((1-true_theta1)*n_examples)))])\n",
    "y2 = np.concatenate([np.ones((int(true_theta2*n_examples))), np.zeros((int((1-true_theta2)*n_examples)))])\n",
    "\n",
    "tA_1 = apply_alpha_beta(y1, true_alpha1, true_beta1)\n",
    "tA_2 = apply_alpha_beta(y2, true_alpha1, true_beta1)\n",
    "\n",
    "tB_1 = apply_alpha_beta(y1, true_alpha2, true_beta2)\n",
    "tB_2 = apply_alpha_beta(y2, true_alpha2, true_beta2)\n",
    "\n",
    "tC_1 = apply_alpha_beta(y1, true_alpha3, true_beta3)\n",
    "tC_2 = apply_alpha_beta(y2, true_alpha3, true_beta3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17fcceb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2835 1965 1265 3935\n",
      "5080 1520 1120 2280\n",
      "Test A alpha:  0.10000000149011612 \tbeta:  0.8999999761581421\n",
      "Test B alpha:  0.10000000149011612 \tbeta:  0.8999999761581421\n",
      "Theta1:  0.8999999761581421 Theta2:  0.30000001192092896\n",
      "\n",
      "Test A alpha:  0.18778540194034576 \tbeta:  0.1925613284111023\n",
      "Test B alpha:  0.11461319029331207 \tbeta:  0.23103028535842896\n",
      "Theta1:  0.46214595437049866 Theta2:  0.7470922470092773\n",
      "\n",
      "Test A alpha:  0.26472848653793335 \tbeta:  0.14477919042110443\n",
      "Test B alpha:  0.16014008224010468 \tbeta:  0.153035506606102\n",
      "Theta1:  0.36046701669692993 Theta2:  0.6670026779174805\n",
      "\n",
      "Test A alpha:  0.28977373242378235 \tbeta:  0.11680442839860916\n",
      "Test B alpha:  0.18973974883556366 \tbeta:  0.12126634269952774\n",
      "Theta1:  0.3183538615703583 Theta2:  0.6243534088134766\n",
      "\n",
      "Test A alpha:  0.29853931069374084 \tbeta:  0.10670668631792068\n",
      "Test B alpha:  0.19908712804317474 \tbeta:  0.10875282436609268\n",
      "Theta1:  0.3044331967830658 Theta2:  0.6080788969993591\n",
      "\n",
      "Test A alpha:  0.30107513070106506 \tbeta:  0.10414670407772064\n",
      "Test B alpha:  0.20138445496559143 \tbeta:  0.10499734431505203\n",
      "Theta1:  0.30073514580726624 Theta2:  0.6035148501396179\n",
      "\n",
      "Test A alpha:  0.30142587423324585 \tbeta:  0.10382937639951706\n",
      "Test B alpha:  0.20166784524917603 \tbeta:  0.10447289049625397\n",
      "Theta1:  0.30025041103363037 Theta2:  0.6029060482978821\n",
      "\n",
      "Test A alpha:  0.3014364242553711 \tbeta:  0.10382014513015747\n",
      "Test B alpha:  0.20167605578899384 \tbeta:  0.10445702821016312\n",
      "Theta1:  0.3002360463142395 Theta2:  0.6028879284858704\n",
      "\n",
      "Test A alpha:  0.3014370799064636 \tbeta:  0.10381941497325897\n",
      "Test B alpha:  0.20167669653892517 \tbeta:  0.1044560894370079\n",
      "Theta1:  0.3002350330352783 Theta2:  0.6028867363929749\n",
      "\n",
      "Test A alpha:  0.3014373183250427 \tbeta:  0.10381903499364853\n",
      "Test B alpha:  0.20167703926563263 \tbeta:  0.10445566475391388\n",
      "Theta1:  0.3002345561981201 Theta2:  0.6028861999511719\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Two tests, two populations\n",
    "n_epochs = 10000\n",
    "log_interval = 1000\n",
    "\n",
    "alpha1_v = Variable(torch.Tensor([inv_sigm(.1)]), requires_grad=True)\n",
    "alpha2_v = Variable(torch.Tensor([inv_sigm(.1)]), requires_grad=True)\n",
    "beta1_v = Variable(torch.Tensor([inv_sigm(.9)]), requires_grad=True)\n",
    "beta2_v = Variable(torch.Tensor([inv_sigm(.9)]), requires_grad=True)\n",
    "theta1_v = Variable(torch.Tensor([inv_sigm(.9)]), requires_grad=True)\n",
    "theta2_v = Variable(torch.Tensor([inv_sigm(.3)]), requires_grad=True)\n",
    "\n",
    "tA_1_pos = np.where(tA_1 == 1)[0]\n",
    "tA_1_neg = np.where(tA_1 == 0)[0]\n",
    "tB_1_pos = np.where(tB_1 == 1)[0]\n",
    "tB_1_neg = np.where(tB_1 == 0)[0]\n",
    "n_tA_1_pos, n_tA_1_neg = len(tA_1_pos), len(tA_1_neg)\n",
    "n_tB_1_pos, n_tB_1_neg = len(tB_1_pos), len(tB_1_neg)\n",
    "\n",
    "n_pos_pos_1 = len(set(tA_1_pos).intersection(tB_1_pos))\n",
    "n_pos_neg_1 = len(set(tA_1_pos).intersection(tB_1_neg))\n",
    "n_neg_pos_1 = len(set(tA_1_neg).intersection(tB_1_pos))\n",
    "n_neg_neg_1 = len(set(tA_1_neg).intersection(tB_1_neg))\n",
    "\n",
    "tA_2_pos = np.where(tA_2 == 1)[0]\n",
    "tA_2_neg = np.where(tA_2 == 0)[0]\n",
    "tB_2_pos = np.where(tB_2 == 1)[0]\n",
    "tB_2_neg = np.where(tB_2 == 0)[0]\n",
    "n_tA_2_pos, n_tA_2_neg = len(tA_2_pos), len(tA_2_neg)\n",
    "n_tB_2_pos, n_tB_2_neg = len(tB_2_pos), len(tB_2_neg)\n",
    "\n",
    "n_pos_pos_2 = len(set(tA_2_pos).intersection(tB_2_pos))\n",
    "n_pos_neg_2 = len(set(tA_2_pos).intersection(tB_2_neg))\n",
    "n_neg_pos_2 = len(set(tA_2_neg).intersection(tB_2_pos))\n",
    "n_neg_neg_2 = len(set(tA_2_neg).intersection(tB_2_neg))\n",
    "\n",
    "print(n_pos_pos_1, n_pos_neg_1, n_neg_pos_1, n_neg_neg_1)\n",
    "print(n_pos_pos_2, n_pos_neg_2, n_neg_pos_2, n_neg_neg_2)\n",
    "\n",
    "\n",
    "learning_rate = .01\n",
    "optim = torch.optim.Adam([theta1_v, theta2_v, beta2_v, beta1_v, alpha2_v, alpha1_v], lr = learning_rate)\n",
    "losses, beta2s = [], []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    optim.zero_grad()\n",
    "    alpha1, alpha2 = torch.sigmoid(alpha1_v), torch.sigmoid(alpha2_v)\n",
    "    beta1, beta2 = torch.sigmoid(beta1_v), torch.sigmoid(beta2_v)\n",
    "    theta1, theta2 = torch.sigmoid(theta1_v), torch.sigmoid(theta2_v)\n",
    "    n_1 = len(tA_1)\n",
    "    n_2 = len(tA_2)\n",
    "    l = n_pos_pos_1*torch.log(((theta1 * (1-beta1) * (1 - beta2)) + ((1-theta1)*alpha1*alpha2)))/n_1\n",
    "    l += n_pos_neg_1*torch.log((theta1*(1-beta1)*beta2) + ((1-theta1)*alpha1*(1-alpha2)))/n_1\n",
    "    l += n_neg_pos_1*torch.log(theta1*beta1*(1-beta2) +  (1-theta1)*(1-alpha1)*alpha2)/n_1\n",
    "    l += n_neg_neg_1*torch.log(theta1*beta1*beta2 + (1-theta1)*(1-alpha1)*(1-alpha2))/n_1\n",
    "\n",
    "    l += n_pos_pos_2*torch.log(((theta2 * (1-beta1) * (1 - beta2)) + ((1-theta2)*alpha1*alpha2)))/n_2\n",
    "    l += n_pos_neg_2*torch.log((theta2*(1-beta1)*beta2) + ((1-theta2)*alpha1*(1-alpha2)))/n_2\n",
    "    l += n_neg_pos_2*torch.log(theta2*beta1*(1-beta2) +  (1-theta2)*(1-alpha1)*alpha2)/n_2\n",
    "    l += n_neg_neg_2*torch.log(theta2*beta1*beta2 + (1-theta2)*(1-alpha1)*(1-alpha2))/n_2\n",
    "    \n",
    "    loss = -l\n",
    "    loss.backward()\n",
    "    \n",
    "    optim.step()\n",
    "    if  epoch % log_interval == 0:\n",
    "        print(\"Test A alpha: \", alpha1.item(), \"\\tbeta: \", beta1.item())\n",
    "        print(\"Test B alpha: \", alpha2.item(), \"\\tbeta: \", beta2.item())\n",
    "        print(\"Theta1: \", theta1.item(), \"Theta2: \", theta2.item())\n",
    "        print()\n",
    "    beta2s.append(beta2.item())\n",
    "    losses.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "931afd9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1304 443 314 839\n",
      "1531 1522 951 3096\n",
      "Test A alpha:  0.10000000149011612 \tbeta:  0.8999999761581421\n",
      "Test B alpha:  0.10000000149011612 \tbeta:  0.8999999761581421\n",
      "Test C alpha:  0.8319999575614929 \tbeta:  0.30000001192092896\n",
      "Theta1:  0.3333333432674408\n",
      "\n",
      "Test A alpha:  0.16549690067768097 \tbeta:  0.31640398502349854\n",
      "Test B alpha:  0.11652206629514694 \tbeta:  0.38648658990859985\n",
      "Test C alpha:  0.20833376049995422 \tbeta:  0.6460244655609131\n",
      "Theta1:  0.5885652303695679\n",
      "\n",
      "Test A alpha:  0.2736150324344635 \tbeta:  0.17340947687625885\n",
      "Test B alpha:  0.1629699319601059 \tbeta:  0.17351561784744263\n",
      "Test C alpha:  0.18695004284381866 \tbeta:  0.5363795757293701\n",
      "Theta1:  0.36892181634902954\n",
      "\n",
      "Test A alpha:  0.3043733835220337 \tbeta:  0.10402128845453262\n",
      "Test B alpha:  0.20264922082424164 \tbeta:  0.09817074984312057\n",
      "Test C alpha:  0.20125539600849152 \tbeta:  0.4991602599620819\n",
      "Theta1:  0.2957342267036438\n",
      "\n",
      "Test A alpha:  0.3139350414276123 \tbeta:  0.07940417528152466\n",
      "Test B alpha:  0.2135593593120575 \tbeta:  0.06871592998504639\n",
      "Test C alpha:  0.2069910168647766 \tbeta:  0.48950573801994324\n",
      "Theta1:  0.2733921408653259\n",
      "\n",
      "Test A alpha:  0.3185865879058838 \tbeta:  0.06849300116300583\n",
      "Test B alpha:  0.21808789670467377 \tbeta:  0.053240906447172165\n",
      "Test C alpha:  0.2098284512758255 \tbeta:  0.48565298318862915\n",
      "Theta1:  0.26324108242988586\n",
      "\n",
      "Test A alpha:  0.3212449550628662 \tbeta:  0.0634596049785614\n",
      "Test B alpha:  0.2201274037361145 \tbeta:  0.0440436489880085\n",
      "Test C alpha:  0.21136829257011414 \tbeta:  0.48384833335876465\n",
      "Theta1:  0.2579823136329651\n",
      "\n",
      "Test A alpha:  0.32279279828071594 \tbeta:  0.06131488457322121\n",
      "Test B alpha:  0.2209908813238144 \tbeta:  0.03856601566076279\n",
      "Test C alpha:  0.2121991664171219 \tbeta:  0.48299339413642883\n",
      "Theta1:  0.2552427351474762\n",
      "\n",
      "Test A alpha:  0.3236057758331299 \tbeta:  0.06045696511864662\n",
      "Test B alpha:  0.2213367074728012 \tbeta:  0.035652630031108856\n",
      "Test C alpha:  0.21261298656463623 \tbeta:  0.48260730504989624\n",
      "Theta1:  0.25391054153442383\n",
      "\n",
      "Test A alpha:  0.3239278495311737 \tbeta:  0.06015577167272568\n",
      "Test B alpha:  0.22145827114582062 \tbeta:  0.034491416066884995\n",
      "Test C alpha:  0.21277382969856262 \tbeta:  0.4824638068675995\n",
      "Theta1:  0.25339779257774353\n",
      "\n",
      "True A alpha1:  0.3 \t beta:  0.09999999999999998\n",
      "True B alpha2:  0.2 \t beta:  0.09999999999999998\n",
      "True C alpha2:  0.2 \t betat:  0.5\n",
      "True theta1:  0.3\n"
     ]
    }
   ],
   "source": [
    "# Three tests, one population\n",
    "n_epochs = 10000\n",
    "log_interval = 1000\n",
    "\n",
    "alpha1_v = Variable(torch.Tensor([inv_sigm(.1)]), requires_grad=True)\n",
    "alpha2_v = Variable(torch.Tensor([inv_sigm(.1)]), requires_grad=True)\n",
    "alpha3_v = Variable(torch.Tensor([inv_sigm(.832)]), requires_grad=True)\n",
    "beta1_v = Variable(torch.Tensor([inv_sigm(.9)]), requires_grad=True)\n",
    "beta2_v = Variable(torch.Tensor([inv_sigm(.9)]), requires_grad=True)\n",
    "beta3_v = Variable(torch.Tensor([inv_sigm(.3)]), requires_grad=True)\n",
    "theta1_v = Variable(torch.Tensor([inv_sigm(1/3)]), requires_grad=True)\n",
    "\n",
    "tA_1_pos = np.where(tA_1 == 1)[0]\n",
    "tA_1_neg = np.where(tA_1 == 0)[0]\n",
    "tB_1_pos = np.where(tB_1 == 1)[0]\n",
    "tB_1_neg = np.where(tB_1 == 0)[0]\n",
    "tC_1_pos = np.where(tC_1 == 1)[0]\n",
    "tC_1_neg = np.where(tC_1 == 0)[0]\n",
    "n_tA_1_pos, n_tA_1_neg = len(tA_1_pos), len(tA_1_neg)\n",
    "n_tB_1_pos, n_tB_1_neg = len(tB_1_pos), len(tB_1_neg)\n",
    "\n",
    "n_pos_pos_pos_1 = len(set(tA_1_pos).intersection(tB_1_pos).intersection(tC_1_pos))\n",
    "n_pos_pos_neg_1 = len(set(tA_1_pos).intersection(tB_1_pos).intersection(tC_1_neg))\n",
    "\n",
    "n_pos_neg_pos_1 = len(set(tA_1_pos).intersection(tB_1_neg).intersection(tC_1_pos))\n",
    "n_pos_neg_neg_1 = len(set(tA_1_pos).intersection(tB_1_neg).intersection(tC_1_neg))\n",
    "\n",
    "n_neg_pos_pos_1 = len(set(tA_1_neg).intersection(tB_1_pos).intersection(tC_1_pos))\n",
    "n_neg_pos_neg_1 = len(set(tA_1_neg).intersection(tB_1_pos).intersection(tC_1_neg))\n",
    "\n",
    "n_neg_neg_pos_1 = len(set(tA_1_neg).intersection(tB_1_neg).intersection(tC_1_pos))\n",
    "n_neg_neg_neg_1 = len(set(tA_1_neg).intersection(tB_1_neg).intersection(tC_1_neg))\n",
    "\n",
    "print(n_pos_pos_pos_1, n_pos_neg_pos_1, n_neg_pos_pos_1, n_neg_neg_pos_1)\n",
    "print(n_pos_pos_neg_1, n_pos_neg_neg_1, n_neg_pos_neg_1, n_neg_neg_neg_1)\n",
    "\n",
    "\n",
    "learning_rate = .01\n",
    "optim = torch.optim.Adam([theta1_v, beta3_v, beta2_v, beta1_v, alpha3_v, alpha2_v, alpha1_v], lr = learning_rate)\n",
    "\n",
    "losses, beta2s = [], []\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    optim.zero_grad()\n",
    "    alpha1, alpha2, alpha3 = torch.sigmoid(alpha1_v), torch.sigmoid(alpha2_v), torch.sigmoid(alpha3_v)\n",
    "    beta1, beta2, beta3 = torch.sigmoid(beta1_v), torch.sigmoid(beta2_v), torch.sigmoid(beta3_v)\n",
    "    theta1 = torch.sigmoid(theta1_v)\n",
    "    n_1 = len(tA_1)\n",
    "    n_2 = len(tA_2)\n",
    "    l = n_pos_pos_pos_1*torch.log(((theta1 * (1-beta1) * (1-beta2) * (1-beta3)) + ((1-theta1)*alpha1*alpha2*alpha3)))/n_1\n",
    "    l += n_pos_pos_neg_1*torch.log(((theta1 * (1-beta1) * (1-beta2) * (beta3)) + ((1-theta1)*alpha1*alpha2*(1-alpha3))))/n_1\n",
    "\n",
    "    l += n_pos_neg_pos_1*torch.log((theta1*(1-beta1)*beta2*(1-beta3)) + ((1-theta1)*alpha1*(1-alpha2)*alpha3))/n_1\n",
    "    l += n_pos_neg_neg_1*torch.log((theta1*(1-beta1)*beta2*(beta3)) + ((1-theta1)*alpha1*(1-alpha2)*(1-alpha3)))/n_1\n",
    "\n",
    "    l += n_neg_pos_pos_1*torch.log(theta1*beta1*(1-beta2)*(1-beta3) +  (1-theta1)*(1-alpha1)*alpha2*alpha3)/n_1\n",
    "    l += n_neg_pos_neg_1*torch.log(theta1*beta1*(1-beta2)*(beta3) +  (1-theta1)*(1-alpha1)*alpha2*(1-alpha3))/n_1\n",
    "    \n",
    "    l += n_neg_neg_pos_1*torch.log(theta1*beta1*beta2*(1-beta3) + (1-theta1)*(1-alpha1)*(1-alpha2)*alpha3)/n_1\n",
    "    l += n_neg_neg_neg_1*torch.log(theta1*beta1*beta2*beta3 + (1-theta1)*(1-alpha1)*(1-alpha2)*(1-alpha3))/n_1\n",
    "\n",
    "    \n",
    "    loss = -l\n",
    "    loss.backward()\n",
    "    \n",
    "    optim.step()\n",
    "    if  epoch % log_interval == 0:\n",
    "        print(\"Test A alpha: \", alpha1.item(), \"\\tbeta: \", beta1.item())\n",
    "        print(\"Test B alpha: \", alpha2.item(), \"\\tbeta: \", beta2.item())\n",
    "        print(\"Test C alpha: \", alpha3.item(), \"\\tbeta: \", beta3.item())\n",
    "        print(\"Theta1: \", theta1.item())\n",
    "        print()\n",
    "    beta2s.append(beta2.item())\n",
    "    losses.append(loss.item())\n",
    "\n",
    "print(\"True A alpha1: \",np.mean(tA_1[y1 == 0]), '\\t beta: ', 1-np.mean(tA_1[y1 == 1]))\n",
    "print(\"True B alpha2: \", np.mean(tB_1[y1 == 0]), '\\t beta: ',  1 - np.mean(tB_1[y1 == 1]))\n",
    "print(\"True C alpha2: \", np.mean(tC_1[y1 == 0]), '\\t betat: ',  1 - np.mean(tC_1[y1 == 1]))\n",
    "\n",
    "print(\"True theta1: \", np.mean(y1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf74e0fa",
   "metadata": {},
   "source": [
    "## Testing on logistic regressions (introducing correlated errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a24937e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(penalty='none')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate x \n",
    "train_size = 20000\n",
    "pos = np.random.multivariate_normal((1, 1), [[1, 1], [1, 1]], size=train_size)\n",
    "neg = np.random.multivariate_normal((-1, -1), [[1, 1], [1, 1]], size=train_size)\n",
    "\n",
    "x_train = np.concatenate([pos, neg], axis=0)\n",
    "y_train = np.concatenate([np.ones(pos.shape[0]), np.zeros(neg.shape[0])], axis=0)\n",
    "\n",
    "# Train Model 1 \n",
    "mA_alpha, mA_beta = .2, .1\n",
    "xA_train, yA_train = x_train.copy(), apply_alpha_beta(y_train, mA_alpha, mA_beta)\n",
    "lrA = LogisticRegression(penalty='none')\n",
    "lrA.fit(xA_train, yA_train)\n",
    "\n",
    "# Train Model 2 \n",
    "mB_alpha, mB_beta = .3, .1\n",
    "xB_train, yB_train = x_train.copy(), apply_alpha_beta(y_train, mB_alpha, mB_beta)\n",
    "lrB = LogisticRegression(penalty='none')\n",
    "lrB.fit(xB_train, yB_train)\n",
    "\n",
    "# Train Model 3\n",
    "mC_alpha, mC_beta = .3, .2\n",
    "xC_train, yC_train = x_train.copy(), apply_alpha_beta(y_train, mC_alpha, mC_beta)\n",
    "lrC = LogisticRegression(penalty='none')\n",
    "lrC.fit(xC_train, yC_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0042b5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two test populations\n",
    "n_samples = 10000\n",
    "pos_1 = np.random.multivariate_normal((1, 1), [[1, 1], [1, 1]], size=n_samples)\n",
    "neg_1 = np.random.multivariate_normal((-1, -1), [[1, 1], [1, 1]], size=2*n_samples)\n",
    "\n",
    "pos_2 = np.random.multivariate_normal((1, 1), [[1, 1], [1, 1]], size=2*n_samples)\n",
    "neg_2 = np.random.multivariate_normal((-1, -1), [[1, 1], [1, 1]], size=2*n_samples)\n",
    "\n",
    "x1 = np.concatenate([pos_1, neg_1], axis=0)\n",
    "y1 = np.concatenate([np.ones(n_samples), np.zeros(2*n_samples)])\n",
    "x2 = np.concatenate([pos_2, neg_2], axis=0)\n",
    "y2 = np.concatenate([np.ones(2*n_samples), np.zeros(2*n_samples)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe9ae32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test A alpha1:  0.2447 \tbeta1:  0.10519999999999996\n",
      "Test B alpha2:  0.37395 \tbeta2:  0.0524\n",
      "Test C alpha3:  0.27765 \tbeta3:  0.08679999999999999\n",
      "True theta1:  0.3333333333333333 \tTrue theta2:  0.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tA_1 = lrA.predict(x1)\n",
    "tB_1 = lrB.predict(x1)\n",
    "tC_1 = lrC.predict(x1)\n",
    "tA_2 = lrA.predict(x2)\n",
    "tB_2 = lrB.predict(x2)\n",
    "tC_2 = lrC.predict(x2)\n",
    "\n",
    "\n",
    "print(\"Test A alpha1: \",np.mean(tA_1[y1 == 0]), \"\\tbeta1: \", 1-np.mean(tA_1[y1 == 1]))\n",
    "print(\"Test B alpha2: \", np.mean(tB_1[y1 == 0]), \"\\tbeta2: \", 1 - np.mean(tB_1[y1 == 1]))\n",
    "print(\"Test C alpha3: \", np.mean(tC_1[y1 == 0]), \"\\tbeta3: \", 1 - np.mean(tC_1[y1 == 1]))\n",
    "\n",
    "print(\"True theta1: \", np.mean(y1), \"\\tTrue theta2: \", np.mean(y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9ccb902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13842 0 3113 13045\n",
      "22805 0 3589 13606\n",
      "Test A alpha:  0.36000001430511475 \tbeta:  0.2600000202655792\n",
      "Test B alpha:  0.42000001668930054 \tbeta:  0.10000000149011612\n",
      "Theta1:  0.3333333432674408 Theta2:  0.5\n",
      "\n",
      "Test A alpha:  0.36000001430511475 \tbeta:  0.13222569227218628\n",
      "Test B alpha:  0.42000001668930054 \tbeta:  0.043846167623996735\n",
      "Theta1:  0.38604527711868286 Theta2:  0.5271825194358826\n",
      "\n",
      "Test A alpha:  0.36000001430511475 \tbeta:  0.07563553005456924\n",
      "Test B alpha:  0.42000001668930054 \tbeta:  0.022362666204571724\n",
      "Theta1:  0.3779316842556 Theta2:  0.5143343210220337\n",
      "\n",
      "Test A alpha:  0.36000001430511475 \tbeta:  0.04669126495718956\n",
      "Test B alpha:  0.42000001668930054 \tbeta:  0.012688428163528442\n",
      "Theta1:  0.37355613708496094 Theta2:  0.5072187185287476\n",
      "\n",
      "Test A alpha:  0.36000001430511475 \tbeta:  0.030023159459233284\n",
      "Test B alpha:  0.42000001668930054 \tbeta:  0.007658025249838829\n",
      "Theta1:  0.3708486258983612 Theta2:  0.5027210116386414\n",
      "\n",
      "Test A alpha:  0.36000001430511475 \tbeta:  0.019704284146428108\n",
      "Test B alpha:  0.42000001668930054 \tbeta:  0.004790145438164473\n",
      "Theta1:  0.3690800964832306 Theta2:  0.49974191188812256\n",
      "\n",
      "Test A alpha:  0.36000001430511475 \tbeta:  0.01305430382490158\n",
      "Test B alpha:  0.42000001668930054 \tbeta:  0.0030601071193814278\n",
      "Theta1:  0.3678966164588928 Theta2:  0.4977310597896576\n",
      "\n",
      "Test A alpha:  0.36000001430511475 \tbeta:  0.008679550141096115\n",
      "Test B alpha:  0.42000001668930054 \tbeta:  0.001979796914383769\n",
      "Theta1:  0.3670973479747772 Theta2:  0.4963659346103668\n",
      "\n",
      "Test A alpha:  0.36000001430511475 \tbeta:  0.005774563178420067\n",
      "Test B alpha:  0.42000001668930054 \tbeta:  0.0012907879427075386\n",
      "Theta1:  0.3665567934513092 Theta2:  0.49543988704681396\n",
      "\n",
      "Test A alpha:  0.36000001430511475 \tbeta:  0.003839103039354086\n",
      "Test B alpha:  0.42000001668930054 \tbeta:  0.0008455849601887167\n",
      "Theta1:  0.3661920726299286 Theta2:  0.49481379985809326\n",
      "\n",
      "True A alpha1:  0.2447 \t beta:  0.10519999999999996\n",
      "True B alpha2:  0.37395 \t beta:  0.0524\n",
      "True theta1:  0.3333333333333333 \t  theta2:  0.5\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10000\n",
    "log_interval = 1000\n",
    "\n",
    "alpha1_v = Variable(torch.Tensor([inv_sigm(.36)]), requires_grad=True)\n",
    "alpha2_v = Variable(torch.Tensor([inv_sigm(.42)]), requires_grad=True)\n",
    "beta1_v = Variable(torch.Tensor([inv_sigm(.26)]), requires_grad=True)\n",
    "beta2_v = Variable(torch.Tensor([inv_sigm(.1)]), requires_grad=True)\n",
    "theta1_v = Variable(torch.Tensor([inv_sigm(1/3)]), requires_grad=True)\n",
    "theta2_v = Variable(torch.Tensor([inv_sigm(.5)]), requires_grad=True)\n",
    "\n",
    "tA_1_pos = np.where(tA_1 == 1)[0]\n",
    "tA_1_neg = np.where(tA_1 == 0)[0]\n",
    "tB_1_pos = np.where(tB_1 == 1)[0]\n",
    "tB_1_neg = np.where(tB_1 == 0)[0]\n",
    "n_tA_1_pos, n_tA_1_neg = len(tA_1_pos), len(tA_1_neg)\n",
    "n_tB_1_pos, n_tB_1_neg = len(tB_1_pos), len(tB_1_neg)\n",
    "\n",
    "n_pos_pos_1 = len(set(tA_1_pos).intersection(tB_1_pos))\n",
    "n_pos_neg_1 = len(set(tA_1_pos).intersection(tB_1_neg))\n",
    "n_neg_pos_1 = len(set(tA_1_neg).intersection(tB_1_pos))\n",
    "n_neg_neg_1 = len(set(tA_1_neg).intersection(tB_1_neg))\n",
    "\n",
    "tA_2_pos = np.where(tA_2 == 1)[0]\n",
    "tA_2_neg = np.where(tA_2 == 0)[0]\n",
    "tB_2_pos = np.where(tB_2 == 1)[0]\n",
    "tB_2_neg = np.where(tB_2 == 0)[0]\n",
    "n_tA_2_pos, n_tA_2_neg = len(tA_2_pos), len(tA_2_neg)\n",
    "n_tB_2_pos, n_tB_2_neg = len(tB_2_pos), len(tB_2_neg)\n",
    "\n",
    "n_pos_pos_2 = len(set(tA_2_pos).intersection(tB_2_pos))\n",
    "n_pos_neg_2 = len(set(tA_2_pos).intersection(tB_2_neg))\n",
    "n_neg_pos_2 = len(set(tA_2_neg).intersection(tB_2_pos))\n",
    "n_neg_neg_2 = len(set(tA_2_neg).intersection(tB_2_neg))\n",
    "\n",
    "print(n_pos_pos_1, n_pos_neg_1, n_neg_pos_1, n_neg_neg_1)\n",
    "print(n_pos_pos_2, n_pos_neg_2, n_neg_pos_2, n_neg_neg_2)\n",
    "\n",
    "\n",
    "learning_rate = .001\n",
    "### I've played around with fixing certain variables, optimizing others, optimizing all: \n",
    "# optim = torch.optim.Adam([alpha1_v, alpha2_v, beta1_v, beta2_v, theta1_v, theta2_v], lr = learning_rate)\n",
    "optim = torch.optim.Adam([theta2_v, theta1_v, beta1_v,  beta2_v], lr = learning_rate)\n",
    "\n",
    "losses = []\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    optim.zero_grad()\n",
    "    alpha1, alpha2 = torch.sigmoid(alpha1_v), torch.sigmoid(alpha2_v)\n",
    "    beta1, beta2 = torch.sigmoid(beta1_v), torch.sigmoid(beta2_v)\n",
    "    theta1, theta2 = torch.sigmoid(theta1_v), torch.sigmoid(theta2_v)\n",
    "    n_1 = len(tA_1)\n",
    "    n_2 = len(tA_2)\n",
    "    \n",
    "    l = n_pos_pos_1*torch.log(((theta1 * (1-beta1) * (1 - beta2)) + ((1-theta1)*alpha1*alpha2)))/n_1\n",
    "    l += n_pos_neg_1*torch.log((theta1*(1-beta1)*beta2) + ((1-theta1)*alpha1*(1-alpha2)))/n_1\n",
    "    l += n_neg_pos_1*torch.log(theta1*beta1*(1-beta2) +  (1-theta1)*(1-alpha1)*alpha2)/n_1\n",
    "    l += n_neg_neg_1*torch.log(theta1*beta1*beta2 + (1-theta1)*(1-alpha1)*(1-alpha2))/n_1\n",
    "\n",
    "    l += n_pos_pos_2*torch.log(((theta2 * (1-beta1) * (1 - beta2)) + ((1-theta2)*alpha1*alpha2)))/n_2\n",
    "    l += n_pos_neg_2*torch.log((theta2*(1-beta1)*beta2) + ((1-theta2)*alpha1*(1-alpha2)))/n_2\n",
    "    l += n_neg_pos_2*torch.log(theta2*beta1*(1-beta2) +  (1-theta2)*(1-alpha1)*alpha2)/n_2\n",
    "    l += n_neg_neg_2*torch.log(theta2*beta1*beta2 + (1-theta2)*(1-alpha1)*(1-alpha2))/n_2\n",
    "    \n",
    "    loss = -l\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    if  epoch % log_interval == 0:\n",
    "        print(\"Test A alpha: \", alpha1.item(), \"\\tbeta: \", beta1.item())\n",
    "        print(\"Test B alpha: \", alpha2.item(), \"\\tbeta: \", beta2.item())\n",
    "        print(\"Theta1: \", theta1.item(), \"Theta2: \", theta2.item())\n",
    "        print()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "print(\"True A alpha1: \",np.mean(tA_1[y1 == 0]), '\\t beta: ', 1-np.mean(tA_1[y1 == 1]))\n",
    "print(\"True B alpha2: \", np.mean(tB_1[y1 == 0]), '\\t beta: ',  1 - np.mean(tB_1[y1 == 1]))\n",
    "\n",
    "print(\"True theta1: \", np.mean(y1), \"\\t  theta2: \", np.mean(y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7961432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13842 0 843 0\n",
      "0 0 2270 13045\n",
      "Test A alpha:  0.10000000149011612 \tbeta:  0.10000000149011612\n",
      "Test B alpha:  0.10000000149011612 \tbeta:  0.10000000149011612\n",
      "Test C alpha:  0.8100000023841858 \tbeta:  0.005999999586492777\n",
      "Theta1:  0.3333333432674408\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ddmg/prism/.conda/envs/tta_nfs/lib/python3.6/site-packages/torch/optim/adam.py:74: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  super(Adam, self).__init__(params, defaults)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test A alpha:  0.0029628148768097162 \tbeta:  0.049351077526807785\n",
      "Test B alpha:  0.15513065457344055 \tbeta:  0.0031594757456332445\n",
      "Test C alpha:  0.052851445972919464 \tbeta:  0.005999999586492777\n",
      "Theta1:  0.4853556156158447\n",
      "\n",
      "Test A alpha:  0.0009176111198030412 \tbeta:  0.05547148734331131\n",
      "Test B alpha:  0.1498972326517105 \tbeta:  0.0009668408310972154\n",
      "Test C alpha:  0.015157789923250675 \tbeta:  0.005999999586492777\n",
      "Theta1:  0.4884958565235138\n",
      "\n",
      "Test A alpha:  0.0004116959753446281 \tbeta:  0.05675632134079933\n",
      "Test B alpha:  0.14878427982330322 \tbeta:  0.000431859923992306\n",
      "Test C alpha:  0.006621029227972031 \tbeta:  0.005999999586492777\n",
      "Theta1:  0.48916247487068176\n",
      "\n",
      "Test A alpha:  0.0002131867513526231 \tbeta:  0.05723519250750542\n",
      "Test B alpha:  0.14836879074573517 \tbeta:  0.00022315642854664475\n",
      "Test C alpha:  0.003385787596926093 \tbeta:  0.005999999586492777\n",
      "Theta1:  0.4894113838672638\n",
      "\n",
      "Test A alpha:  0.00011837219790322706 \tbeta:  0.05749208852648735\n",
      "Test B alpha:  0.1481749415397644 \tbeta:  0.00012376459199003875\n",
      "Test C alpha:  0.0018672114238142967 \tbeta:  0.005999999586492777\n",
      "Theta1:  0.4895274043083191\n",
      "\n",
      "Test A alpha:  6.823983130743727e-05 \tbeta:  0.057772185653448105\n",
      "Test B alpha:  0.14807403087615967 \tbeta:  7.130012818379328e-05\n",
      "Test C alpha:  0.0010721249273046851 \tbeta:  0.005999999586492777\n",
      "Theta1:  0.4895877242088318\n",
      "\n",
      "Test A alpha:  4.0183531382353976e-05 \tbeta:  0.05792918801307678\n",
      "Test B alpha:  0.14801840484142303 \tbeta:  4.196817462798208e-05\n",
      "Test C alpha:  0.000629785587079823 \tbeta:  0.005999999586492777\n",
      "Theta1:  0.48962029814720154\n",
      "\n",
      "Test A alpha:  2.3956590666784905e-05 \tbeta:  0.057716596871614456\n",
      "Test B alpha:  0.14798590540885925 \tbeta:  2.50140146818012e-05\n",
      "Test C alpha:  0.0003748862072825432 \tbeta:  0.005999999586492777\n",
      "Theta1:  0.48963361978530884\n",
      "\n",
      "Test A alpha:  1.4387140254257247e-05 \tbeta:  0.0573396161198616\n",
      "Test B alpha:  0.14795973896980286 \tbeta:  1.501964834460523e-05\n",
      "Test C alpha:  0.00022491825802717358 \tbeta:  0.005999999586492777\n",
      "Theta1:  0.48964929580688477\n",
      "\n",
      "True A alpha1:  0.2447 \t beta:  0.10519999999999996\n",
      "True B alpha2:  0.37395 \t beta:  0.0524\n",
      "True C alpha3:  0.27765 \t beta:  0.08679999999999999\n",
      "True theta1:  0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "## 3 models, 1 population\n",
    "n_epochs = 10000\n",
    "log_interval = 1000\n",
    "\n",
    "alpha1_v = Variable(torch.Tensor([inv_sigm(.1)]), requires_grad=True)\n",
    "alpha2_v = Variable(torch.Tensor([inv_sigm(.1)]), requires_grad=True)\n",
    "alpha3_v = Variable(torch.Tensor([inv_sigm(.81)]), requires_grad=True)\n",
    "beta1_v = Variable(torch.Tensor([inv_sigm(.1)]), requires_grad=True)\n",
    "beta2_v = Variable(torch.Tensor([inv_sigm(.1)]), requires_grad=True)\n",
    "beta3_v = Variable(torch.Tensor([inv_sigm(.006)]), requires_grad=True)\n",
    "theta1_v = Variable(torch.Tensor([inv_sigm(1/3)]), requires_grad=True)\n",
    "\n",
    "tA_1_pos = np.where(tA_1 == 1)[0]\n",
    "tA_1_neg = np.where(tA_1 == 0)[0]\n",
    "tB_1_pos = np.where(tB_1 == 1)[0]\n",
    "tB_1_neg = np.where(tB_1 == 0)[0]\n",
    "tC_1_pos = np.where(tC_1 == 1)[0]\n",
    "tC_1_neg = np.where(tC_1 == 0)[0]\n",
    "n_tA_1_pos, n_tA_1_neg = len(tA_1_pos), len(tA_1_neg)\n",
    "n_tB_1_pos, n_tB_1_neg = len(tB_1_pos), len(tB_1_neg)\n",
    "\n",
    "n_pos_pos_pos_1 = len(set(tA_1_pos).intersection(tB_1_pos).intersection(tC_1_pos))\n",
    "n_pos_pos_neg_1 = len(set(tA_1_pos).intersection(tB_1_pos).intersection(tC_1_neg))\n",
    "\n",
    "n_pos_neg_pos_1 = len(set(tA_1_pos).intersection(tB_1_neg).intersection(tC_1_pos))\n",
    "n_pos_neg_neg_1 = len(set(tA_1_pos).intersection(tB_1_neg).intersection(tC_1_neg))\n",
    "\n",
    "n_neg_pos_pos_1 = len(set(tA_1_neg).intersection(tB_1_pos).intersection(tC_1_pos))\n",
    "n_neg_pos_neg_1 = len(set(tA_1_neg).intersection(tB_1_pos).intersection(tC_1_neg))\n",
    "\n",
    "n_neg_neg_pos_1 = len(set(tA_1_neg).intersection(tB_1_neg).intersection(tC_1_pos))\n",
    "n_neg_neg_neg_1 = len(set(tA_1_neg).intersection(tB_1_neg).intersection(tC_1_neg))\n",
    "\n",
    "print(n_pos_pos_pos_1, n_pos_neg_pos_1, n_neg_pos_pos_1, n_neg_neg_pos_1)\n",
    "print(n_pos_pos_neg_1, n_pos_neg_neg_1, n_neg_pos_neg_1, n_neg_neg_neg_1)\n",
    "\n",
    "\n",
    "learning_rate = .01\n",
    "optim = torch.optim.Adam([theta1_v, beta3_v, beta2_v, beta1_v, alpha3_v, alpha2_v, alpha1_v], lr = learning_rate)\n",
    "optim = torch.optim.Adam([theta1_v, beta1_v, beta2_v, beta1_v, alpha3_v,  alpha2_v, alpha1_v], lr = learning_rate)\n",
    "\n",
    "losses, beta2s = [], []\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    optim.zero_grad()\n",
    "    alpha1, alpha2, alpha3 = torch.sigmoid(alpha1_v), torch.sigmoid(alpha2_v), torch.sigmoid(alpha3_v)\n",
    "    beta1, beta2, beta3 = torch.sigmoid(beta1_v), torch.sigmoid(beta2_v), torch.sigmoid(beta3_v)\n",
    "    theta1 = torch.sigmoid(theta1_v)\n",
    "    n_1 = len(tA_1)\n",
    "    n_2 = len(tA_2)\n",
    "    l = n_pos_pos_pos_1*torch.log(((theta1 * (1-beta1) * (1-beta2) * (1-beta3)) + ((1-theta1)*alpha1*alpha2*alpha3)))/n_1\n",
    "    l += n_pos_pos_neg_1*torch.log(((theta1 * (1-beta1) * (1-beta2) * (beta3)) + ((1-theta1)*alpha1*alpha2*(1-alpha3))))/n_1\n",
    "\n",
    "    l += n_pos_neg_pos_1*torch.log((theta1*(1-beta1)*beta2*(1-beta3)) + ((1-theta1)*alpha1*(1-alpha2)*alpha3))/n_1\n",
    "    l += n_pos_neg_neg_1*torch.log((theta1*(1-beta1)*beta2*(beta3)) + ((1-theta1)*alpha1*(1-alpha2)*(1-alpha3)))/n_1\n",
    "\n",
    "    l += n_neg_pos_pos_1*torch.log(theta1*beta1*(1-beta2)*(1-beta3) +  (1-theta1)*(1-alpha1)*alpha2*alpha3)/n_1\n",
    "    l += n_neg_pos_neg_1*torch.log(theta1*beta1*(1-beta2)*(beta3) +  (1-theta1)*(1-alpha1)*alpha2*(1-alpha3))/n_1\n",
    "    \n",
    "    l += n_neg_neg_pos_1*torch.log(theta1*beta1*beta2*(1-beta3) + (1-theta1)*(1-alpha1)*(1-alpha2)*alpha3)/n_1\n",
    "    l += n_neg_neg_neg_1*torch.log(theta1*beta1*beta2*beta3 + (1-theta1)*(1-alpha1)*(1-alpha2)*(1-alpha3))/n_1\n",
    "\n",
    "    \n",
    "    loss = -l\n",
    "    loss.backward()\n",
    "    \n",
    "    optim.step()\n",
    "    if  epoch % log_interval == 0:\n",
    "        print(\"Test A alpha: \", alpha1.item(), \"\\tbeta: \", beta1.item())\n",
    "        print(\"Test B alpha: \", alpha2.item(), \"\\tbeta: \", beta2.item())\n",
    "        print(\"Test C alpha: \", alpha3.item(), \"\\tbeta: \", beta3.item())\n",
    "        print(\"Theta1: \", theta1.item())\n",
    "        print()\n",
    "    beta2s.append(beta2.item())\n",
    "    losses.append(loss.item())\n",
    "\n",
    "print(\"True A alpha1: \",np.mean(tA_1[y1 == 0]), '\\t beta: ', 1-np.mean(tA_1[y1 == 1]))\n",
    "print(\"True B alpha2: \", np.mean(tB_1[y1 == 0]), '\\t beta: ',  1 - np.mean(tB_1[y1 == 1]))\n",
    "print(\"True C alpha3: \", np.mean(tC_1[y1 == 0]), '\\t beta: ',  1 - np.mean(tC_1[y1 == 1]))\n",
    "\n",
    "print(\"True theta1: \", np.mean(y1))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7697ea3",
   "metadata": {},
   "source": [
    "### Experiments - No Labeled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "179a2411",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a04c90a9fa704cfcbde50870e09e3875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We can't get accurate estimates of alpha or beta for any classifier without additional assumptions about the errors. \n",
    "# Also, the assumption that the errors are independent  of x is broken.\n",
    "# Can we get an ordering of classifiers if we know 1 classifiers specificity?\n",
    "# But maybe you can get an ordering of classifiers on your data\n",
    "import itertools\n",
    "from tqdm.notebook import tqdm\n",
    "# Sample different alpha, beta, and theta values \n",
    "# Train different models \n",
    "\n",
    "alpha_opts = [.1, .2, .3]\n",
    "beta_opts = [.1, .2, .3]\n",
    "theta_opts = [.1, .3, .5, .7]\n",
    "\n",
    "alpha_sets = []\n",
    "for i, alphaA in enumerate(alpha_opts):\n",
    "    if i > 1:\n",
    "        continue\n",
    "    for alphaB in alpha_opts[i+1:]:\n",
    "        for alphaC in alpha_opts[i+2:]:\n",
    "            alpha_sets.append((alphaA, alphaB, alphaC))\n",
    "beta_sets = []\n",
    "for i,betaA in enumerate(beta_opts):\n",
    "    for betaB in beta_opts[i+1:]:\n",
    "        for betaC in beta_opts[i+2:]:\n",
    "            if betaC == betaB:\n",
    "                continue\n",
    "            beta_sets.append((betaA, betaB, betaC))\n",
    "\n",
    "train_size = 10000\n",
    "test_size = 20000\n",
    "n_epochs = 10000\n",
    "results = []\n",
    "configs = list(itertools.product(alpha_sets, beta_sets, theta_opts))\n",
    "for alpha_set, beta_set, true_theta in tqdm(configs):\n",
    "    true_alphaA, true_alphaB, true_alphaC = alpha_set\n",
    "    true_betaA, true_betaB, true_betaC = beta_set\n",
    "    \n",
    "    pos = np.random.multivariate_normal((1, 1), [[1, 1], [1, 1]], size=train_size)\n",
    "    neg = np.random.multivariate_normal((-1, -1), [[1, 1], [1, 1]], size=train_size)\n",
    "    x_train = np.concatenate([pos, neg], axis=0)\n",
    "    y_train = np.concatenate([np.ones(train_size), np.zeros(train_size)], axis=0)\n",
    "    models = []\n",
    "    \n",
    "    for m_params in [(true_alphaA, true_betaA), \n",
    "                     (true_alphaB, true_betaB), \n",
    "                     (true_alphaC, true_betaC)]:\n",
    "        xM_train, yM_train = x_train.copy(), apply_alpha_beta(y_train, m_params[0], m_params[1])\n",
    "        lrM = LogisticRegression(penalty='none')\n",
    "        lrM.fit(xM_train, yM_train)\n",
    "        models.append(lrM)\n",
    "\n",
    "    # Create test population\n",
    "    pos_1 = np.random.multivariate_normal((1, 1), [[1, 1], [1, 1]], size=int(true_theta*test_size))\n",
    "    neg_1 = np.random.multivariate_normal((-1, -1), [[1, 1], [1, 1]], size=int((1-true_theta)*test_size))\n",
    "    \n",
    "    x1 = np.concatenate([pos_1, neg_1], axis=0)\n",
    "    y1 = np.concatenate([np.ones(pos_1.shape[0]), np.zeros(neg_1.shape[0])])\n",
    "    \n",
    "    model_preds = [gibbs_classification(model, x1) for model in models]\n",
    "    nPPP, nPPN, nPNP, nPNN,  nNPP, nNPN, nNNP, nNNN = get_agreements(*model_preds)\n",
    "\n",
    "    # alpha_ranking\n",
    "    model_names = ['A', 'B', 'C']\n",
    "    true_alpha_ranking = [model_names[i] for i in np.argsort([true_alphaA, true_alphaB, true_alphaC])]\n",
    "    true_beta_ranking = [model_names[i] for i in np.argsort([true_betaA, true_betaB, true_betaC])]\n",
    "    \n",
    "    alphaA_v, alphaB_v, alphaC_v = [Variable(torch.Tensor([inv_sigm(.5)]), requires_grad=True) for i in range(3)]\n",
    "    betaA_v, betaB_v, betaC_v = [Variable(torch.Tensor([inv_sigm(.5)]), requires_grad=True) for i in range(3)]\n",
    "    theta1_v = Variable(torch.Tensor([inv_sigm(.5)]), requires_grad=True)\n",
    "    \n",
    "    # We know betaC_v, or the \n",
    "    betaC_v.requires_grad = False\n",
    "    betaC_v[0] = inv_sigm(1 - np.mean(model_preds[2][y1==1]))\n",
    "    betaC_v.requires_grad = False\n",
    "    learning_rate = .01\n",
    "    optim = torch.optim.Adam([theta1_v, betaA_v, betaB_v, alphaA_v, alphaB_v, alphaC_v], lr = learning_rate)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        optim.zero_grad()\n",
    "        alphaA, alphaB, alphaC = torch.sigmoid(alphaA_v), torch.sigmoid(alphaB_v), torch.sigmoid(alphaC_v)\n",
    "        betaA, betaB, betaC = torch.sigmoid(betaA_v), torch.sigmoid(betaB_v), torch.sigmoid(betaC_v)\n",
    "        theta1 = torch.sigmoid(theta1_v)\n",
    "        n_1 = len(model_preds[0])\n",
    "        l = nPPP*torch.log(((theta1 * (1-betaA) * (1-betaB) * (1-betaC)) + ((1-theta1)*alphaA*alphaB*alphaC)))/n_1\n",
    "        l += nPPN*torch.log(((theta1 * (1-betaA) * (1-betaB) * (betaC)) + ((1-theta1)*alphaA*alphaB*(1-alphaC))))/n_1\n",
    "\n",
    "        l += nPNP*torch.log((theta1*(1-betaA)*betaB*(1-betaC)) + ((1-theta1)*alphaA*(1-alphaB)*alphaC))/n_1\n",
    "        l += nPNN*torch.log((theta1*(1-betaA)*betaB*(betaC)) + ((1-theta1)*alphaA*(1-alphaB)*(1-alphaC)))/n_1\n",
    "\n",
    "        l += nNPP*torch.log(theta1*betaA*(1-betaB)*(1-betaC) +  (1-theta1)*(1-alphaA)*alphaB*alphaC)/n_1\n",
    "        l += nNPN*torch.log(theta1*betaA*(1-betaB)*(betaC) +  (1-theta1)*(1-alphaA)*alphaB*(1-alphaC))/n_1\n",
    "\n",
    "        l += nNNP*torch.log(theta1*betaA*betaB*(1-betaC) + (1-theta1)*(1-alphaA)*(1-alphaB)*alphaC)/n_1\n",
    "        l += nNNN*torch.log(theta1*betaA*betaB*betaC + (1-theta1)*(1-alphaA)*(1-alphaB)*(1-alphaC))/n_1\n",
    "\n",
    "        loss = -l\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    \n",
    "    result = {'pred_alphaA': alphaA.item(), 'pred_alphaB': alphaB.item(), 'pred_alphaC': alphaC.item(),\n",
    "              'pred_betaA': betaA.item(), 'pred_betaB': betaB.item(), 'pred_betaC': betaC.item(),\n",
    "              'pred_theta1': theta1.item(),\n",
    "              'true_alphaA': true_alphaA, 'true_alphaB': true_alphaB, 'true_alphaC': true_alphaC,\n",
    "              'true_betaA': true_betaA, 'true_betaB': true_betaB, 'true_betaC': true_betaC, 'true_theta1': true_theta}\n",
    "    result['true_alpha_ranking'] = true_alpha_ranking\n",
    "    result['true_beta_ranking'] = true_beta_ranking\n",
    "    result['pred_alpha_ranking'] = [model_names[i] for i in np.argsort([result['pred_alphaA'], result['pred_alphaB'],\n",
    "                                                           result['pred_alphaB']])]\n",
    "    result['pred_beta_ranking'] = [model_names[i] for i in np.argsort([result['pred_betaA'], result['pred_betaB'],\n",
    "                                                           result['pred_betaB']])]\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "#  % of time our algorithm ranks the correct method as 1st\n",
    "# % of time our algorithm ranks the correct method as 2nd\n",
    "# % of time our algorithm ranks the correct method as 3rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "485bf99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "for quantity in ['alphaA', 'alphaB', 'alphaC', 'betaA', 'betaB', 'betaC']:\n",
    "    results_df[quantity + '_err'] = results_df['true_' + quantity] - results_df['pred_' + quantity]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53faf861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluating rankings based on sensitivity and specificity\n",
    "print(np.mean(results_df['true_beta_ranking'] == results_df['pred_beta_ranking']))\n",
    "print(np.mean(results_df['true_alpha_ranking'] == results_df['pred_alpha_ranking']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "17ee3ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['correct_beta_order'] = results_df['true_beta_ranking'] ==  results_df['pred_beta_ranking']\n",
    "results_df['correct_alpha_order'] = results_df['true_alpha_ranking'] ==  results_df['pred_alpha_ranking']\n",
    "results_df['mean_alpha'] = (results_df['true_alphaA'] + results_df['true_alphaB'] + results_df['true_alphaC'])/3\n",
    "results_df['mean_beta'] = (results_df['true_betaA'] + results_df['true_betaB'] + results_df['true_betaC'])/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a91bde6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='true_betaC', ylabel='alphaA_err'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "sns.pointplot(x='true_betaC', y='alphaA_err', data=results_df, join=False)\n",
    "# What % of the top ranked model in terms of alpha is correct?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a9109f6",
   "metadata": {},
   "source": [
    "### Improvements\n",
    "\n",
    "- Incorporating outputted probabilities?\n",
    "- Modeling how errors are likely correlated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f98bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
